{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProjectModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSqAt7qwqftr",
        "outputId": "4656ef9b-b017-451d-b972-230153c8e8d1"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-XwiUMAqjSw"
      },
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "\n",
        "data, sampling_rate = librosa.load('/content/drive/My Drive/Final_Dataset/anger001.wav')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "Bb2gBn7Lq_bF",
        "outputId": "12a428e2-87ed-46ee-ba81-27c233f8ecaf"
      },
      "source": [
        "% pylab inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7f3462aa6a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61CZ8IdwrEJF",
        "outputId": "30d074a6-5a6b-4a33-cb86-a97b3e2ae52f"
      },
      "source": [
        "import time\n",
        "\n",
        "path = '/content/drive/My Drive/Final_Dataset/'\n",
        "Y_File=[]\n",
        "Y = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "    if 'anger' in file:\n",
        "      Y.append(0)\n",
        "      Y_File.append(os.path.join(subdir,file))\n",
        "    if 'disgust' in file:\n",
        "      Y.append(1)\n",
        "      Y_File.append(os.path.join(subdir,file))\n",
        "    if 'fear' in file:\n",
        "      Y.append(2)\n",
        "      Y_File.append(os.path.join(subdir,file))\n",
        "    if 'happy' in file:\n",
        "      Y.append(3)\n",
        "      Y_File.append(os.path.join(subdir,file))\n",
        "    if 'neutral' in file:\n",
        "      Y.append(4)\n",
        "      Y_File.append(os.path.join(subdir,file))\n",
        "    if 'sad' in file:\n",
        "      Y.append(5)\n",
        "      Y_File.append(os.path.join(subdir,file))\n",
        "    if 'surprise' in file:\n",
        "      Y.append(6)\n",
        "      Y_File.append(os.path.join(subdir,file))\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Data loaded. Loading time: 0.03161501884460449 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh7QVe5FrMIK",
        "outputId": "2286fb02-3705-4133-fa49-1cd2f9a7f13d"
      },
      "source": [
        "len(Y),len(Y_File)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2568, 2568)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z28xNNXMrRCo",
        "outputId": "3bf1f973-30cf-4476-ebfd-dc48c227e7f8"
      },
      "source": [
        "import time\n",
        "\n",
        "lst = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(len(Y_File)):\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(Y_File[i], res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        lst.append(mfccs)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Data loaded. Loading time: 710.3960571289062 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqUK3jZvrVtK",
        "outputId": "50f33465-ab6d-4c40-dfb2-a50d39f4f0f9"
      },
      "source": [
        "len(lst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2568"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "X9h_O10OvHfr",
        "outputId": "ae82fbb3-59a9-48c4-c2b2-51ee9b2f1cf3"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(lst)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-648.970947</td>\n",
              "      <td>71.907417</td>\n",
              "      <td>2.742204</td>\n",
              "      <td>21.670790</td>\n",
              "      <td>10.711693</td>\n",
              "      <td>8.265316</td>\n",
              "      <td>7.268557</td>\n",
              "      <td>2.156604</td>\n",
              "      <td>-6.138276</td>\n",
              "      <td>7.114227</td>\n",
              "      <td>-0.638948</td>\n",
              "      <td>2.334098</td>\n",
              "      <td>4.068620</td>\n",
              "      <td>3.934223</td>\n",
              "      <td>-0.716825</td>\n",
              "      <td>1.969140</td>\n",
              "      <td>-1.288988</td>\n",
              "      <td>-0.913698</td>\n",
              "      <td>-0.759566</td>\n",
              "      <td>2.153710</td>\n",
              "      <td>-2.214774</td>\n",
              "      <td>1.103560</td>\n",
              "      <td>-1.414614</td>\n",
              "      <td>1.267050</td>\n",
              "      <td>-0.493648</td>\n",
              "      <td>0.134097</td>\n",
              "      <td>-1.264701</td>\n",
              "      <td>1.224581</td>\n",
              "      <td>-2.835206</td>\n",
              "      <td>-0.283147</td>\n",
              "      <td>-0.060137</td>\n",
              "      <td>-0.057659</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.087855</td>\n",
              "      <td>-1.292643</td>\n",
              "      <td>-0.410933</td>\n",
              "      <td>0.224731</td>\n",
              "      <td>-0.935340</td>\n",
              "      <td>-0.405711</td>\n",
              "      <td>-1.539037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-745.476807</td>\n",
              "      <td>61.395737</td>\n",
              "      <td>1.011072</td>\n",
              "      <td>12.793580</td>\n",
              "      <td>-4.854984</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>-6.133131</td>\n",
              "      <td>-9.800947</td>\n",
              "      <td>-9.225055</td>\n",
              "      <td>0.980532</td>\n",
              "      <td>-5.173116</td>\n",
              "      <td>-2.589133</td>\n",
              "      <td>-9.412117</td>\n",
              "      <td>4.176839</td>\n",
              "      <td>-5.030715</td>\n",
              "      <td>-5.789496</td>\n",
              "      <td>-5.578049</td>\n",
              "      <td>-1.107720</td>\n",
              "      <td>-4.441068</td>\n",
              "      <td>-4.955767</td>\n",
              "      <td>-4.737144</td>\n",
              "      <td>-2.706756</td>\n",
              "      <td>-8.366491</td>\n",
              "      <td>-2.773106</td>\n",
              "      <td>-4.010757</td>\n",
              "      <td>-3.787622</td>\n",
              "      <td>-4.050997</td>\n",
              "      <td>-1.320769</td>\n",
              "      <td>-4.180398</td>\n",
              "      <td>-0.914175</td>\n",
              "      <td>-3.937309</td>\n",
              "      <td>-2.184175</td>\n",
              "      <td>-2.725434</td>\n",
              "      <td>-0.365667</td>\n",
              "      <td>-0.593917</td>\n",
              "      <td>2.263465</td>\n",
              "      <td>3.600143</td>\n",
              "      <td>5.182899</td>\n",
              "      <td>6.436582</td>\n",
              "      <td>6.961022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-694.004333</td>\n",
              "      <td>61.496510</td>\n",
              "      <td>-3.262744</td>\n",
              "      <td>16.971298</td>\n",
              "      <td>2.142968</td>\n",
              "      <td>4.266798</td>\n",
              "      <td>-5.015399</td>\n",
              "      <td>-2.617859</td>\n",
              "      <td>-12.885774</td>\n",
              "      <td>-1.302279</td>\n",
              "      <td>-1.563737</td>\n",
              "      <td>-0.917418</td>\n",
              "      <td>0.153905</td>\n",
              "      <td>-1.951386</td>\n",
              "      <td>-3.258693</td>\n",
              "      <td>1.956144</td>\n",
              "      <td>-5.813616</td>\n",
              "      <td>-2.408270</td>\n",
              "      <td>-0.432048</td>\n",
              "      <td>0.449308</td>\n",
              "      <td>-5.577334</td>\n",
              "      <td>-0.920122</td>\n",
              "      <td>-1.964510</td>\n",
              "      <td>-3.439345</td>\n",
              "      <td>-1.153978</td>\n",
              "      <td>-0.557548</td>\n",
              "      <td>-3.899142</td>\n",
              "      <td>-0.646266</td>\n",
              "      <td>-1.816866</td>\n",
              "      <td>-0.701520</td>\n",
              "      <td>-1.382858</td>\n",
              "      <td>-2.590943</td>\n",
              "      <td>-1.929074</td>\n",
              "      <td>-2.390322</td>\n",
              "      <td>-2.269381</td>\n",
              "      <td>-2.486079</td>\n",
              "      <td>-0.589257</td>\n",
              "      <td>-3.248326</td>\n",
              "      <td>-2.979813</td>\n",
              "      <td>-2.769281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-710.096069</td>\n",
              "      <td>40.656712</td>\n",
              "      <td>-13.243647</td>\n",
              "      <td>14.821146</td>\n",
              "      <td>-11.988942</td>\n",
              "      <td>-4.326802</td>\n",
              "      <td>-11.298809</td>\n",
              "      <td>-7.587059</td>\n",
              "      <td>-9.825305</td>\n",
              "      <td>-0.084983</td>\n",
              "      <td>-8.675142</td>\n",
              "      <td>-1.556004</td>\n",
              "      <td>-8.769038</td>\n",
              "      <td>2.118348</td>\n",
              "      <td>-9.668043</td>\n",
              "      <td>-3.032947</td>\n",
              "      <td>-5.550335</td>\n",
              "      <td>-2.129647</td>\n",
              "      <td>-3.414623</td>\n",
              "      <td>-1.301786</td>\n",
              "      <td>-0.036070</td>\n",
              "      <td>1.410935</td>\n",
              "      <td>-2.667373</td>\n",
              "      <td>1.127486</td>\n",
              "      <td>-1.977145</td>\n",
              "      <td>1.986175</td>\n",
              "      <td>4.099287</td>\n",
              "      <td>4.674964</td>\n",
              "      <td>2.503084</td>\n",
              "      <td>2.805913</td>\n",
              "      <td>-0.246652</td>\n",
              "      <td>0.444371</td>\n",
              "      <td>-0.521537</td>\n",
              "      <td>0.362264</td>\n",
              "      <td>0.854505</td>\n",
              "      <td>0.650243</td>\n",
              "      <td>-0.423510</td>\n",
              "      <td>-1.529283</td>\n",
              "      <td>-1.998029</td>\n",
              "      <td>-0.062299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-600.512085</td>\n",
              "      <td>73.287621</td>\n",
              "      <td>5.885289</td>\n",
              "      <td>19.855776</td>\n",
              "      <td>5.840355</td>\n",
              "      <td>-0.147993</td>\n",
              "      <td>1.414411</td>\n",
              "      <td>-2.404692</td>\n",
              "      <td>-10.087945</td>\n",
              "      <td>2.899282</td>\n",
              "      <td>-1.044273</td>\n",
              "      <td>-0.664250</td>\n",
              "      <td>-1.161344</td>\n",
              "      <td>4.153498</td>\n",
              "      <td>-1.920255</td>\n",
              "      <td>0.318724</td>\n",
              "      <td>-2.219993</td>\n",
              "      <td>0.929482</td>\n",
              "      <td>0.388972</td>\n",
              "      <td>-0.788825</td>\n",
              "      <td>-2.326617</td>\n",
              "      <td>0.063424</td>\n",
              "      <td>-2.413080</td>\n",
              "      <td>-3.108146</td>\n",
              "      <td>-1.687048</td>\n",
              "      <td>-1.630714</td>\n",
              "      <td>-2.334900</td>\n",
              "      <td>-1.168309</td>\n",
              "      <td>-0.079526</td>\n",
              "      <td>0.794789</td>\n",
              "      <td>-0.770868</td>\n",
              "      <td>-1.369582</td>\n",
              "      <td>-1.891460</td>\n",
              "      <td>-1.157018</td>\n",
              "      <td>-1.040969</td>\n",
              "      <td>-1.041425</td>\n",
              "      <td>-2.091770</td>\n",
              "      <td>-2.694788</td>\n",
              "      <td>-2.069784</td>\n",
              "      <td>-1.805427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2563</th>\n",
              "      <td>-578.713196</td>\n",
              "      <td>61.370003</td>\n",
              "      <td>-8.907619</td>\n",
              "      <td>21.265520</td>\n",
              "      <td>5.554171</td>\n",
              "      <td>-2.427649</td>\n",
              "      <td>-2.837762</td>\n",
              "      <td>0.677961</td>\n",
              "      <td>-12.619606</td>\n",
              "      <td>7.729056</td>\n",
              "      <td>-2.592522</td>\n",
              "      <td>-0.465079</td>\n",
              "      <td>-3.177077</td>\n",
              "      <td>3.720269</td>\n",
              "      <td>-3.148828</td>\n",
              "      <td>1.093849</td>\n",
              "      <td>-7.312330</td>\n",
              "      <td>1.868895</td>\n",
              "      <td>-4.143651</td>\n",
              "      <td>-0.830634</td>\n",
              "      <td>-4.200087</td>\n",
              "      <td>1.502687</td>\n",
              "      <td>-6.224370</td>\n",
              "      <td>-1.663106</td>\n",
              "      <td>-2.695183</td>\n",
              "      <td>-2.355363</td>\n",
              "      <td>-1.806027</td>\n",
              "      <td>-0.406772</td>\n",
              "      <td>-0.936953</td>\n",
              "      <td>-0.639671</td>\n",
              "      <td>-1.645374</td>\n",
              "      <td>-0.923842</td>\n",
              "      <td>-2.112805</td>\n",
              "      <td>-1.375529</td>\n",
              "      <td>-1.862561</td>\n",
              "      <td>-0.290023</td>\n",
              "      <td>-2.385124</td>\n",
              "      <td>-1.649704</td>\n",
              "      <td>-2.507972</td>\n",
              "      <td>-1.427291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2564</th>\n",
              "      <td>-576.611877</td>\n",
              "      <td>41.392681</td>\n",
              "      <td>-17.155254</td>\n",
              "      <td>6.986942</td>\n",
              "      <td>-9.122391</td>\n",
              "      <td>-3.664443</td>\n",
              "      <td>-20.467916</td>\n",
              "      <td>-3.572575</td>\n",
              "      <td>-14.322601</td>\n",
              "      <td>2.762153</td>\n",
              "      <td>-10.094601</td>\n",
              "      <td>-6.661758</td>\n",
              "      <td>-2.747277</td>\n",
              "      <td>-7.654290</td>\n",
              "      <td>-2.902933</td>\n",
              "      <td>-5.420545</td>\n",
              "      <td>-9.148721</td>\n",
              "      <td>-1.615915</td>\n",
              "      <td>-7.005185</td>\n",
              "      <td>-4.869830</td>\n",
              "      <td>-6.075991</td>\n",
              "      <td>-3.604834</td>\n",
              "      <td>-7.162820</td>\n",
              "      <td>-3.446610</td>\n",
              "      <td>-5.309924</td>\n",
              "      <td>0.403759</td>\n",
              "      <td>-0.442520</td>\n",
              "      <td>3.125771</td>\n",
              "      <td>4.213610</td>\n",
              "      <td>3.124016</td>\n",
              "      <td>4.282145</td>\n",
              "      <td>3.382751</td>\n",
              "      <td>3.443053</td>\n",
              "      <td>4.978160</td>\n",
              "      <td>5.427543</td>\n",
              "      <td>5.740356</td>\n",
              "      <td>4.451098</td>\n",
              "      <td>2.878379</td>\n",
              "      <td>0.935435</td>\n",
              "      <td>2.211232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2565</th>\n",
              "      <td>-513.391968</td>\n",
              "      <td>35.304344</td>\n",
              "      <td>-27.528683</td>\n",
              "      <td>-1.573403</td>\n",
              "      <td>-19.190140</td>\n",
              "      <td>-7.995622</td>\n",
              "      <td>-18.173841</td>\n",
              "      <td>-11.630806</td>\n",
              "      <td>-8.921432</td>\n",
              "      <td>-0.404055</td>\n",
              "      <td>-10.451320</td>\n",
              "      <td>0.513336</td>\n",
              "      <td>-8.950840</td>\n",
              "      <td>-2.935175</td>\n",
              "      <td>-5.182651</td>\n",
              "      <td>-6.297908</td>\n",
              "      <td>-8.257548</td>\n",
              "      <td>-5.361291</td>\n",
              "      <td>-6.544790</td>\n",
              "      <td>-0.453004</td>\n",
              "      <td>0.816983</td>\n",
              "      <td>1.280731</td>\n",
              "      <td>2.082585</td>\n",
              "      <td>1.808482</td>\n",
              "      <td>-0.245074</td>\n",
              "      <td>0.945363</td>\n",
              "      <td>0.798398</td>\n",
              "      <td>3.540350</td>\n",
              "      <td>4.012304</td>\n",
              "      <td>3.273612</td>\n",
              "      <td>2.815805</td>\n",
              "      <td>1.460280</td>\n",
              "      <td>-0.774430</td>\n",
              "      <td>0.655724</td>\n",
              "      <td>0.936522</td>\n",
              "      <td>1.376725</td>\n",
              "      <td>-0.514725</td>\n",
              "      <td>-2.646844</td>\n",
              "      <td>-2.270415</td>\n",
              "      <td>0.339581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2566</th>\n",
              "      <td>-503.895874</td>\n",
              "      <td>46.516319</td>\n",
              "      <td>-5.204802</td>\n",
              "      <td>3.634700</td>\n",
              "      <td>-7.994336</td>\n",
              "      <td>1.957494</td>\n",
              "      <td>-17.072124</td>\n",
              "      <td>-12.035769</td>\n",
              "      <td>-11.504045</td>\n",
              "      <td>-2.664160</td>\n",
              "      <td>-9.096724</td>\n",
              "      <td>-3.757857</td>\n",
              "      <td>-9.150162</td>\n",
              "      <td>-4.733320</td>\n",
              "      <td>-6.719837</td>\n",
              "      <td>0.308393</td>\n",
              "      <td>-8.646714</td>\n",
              "      <td>-2.159988</td>\n",
              "      <td>-4.451622</td>\n",
              "      <td>-3.077097</td>\n",
              "      <td>-6.187653</td>\n",
              "      <td>-3.097657</td>\n",
              "      <td>-3.816869</td>\n",
              "      <td>-3.805601</td>\n",
              "      <td>-4.331494</td>\n",
              "      <td>-1.828928</td>\n",
              "      <td>-3.806374</td>\n",
              "      <td>-0.731557</td>\n",
              "      <td>-0.000963</td>\n",
              "      <td>1.330716</td>\n",
              "      <td>1.240385</td>\n",
              "      <td>0.871198</td>\n",
              "      <td>-0.147335</td>\n",
              "      <td>-0.468855</td>\n",
              "      <td>0.425717</td>\n",
              "      <td>2.372419</td>\n",
              "      <td>0.957126</td>\n",
              "      <td>-0.369405</td>\n",
              "      <td>-0.379200</td>\n",
              "      <td>-1.097025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2567</th>\n",
              "      <td>-560.837097</td>\n",
              "      <td>50.168743</td>\n",
              "      <td>-19.705780</td>\n",
              "      <td>4.857205</td>\n",
              "      <td>-14.152937</td>\n",
              "      <td>-1.806413</td>\n",
              "      <td>-14.607023</td>\n",
              "      <td>-10.531888</td>\n",
              "      <td>-8.830826</td>\n",
              "      <td>1.842284</td>\n",
              "      <td>-10.997461</td>\n",
              "      <td>3.024669</td>\n",
              "      <td>-11.034261</td>\n",
              "      <td>0.173616</td>\n",
              "      <td>-5.544483</td>\n",
              "      <td>-4.531592</td>\n",
              "      <td>-1.218367</td>\n",
              "      <td>-4.226231</td>\n",
              "      <td>-7.439675</td>\n",
              "      <td>-2.918094</td>\n",
              "      <td>-4.972703</td>\n",
              "      <td>-4.242082</td>\n",
              "      <td>-4.365048</td>\n",
              "      <td>-2.168633</td>\n",
              "      <td>-3.950434</td>\n",
              "      <td>-0.162558</td>\n",
              "      <td>2.696079</td>\n",
              "      <td>3.357330</td>\n",
              "      <td>4.228732</td>\n",
              "      <td>5.612514</td>\n",
              "      <td>6.008360</td>\n",
              "      <td>8.070698</td>\n",
              "      <td>5.162092</td>\n",
              "      <td>4.522226</td>\n",
              "      <td>2.342329</td>\n",
              "      <td>0.923058</td>\n",
              "      <td>1.369484</td>\n",
              "      <td>1.216854</td>\n",
              "      <td>0.766931</td>\n",
              "      <td>0.479213</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2568 rows  40 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0          1          2   ...        37        38        39\n",
              "0    -648.970947  71.907417   2.742204  ... -0.935340 -0.405711 -1.539037\n",
              "1    -745.476807  61.395737   1.011072  ...  5.182899  6.436582  6.961022\n",
              "2    -694.004333  61.496510  -3.262744  ... -3.248326 -2.979813 -2.769281\n",
              "3    -710.096069  40.656712 -13.243647  ... -1.529283 -1.998029 -0.062299\n",
              "4    -600.512085  73.287621   5.885289  ... -2.694788 -2.069784 -1.805427\n",
              "...          ...        ...        ...  ...       ...       ...       ...\n",
              "2563 -578.713196  61.370003  -8.907619  ... -1.649704 -2.507972 -1.427291\n",
              "2564 -576.611877  41.392681 -17.155254  ...  2.878379  0.935435  2.211232\n",
              "2565 -513.391968  35.304344 -27.528683  ... -2.646844 -2.270415  0.339581\n",
              "2566 -503.895874  46.516319  -5.204802  ... -0.369405 -0.379200 -1.097025\n",
              "2567 -560.837097  50.168743 -19.705780  ...  1.216854  0.766931  0.479213\n",
              "\n",
              "[2568 rows x 40 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF_Ak3hqvbzp"
      },
      "source": [
        "df['labels']=Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "ZzujTTf_v_dW",
        "outputId": "072f8c27-05b8-4ded-ac85-19d266ff7e49"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-648.970947</td>\n",
              "      <td>71.907417</td>\n",
              "      <td>2.742204</td>\n",
              "      <td>21.670790</td>\n",
              "      <td>10.711693</td>\n",
              "      <td>8.265316</td>\n",
              "      <td>7.268557</td>\n",
              "      <td>2.156604</td>\n",
              "      <td>-6.138276</td>\n",
              "      <td>7.114227</td>\n",
              "      <td>-0.638948</td>\n",
              "      <td>2.334098</td>\n",
              "      <td>4.068620</td>\n",
              "      <td>3.934223</td>\n",
              "      <td>-0.716825</td>\n",
              "      <td>1.969140</td>\n",
              "      <td>-1.288988</td>\n",
              "      <td>-0.913698</td>\n",
              "      <td>-0.759566</td>\n",
              "      <td>2.153710</td>\n",
              "      <td>-2.214774</td>\n",
              "      <td>1.103560</td>\n",
              "      <td>-1.414614</td>\n",
              "      <td>1.267050</td>\n",
              "      <td>-0.493648</td>\n",
              "      <td>0.134097</td>\n",
              "      <td>-1.264701</td>\n",
              "      <td>1.224581</td>\n",
              "      <td>-2.835206</td>\n",
              "      <td>-0.283147</td>\n",
              "      <td>-0.060137</td>\n",
              "      <td>-0.057659</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.087855</td>\n",
              "      <td>-1.292643</td>\n",
              "      <td>-0.410933</td>\n",
              "      <td>0.224731</td>\n",
              "      <td>-0.935340</td>\n",
              "      <td>-0.405711</td>\n",
              "      <td>-1.539037</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-745.476807</td>\n",
              "      <td>61.395737</td>\n",
              "      <td>1.011072</td>\n",
              "      <td>12.793580</td>\n",
              "      <td>-4.854984</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>-6.133131</td>\n",
              "      <td>-9.800947</td>\n",
              "      <td>-9.225055</td>\n",
              "      <td>0.980532</td>\n",
              "      <td>-5.173116</td>\n",
              "      <td>-2.589133</td>\n",
              "      <td>-9.412117</td>\n",
              "      <td>4.176839</td>\n",
              "      <td>-5.030715</td>\n",
              "      <td>-5.789496</td>\n",
              "      <td>-5.578049</td>\n",
              "      <td>-1.107720</td>\n",
              "      <td>-4.441068</td>\n",
              "      <td>-4.955767</td>\n",
              "      <td>-4.737144</td>\n",
              "      <td>-2.706756</td>\n",
              "      <td>-8.366491</td>\n",
              "      <td>-2.773106</td>\n",
              "      <td>-4.010757</td>\n",
              "      <td>-3.787622</td>\n",
              "      <td>-4.050997</td>\n",
              "      <td>-1.320769</td>\n",
              "      <td>-4.180398</td>\n",
              "      <td>-0.914175</td>\n",
              "      <td>-3.937309</td>\n",
              "      <td>-2.184175</td>\n",
              "      <td>-2.725434</td>\n",
              "      <td>-0.365667</td>\n",
              "      <td>-0.593917</td>\n",
              "      <td>2.263465</td>\n",
              "      <td>3.600143</td>\n",
              "      <td>5.182899</td>\n",
              "      <td>6.436582</td>\n",
              "      <td>6.961022</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-694.004333</td>\n",
              "      <td>61.496510</td>\n",
              "      <td>-3.262744</td>\n",
              "      <td>16.971298</td>\n",
              "      <td>2.142968</td>\n",
              "      <td>4.266798</td>\n",
              "      <td>-5.015399</td>\n",
              "      <td>-2.617859</td>\n",
              "      <td>-12.885774</td>\n",
              "      <td>-1.302279</td>\n",
              "      <td>-1.563737</td>\n",
              "      <td>-0.917418</td>\n",
              "      <td>0.153905</td>\n",
              "      <td>-1.951386</td>\n",
              "      <td>-3.258693</td>\n",
              "      <td>1.956144</td>\n",
              "      <td>-5.813616</td>\n",
              "      <td>-2.408270</td>\n",
              "      <td>-0.432048</td>\n",
              "      <td>0.449308</td>\n",
              "      <td>-5.577334</td>\n",
              "      <td>-0.920122</td>\n",
              "      <td>-1.964510</td>\n",
              "      <td>-3.439345</td>\n",
              "      <td>-1.153978</td>\n",
              "      <td>-0.557548</td>\n",
              "      <td>-3.899142</td>\n",
              "      <td>-0.646266</td>\n",
              "      <td>-1.816866</td>\n",
              "      <td>-0.701520</td>\n",
              "      <td>-1.382858</td>\n",
              "      <td>-2.590943</td>\n",
              "      <td>-1.929074</td>\n",
              "      <td>-2.390322</td>\n",
              "      <td>-2.269381</td>\n",
              "      <td>-2.486079</td>\n",
              "      <td>-0.589257</td>\n",
              "      <td>-3.248326</td>\n",
              "      <td>-2.979813</td>\n",
              "      <td>-2.769281</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-710.096069</td>\n",
              "      <td>40.656712</td>\n",
              "      <td>-13.243647</td>\n",
              "      <td>14.821146</td>\n",
              "      <td>-11.988942</td>\n",
              "      <td>-4.326802</td>\n",
              "      <td>-11.298809</td>\n",
              "      <td>-7.587059</td>\n",
              "      <td>-9.825305</td>\n",
              "      <td>-0.084983</td>\n",
              "      <td>-8.675142</td>\n",
              "      <td>-1.556004</td>\n",
              "      <td>-8.769038</td>\n",
              "      <td>2.118348</td>\n",
              "      <td>-9.668043</td>\n",
              "      <td>-3.032947</td>\n",
              "      <td>-5.550335</td>\n",
              "      <td>-2.129647</td>\n",
              "      <td>-3.414623</td>\n",
              "      <td>-1.301786</td>\n",
              "      <td>-0.036070</td>\n",
              "      <td>1.410935</td>\n",
              "      <td>-2.667373</td>\n",
              "      <td>1.127486</td>\n",
              "      <td>-1.977145</td>\n",
              "      <td>1.986175</td>\n",
              "      <td>4.099287</td>\n",
              "      <td>4.674964</td>\n",
              "      <td>2.503084</td>\n",
              "      <td>2.805913</td>\n",
              "      <td>-0.246652</td>\n",
              "      <td>0.444371</td>\n",
              "      <td>-0.521537</td>\n",
              "      <td>0.362264</td>\n",
              "      <td>0.854505</td>\n",
              "      <td>0.650243</td>\n",
              "      <td>-0.423510</td>\n",
              "      <td>-1.529283</td>\n",
              "      <td>-1.998029</td>\n",
              "      <td>-0.062299</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-600.512085</td>\n",
              "      <td>73.287621</td>\n",
              "      <td>5.885289</td>\n",
              "      <td>19.855776</td>\n",
              "      <td>5.840355</td>\n",
              "      <td>-0.147993</td>\n",
              "      <td>1.414411</td>\n",
              "      <td>-2.404692</td>\n",
              "      <td>-10.087945</td>\n",
              "      <td>2.899282</td>\n",
              "      <td>-1.044273</td>\n",
              "      <td>-0.664250</td>\n",
              "      <td>-1.161344</td>\n",
              "      <td>4.153498</td>\n",
              "      <td>-1.920255</td>\n",
              "      <td>0.318724</td>\n",
              "      <td>-2.219993</td>\n",
              "      <td>0.929482</td>\n",
              "      <td>0.388972</td>\n",
              "      <td>-0.788825</td>\n",
              "      <td>-2.326617</td>\n",
              "      <td>0.063424</td>\n",
              "      <td>-2.413080</td>\n",
              "      <td>-3.108146</td>\n",
              "      <td>-1.687048</td>\n",
              "      <td>-1.630714</td>\n",
              "      <td>-2.334900</td>\n",
              "      <td>-1.168309</td>\n",
              "      <td>-0.079526</td>\n",
              "      <td>0.794789</td>\n",
              "      <td>-0.770868</td>\n",
              "      <td>-1.369582</td>\n",
              "      <td>-1.891460</td>\n",
              "      <td>-1.157018</td>\n",
              "      <td>-1.040969</td>\n",
              "      <td>-1.041425</td>\n",
              "      <td>-2.091770</td>\n",
              "      <td>-2.694788</td>\n",
              "      <td>-2.069784</td>\n",
              "      <td>-1.805427</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2563</th>\n",
              "      <td>-578.713196</td>\n",
              "      <td>61.370003</td>\n",
              "      <td>-8.907619</td>\n",
              "      <td>21.265520</td>\n",
              "      <td>5.554171</td>\n",
              "      <td>-2.427649</td>\n",
              "      <td>-2.837762</td>\n",
              "      <td>0.677961</td>\n",
              "      <td>-12.619606</td>\n",
              "      <td>7.729056</td>\n",
              "      <td>-2.592522</td>\n",
              "      <td>-0.465079</td>\n",
              "      <td>-3.177077</td>\n",
              "      <td>3.720269</td>\n",
              "      <td>-3.148828</td>\n",
              "      <td>1.093849</td>\n",
              "      <td>-7.312330</td>\n",
              "      <td>1.868895</td>\n",
              "      <td>-4.143651</td>\n",
              "      <td>-0.830634</td>\n",
              "      <td>-4.200087</td>\n",
              "      <td>1.502687</td>\n",
              "      <td>-6.224370</td>\n",
              "      <td>-1.663106</td>\n",
              "      <td>-2.695183</td>\n",
              "      <td>-2.355363</td>\n",
              "      <td>-1.806027</td>\n",
              "      <td>-0.406772</td>\n",
              "      <td>-0.936953</td>\n",
              "      <td>-0.639671</td>\n",
              "      <td>-1.645374</td>\n",
              "      <td>-0.923842</td>\n",
              "      <td>-2.112805</td>\n",
              "      <td>-1.375529</td>\n",
              "      <td>-1.862561</td>\n",
              "      <td>-0.290023</td>\n",
              "      <td>-2.385124</td>\n",
              "      <td>-1.649704</td>\n",
              "      <td>-2.507972</td>\n",
              "      <td>-1.427291</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2564</th>\n",
              "      <td>-576.611877</td>\n",
              "      <td>41.392681</td>\n",
              "      <td>-17.155254</td>\n",
              "      <td>6.986942</td>\n",
              "      <td>-9.122391</td>\n",
              "      <td>-3.664443</td>\n",
              "      <td>-20.467916</td>\n",
              "      <td>-3.572575</td>\n",
              "      <td>-14.322601</td>\n",
              "      <td>2.762153</td>\n",
              "      <td>-10.094601</td>\n",
              "      <td>-6.661758</td>\n",
              "      <td>-2.747277</td>\n",
              "      <td>-7.654290</td>\n",
              "      <td>-2.902933</td>\n",
              "      <td>-5.420545</td>\n",
              "      <td>-9.148721</td>\n",
              "      <td>-1.615915</td>\n",
              "      <td>-7.005185</td>\n",
              "      <td>-4.869830</td>\n",
              "      <td>-6.075991</td>\n",
              "      <td>-3.604834</td>\n",
              "      <td>-7.162820</td>\n",
              "      <td>-3.446610</td>\n",
              "      <td>-5.309924</td>\n",
              "      <td>0.403759</td>\n",
              "      <td>-0.442520</td>\n",
              "      <td>3.125771</td>\n",
              "      <td>4.213610</td>\n",
              "      <td>3.124016</td>\n",
              "      <td>4.282145</td>\n",
              "      <td>3.382751</td>\n",
              "      <td>3.443053</td>\n",
              "      <td>4.978160</td>\n",
              "      <td>5.427543</td>\n",
              "      <td>5.740356</td>\n",
              "      <td>4.451098</td>\n",
              "      <td>2.878379</td>\n",
              "      <td>0.935435</td>\n",
              "      <td>2.211232</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2565</th>\n",
              "      <td>-513.391968</td>\n",
              "      <td>35.304344</td>\n",
              "      <td>-27.528683</td>\n",
              "      <td>-1.573403</td>\n",
              "      <td>-19.190140</td>\n",
              "      <td>-7.995622</td>\n",
              "      <td>-18.173841</td>\n",
              "      <td>-11.630806</td>\n",
              "      <td>-8.921432</td>\n",
              "      <td>-0.404055</td>\n",
              "      <td>-10.451320</td>\n",
              "      <td>0.513336</td>\n",
              "      <td>-8.950840</td>\n",
              "      <td>-2.935175</td>\n",
              "      <td>-5.182651</td>\n",
              "      <td>-6.297908</td>\n",
              "      <td>-8.257548</td>\n",
              "      <td>-5.361291</td>\n",
              "      <td>-6.544790</td>\n",
              "      <td>-0.453004</td>\n",
              "      <td>0.816983</td>\n",
              "      <td>1.280731</td>\n",
              "      <td>2.082585</td>\n",
              "      <td>1.808482</td>\n",
              "      <td>-0.245074</td>\n",
              "      <td>0.945363</td>\n",
              "      <td>0.798398</td>\n",
              "      <td>3.540350</td>\n",
              "      <td>4.012304</td>\n",
              "      <td>3.273612</td>\n",
              "      <td>2.815805</td>\n",
              "      <td>1.460280</td>\n",
              "      <td>-0.774430</td>\n",
              "      <td>0.655724</td>\n",
              "      <td>0.936522</td>\n",
              "      <td>1.376725</td>\n",
              "      <td>-0.514725</td>\n",
              "      <td>-2.646844</td>\n",
              "      <td>-2.270415</td>\n",
              "      <td>0.339581</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2566</th>\n",
              "      <td>-503.895874</td>\n",
              "      <td>46.516319</td>\n",
              "      <td>-5.204802</td>\n",
              "      <td>3.634700</td>\n",
              "      <td>-7.994336</td>\n",
              "      <td>1.957494</td>\n",
              "      <td>-17.072124</td>\n",
              "      <td>-12.035769</td>\n",
              "      <td>-11.504045</td>\n",
              "      <td>-2.664160</td>\n",
              "      <td>-9.096724</td>\n",
              "      <td>-3.757857</td>\n",
              "      <td>-9.150162</td>\n",
              "      <td>-4.733320</td>\n",
              "      <td>-6.719837</td>\n",
              "      <td>0.308393</td>\n",
              "      <td>-8.646714</td>\n",
              "      <td>-2.159988</td>\n",
              "      <td>-4.451622</td>\n",
              "      <td>-3.077097</td>\n",
              "      <td>-6.187653</td>\n",
              "      <td>-3.097657</td>\n",
              "      <td>-3.816869</td>\n",
              "      <td>-3.805601</td>\n",
              "      <td>-4.331494</td>\n",
              "      <td>-1.828928</td>\n",
              "      <td>-3.806374</td>\n",
              "      <td>-0.731557</td>\n",
              "      <td>-0.000963</td>\n",
              "      <td>1.330716</td>\n",
              "      <td>1.240385</td>\n",
              "      <td>0.871198</td>\n",
              "      <td>-0.147335</td>\n",
              "      <td>-0.468855</td>\n",
              "      <td>0.425717</td>\n",
              "      <td>2.372419</td>\n",
              "      <td>0.957126</td>\n",
              "      <td>-0.369405</td>\n",
              "      <td>-0.379200</td>\n",
              "      <td>-1.097025</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2567</th>\n",
              "      <td>-560.837097</td>\n",
              "      <td>50.168743</td>\n",
              "      <td>-19.705780</td>\n",
              "      <td>4.857205</td>\n",
              "      <td>-14.152937</td>\n",
              "      <td>-1.806413</td>\n",
              "      <td>-14.607023</td>\n",
              "      <td>-10.531888</td>\n",
              "      <td>-8.830826</td>\n",
              "      <td>1.842284</td>\n",
              "      <td>-10.997461</td>\n",
              "      <td>3.024669</td>\n",
              "      <td>-11.034261</td>\n",
              "      <td>0.173616</td>\n",
              "      <td>-5.544483</td>\n",
              "      <td>-4.531592</td>\n",
              "      <td>-1.218367</td>\n",
              "      <td>-4.226231</td>\n",
              "      <td>-7.439675</td>\n",
              "      <td>-2.918094</td>\n",
              "      <td>-4.972703</td>\n",
              "      <td>-4.242082</td>\n",
              "      <td>-4.365048</td>\n",
              "      <td>-2.168633</td>\n",
              "      <td>-3.950434</td>\n",
              "      <td>-0.162558</td>\n",
              "      <td>2.696079</td>\n",
              "      <td>3.357330</td>\n",
              "      <td>4.228732</td>\n",
              "      <td>5.612514</td>\n",
              "      <td>6.008360</td>\n",
              "      <td>8.070698</td>\n",
              "      <td>5.162092</td>\n",
              "      <td>4.522226</td>\n",
              "      <td>2.342329</td>\n",
              "      <td>0.923058</td>\n",
              "      <td>1.369484</td>\n",
              "      <td>1.216854</td>\n",
              "      <td>0.766931</td>\n",
              "      <td>0.479213</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2568 rows  41 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0          1          2  ...        38        39  labels\n",
              "0    -648.970947  71.907417   2.742204  ... -0.405711 -1.539037       4\n",
              "1    -745.476807  61.395737   1.011072  ...  6.436582  6.961022       4\n",
              "2    -694.004333  61.496510  -3.262744  ... -2.979813 -2.769281       4\n",
              "3    -710.096069  40.656712 -13.243647  ... -1.998029 -0.062299       4\n",
              "4    -600.512085  73.287621   5.885289  ... -2.069784 -1.805427       4\n",
              "...          ...        ...        ...  ...       ...       ...     ...\n",
              "2563 -578.713196  61.370003  -8.907619  ... -2.507972 -1.427291       1\n",
              "2564 -576.611877  41.392681 -17.155254  ...  0.935435  2.211232       1\n",
              "2565 -513.391968  35.304344 -27.528683  ... -2.270415  0.339581       1\n",
              "2566 -503.895874  46.516319  -5.204802  ... -0.379200 -1.097025       1\n",
              "2567 -560.837097  50.168743 -19.705780  ...  0.766931  0.479213       1\n",
              "\n",
              "[2568 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "dddx-MI6wCzV",
        "outputId": "e3af3f6b-e1ce-4e68-ff49-b1596740f3d9"
      },
      "source": [
        "newDataFrame = df\n",
        "newDataFrame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-648.970947</td>\n",
              "      <td>71.907417</td>\n",
              "      <td>2.742204</td>\n",
              "      <td>21.670790</td>\n",
              "      <td>10.711693</td>\n",
              "      <td>8.265316</td>\n",
              "      <td>7.268557</td>\n",
              "      <td>2.156604</td>\n",
              "      <td>-6.138276</td>\n",
              "      <td>7.114227</td>\n",
              "      <td>-0.638948</td>\n",
              "      <td>2.334098</td>\n",
              "      <td>4.068620</td>\n",
              "      <td>3.934223</td>\n",
              "      <td>-0.716825</td>\n",
              "      <td>1.969140</td>\n",
              "      <td>-1.288988</td>\n",
              "      <td>-0.913698</td>\n",
              "      <td>-0.759566</td>\n",
              "      <td>2.153710</td>\n",
              "      <td>-2.214774</td>\n",
              "      <td>1.103560</td>\n",
              "      <td>-1.414614</td>\n",
              "      <td>1.267050</td>\n",
              "      <td>-0.493648</td>\n",
              "      <td>0.134097</td>\n",
              "      <td>-1.264701</td>\n",
              "      <td>1.224581</td>\n",
              "      <td>-2.835206</td>\n",
              "      <td>-0.283147</td>\n",
              "      <td>-0.060137</td>\n",
              "      <td>-0.057659</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.087855</td>\n",
              "      <td>-1.292643</td>\n",
              "      <td>-0.410933</td>\n",
              "      <td>0.224731</td>\n",
              "      <td>-0.935340</td>\n",
              "      <td>-0.405711</td>\n",
              "      <td>-1.539037</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-745.476807</td>\n",
              "      <td>61.395737</td>\n",
              "      <td>1.011072</td>\n",
              "      <td>12.793580</td>\n",
              "      <td>-4.854984</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>-6.133131</td>\n",
              "      <td>-9.800947</td>\n",
              "      <td>-9.225055</td>\n",
              "      <td>0.980532</td>\n",
              "      <td>-5.173116</td>\n",
              "      <td>-2.589133</td>\n",
              "      <td>-9.412117</td>\n",
              "      <td>4.176839</td>\n",
              "      <td>-5.030715</td>\n",
              "      <td>-5.789496</td>\n",
              "      <td>-5.578049</td>\n",
              "      <td>-1.107720</td>\n",
              "      <td>-4.441068</td>\n",
              "      <td>-4.955767</td>\n",
              "      <td>-4.737144</td>\n",
              "      <td>-2.706756</td>\n",
              "      <td>-8.366491</td>\n",
              "      <td>-2.773106</td>\n",
              "      <td>-4.010757</td>\n",
              "      <td>-3.787622</td>\n",
              "      <td>-4.050997</td>\n",
              "      <td>-1.320769</td>\n",
              "      <td>-4.180398</td>\n",
              "      <td>-0.914175</td>\n",
              "      <td>-3.937309</td>\n",
              "      <td>-2.184175</td>\n",
              "      <td>-2.725434</td>\n",
              "      <td>-0.365667</td>\n",
              "      <td>-0.593917</td>\n",
              "      <td>2.263465</td>\n",
              "      <td>3.600143</td>\n",
              "      <td>5.182899</td>\n",
              "      <td>6.436582</td>\n",
              "      <td>6.961022</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-694.004333</td>\n",
              "      <td>61.496510</td>\n",
              "      <td>-3.262744</td>\n",
              "      <td>16.971298</td>\n",
              "      <td>2.142968</td>\n",
              "      <td>4.266798</td>\n",
              "      <td>-5.015399</td>\n",
              "      <td>-2.617859</td>\n",
              "      <td>-12.885774</td>\n",
              "      <td>-1.302279</td>\n",
              "      <td>-1.563737</td>\n",
              "      <td>-0.917418</td>\n",
              "      <td>0.153905</td>\n",
              "      <td>-1.951386</td>\n",
              "      <td>-3.258693</td>\n",
              "      <td>1.956144</td>\n",
              "      <td>-5.813616</td>\n",
              "      <td>-2.408270</td>\n",
              "      <td>-0.432048</td>\n",
              "      <td>0.449308</td>\n",
              "      <td>-5.577334</td>\n",
              "      <td>-0.920122</td>\n",
              "      <td>-1.964510</td>\n",
              "      <td>-3.439345</td>\n",
              "      <td>-1.153978</td>\n",
              "      <td>-0.557548</td>\n",
              "      <td>-3.899142</td>\n",
              "      <td>-0.646266</td>\n",
              "      <td>-1.816866</td>\n",
              "      <td>-0.701520</td>\n",
              "      <td>-1.382858</td>\n",
              "      <td>-2.590943</td>\n",
              "      <td>-1.929074</td>\n",
              "      <td>-2.390322</td>\n",
              "      <td>-2.269381</td>\n",
              "      <td>-2.486079</td>\n",
              "      <td>-0.589257</td>\n",
              "      <td>-3.248326</td>\n",
              "      <td>-2.979813</td>\n",
              "      <td>-2.769281</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-710.096069</td>\n",
              "      <td>40.656712</td>\n",
              "      <td>-13.243647</td>\n",
              "      <td>14.821146</td>\n",
              "      <td>-11.988942</td>\n",
              "      <td>-4.326802</td>\n",
              "      <td>-11.298809</td>\n",
              "      <td>-7.587059</td>\n",
              "      <td>-9.825305</td>\n",
              "      <td>-0.084983</td>\n",
              "      <td>-8.675142</td>\n",
              "      <td>-1.556004</td>\n",
              "      <td>-8.769038</td>\n",
              "      <td>2.118348</td>\n",
              "      <td>-9.668043</td>\n",
              "      <td>-3.032947</td>\n",
              "      <td>-5.550335</td>\n",
              "      <td>-2.129647</td>\n",
              "      <td>-3.414623</td>\n",
              "      <td>-1.301786</td>\n",
              "      <td>-0.036070</td>\n",
              "      <td>1.410935</td>\n",
              "      <td>-2.667373</td>\n",
              "      <td>1.127486</td>\n",
              "      <td>-1.977145</td>\n",
              "      <td>1.986175</td>\n",
              "      <td>4.099287</td>\n",
              "      <td>4.674964</td>\n",
              "      <td>2.503084</td>\n",
              "      <td>2.805913</td>\n",
              "      <td>-0.246652</td>\n",
              "      <td>0.444371</td>\n",
              "      <td>-0.521537</td>\n",
              "      <td>0.362264</td>\n",
              "      <td>0.854505</td>\n",
              "      <td>0.650243</td>\n",
              "      <td>-0.423510</td>\n",
              "      <td>-1.529283</td>\n",
              "      <td>-1.998029</td>\n",
              "      <td>-0.062299</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-600.512085</td>\n",
              "      <td>73.287621</td>\n",
              "      <td>5.885289</td>\n",
              "      <td>19.855776</td>\n",
              "      <td>5.840355</td>\n",
              "      <td>-0.147993</td>\n",
              "      <td>1.414411</td>\n",
              "      <td>-2.404692</td>\n",
              "      <td>-10.087945</td>\n",
              "      <td>2.899282</td>\n",
              "      <td>-1.044273</td>\n",
              "      <td>-0.664250</td>\n",
              "      <td>-1.161344</td>\n",
              "      <td>4.153498</td>\n",
              "      <td>-1.920255</td>\n",
              "      <td>0.318724</td>\n",
              "      <td>-2.219993</td>\n",
              "      <td>0.929482</td>\n",
              "      <td>0.388972</td>\n",
              "      <td>-0.788825</td>\n",
              "      <td>-2.326617</td>\n",
              "      <td>0.063424</td>\n",
              "      <td>-2.413080</td>\n",
              "      <td>-3.108146</td>\n",
              "      <td>-1.687048</td>\n",
              "      <td>-1.630714</td>\n",
              "      <td>-2.334900</td>\n",
              "      <td>-1.168309</td>\n",
              "      <td>-0.079526</td>\n",
              "      <td>0.794789</td>\n",
              "      <td>-0.770868</td>\n",
              "      <td>-1.369582</td>\n",
              "      <td>-1.891460</td>\n",
              "      <td>-1.157018</td>\n",
              "      <td>-1.040969</td>\n",
              "      <td>-1.041425</td>\n",
              "      <td>-2.091770</td>\n",
              "      <td>-2.694788</td>\n",
              "      <td>-2.069784</td>\n",
              "      <td>-1.805427</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2563</th>\n",
              "      <td>-578.713196</td>\n",
              "      <td>61.370003</td>\n",
              "      <td>-8.907619</td>\n",
              "      <td>21.265520</td>\n",
              "      <td>5.554171</td>\n",
              "      <td>-2.427649</td>\n",
              "      <td>-2.837762</td>\n",
              "      <td>0.677961</td>\n",
              "      <td>-12.619606</td>\n",
              "      <td>7.729056</td>\n",
              "      <td>-2.592522</td>\n",
              "      <td>-0.465079</td>\n",
              "      <td>-3.177077</td>\n",
              "      <td>3.720269</td>\n",
              "      <td>-3.148828</td>\n",
              "      <td>1.093849</td>\n",
              "      <td>-7.312330</td>\n",
              "      <td>1.868895</td>\n",
              "      <td>-4.143651</td>\n",
              "      <td>-0.830634</td>\n",
              "      <td>-4.200087</td>\n",
              "      <td>1.502687</td>\n",
              "      <td>-6.224370</td>\n",
              "      <td>-1.663106</td>\n",
              "      <td>-2.695183</td>\n",
              "      <td>-2.355363</td>\n",
              "      <td>-1.806027</td>\n",
              "      <td>-0.406772</td>\n",
              "      <td>-0.936953</td>\n",
              "      <td>-0.639671</td>\n",
              "      <td>-1.645374</td>\n",
              "      <td>-0.923842</td>\n",
              "      <td>-2.112805</td>\n",
              "      <td>-1.375529</td>\n",
              "      <td>-1.862561</td>\n",
              "      <td>-0.290023</td>\n",
              "      <td>-2.385124</td>\n",
              "      <td>-1.649704</td>\n",
              "      <td>-2.507972</td>\n",
              "      <td>-1.427291</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2564</th>\n",
              "      <td>-576.611877</td>\n",
              "      <td>41.392681</td>\n",
              "      <td>-17.155254</td>\n",
              "      <td>6.986942</td>\n",
              "      <td>-9.122391</td>\n",
              "      <td>-3.664443</td>\n",
              "      <td>-20.467916</td>\n",
              "      <td>-3.572575</td>\n",
              "      <td>-14.322601</td>\n",
              "      <td>2.762153</td>\n",
              "      <td>-10.094601</td>\n",
              "      <td>-6.661758</td>\n",
              "      <td>-2.747277</td>\n",
              "      <td>-7.654290</td>\n",
              "      <td>-2.902933</td>\n",
              "      <td>-5.420545</td>\n",
              "      <td>-9.148721</td>\n",
              "      <td>-1.615915</td>\n",
              "      <td>-7.005185</td>\n",
              "      <td>-4.869830</td>\n",
              "      <td>-6.075991</td>\n",
              "      <td>-3.604834</td>\n",
              "      <td>-7.162820</td>\n",
              "      <td>-3.446610</td>\n",
              "      <td>-5.309924</td>\n",
              "      <td>0.403759</td>\n",
              "      <td>-0.442520</td>\n",
              "      <td>3.125771</td>\n",
              "      <td>4.213610</td>\n",
              "      <td>3.124016</td>\n",
              "      <td>4.282145</td>\n",
              "      <td>3.382751</td>\n",
              "      <td>3.443053</td>\n",
              "      <td>4.978160</td>\n",
              "      <td>5.427543</td>\n",
              "      <td>5.740356</td>\n",
              "      <td>4.451098</td>\n",
              "      <td>2.878379</td>\n",
              "      <td>0.935435</td>\n",
              "      <td>2.211232</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2565</th>\n",
              "      <td>-513.391968</td>\n",
              "      <td>35.304344</td>\n",
              "      <td>-27.528683</td>\n",
              "      <td>-1.573403</td>\n",
              "      <td>-19.190140</td>\n",
              "      <td>-7.995622</td>\n",
              "      <td>-18.173841</td>\n",
              "      <td>-11.630806</td>\n",
              "      <td>-8.921432</td>\n",
              "      <td>-0.404055</td>\n",
              "      <td>-10.451320</td>\n",
              "      <td>0.513336</td>\n",
              "      <td>-8.950840</td>\n",
              "      <td>-2.935175</td>\n",
              "      <td>-5.182651</td>\n",
              "      <td>-6.297908</td>\n",
              "      <td>-8.257548</td>\n",
              "      <td>-5.361291</td>\n",
              "      <td>-6.544790</td>\n",
              "      <td>-0.453004</td>\n",
              "      <td>0.816983</td>\n",
              "      <td>1.280731</td>\n",
              "      <td>2.082585</td>\n",
              "      <td>1.808482</td>\n",
              "      <td>-0.245074</td>\n",
              "      <td>0.945363</td>\n",
              "      <td>0.798398</td>\n",
              "      <td>3.540350</td>\n",
              "      <td>4.012304</td>\n",
              "      <td>3.273612</td>\n",
              "      <td>2.815805</td>\n",
              "      <td>1.460280</td>\n",
              "      <td>-0.774430</td>\n",
              "      <td>0.655724</td>\n",
              "      <td>0.936522</td>\n",
              "      <td>1.376725</td>\n",
              "      <td>-0.514725</td>\n",
              "      <td>-2.646844</td>\n",
              "      <td>-2.270415</td>\n",
              "      <td>0.339581</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2566</th>\n",
              "      <td>-503.895874</td>\n",
              "      <td>46.516319</td>\n",
              "      <td>-5.204802</td>\n",
              "      <td>3.634700</td>\n",
              "      <td>-7.994336</td>\n",
              "      <td>1.957494</td>\n",
              "      <td>-17.072124</td>\n",
              "      <td>-12.035769</td>\n",
              "      <td>-11.504045</td>\n",
              "      <td>-2.664160</td>\n",
              "      <td>-9.096724</td>\n",
              "      <td>-3.757857</td>\n",
              "      <td>-9.150162</td>\n",
              "      <td>-4.733320</td>\n",
              "      <td>-6.719837</td>\n",
              "      <td>0.308393</td>\n",
              "      <td>-8.646714</td>\n",
              "      <td>-2.159988</td>\n",
              "      <td>-4.451622</td>\n",
              "      <td>-3.077097</td>\n",
              "      <td>-6.187653</td>\n",
              "      <td>-3.097657</td>\n",
              "      <td>-3.816869</td>\n",
              "      <td>-3.805601</td>\n",
              "      <td>-4.331494</td>\n",
              "      <td>-1.828928</td>\n",
              "      <td>-3.806374</td>\n",
              "      <td>-0.731557</td>\n",
              "      <td>-0.000963</td>\n",
              "      <td>1.330716</td>\n",
              "      <td>1.240385</td>\n",
              "      <td>0.871198</td>\n",
              "      <td>-0.147335</td>\n",
              "      <td>-0.468855</td>\n",
              "      <td>0.425717</td>\n",
              "      <td>2.372419</td>\n",
              "      <td>0.957126</td>\n",
              "      <td>-0.369405</td>\n",
              "      <td>-0.379200</td>\n",
              "      <td>-1.097025</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2567</th>\n",
              "      <td>-560.837097</td>\n",
              "      <td>50.168743</td>\n",
              "      <td>-19.705780</td>\n",
              "      <td>4.857205</td>\n",
              "      <td>-14.152937</td>\n",
              "      <td>-1.806413</td>\n",
              "      <td>-14.607023</td>\n",
              "      <td>-10.531888</td>\n",
              "      <td>-8.830826</td>\n",
              "      <td>1.842284</td>\n",
              "      <td>-10.997461</td>\n",
              "      <td>3.024669</td>\n",
              "      <td>-11.034261</td>\n",
              "      <td>0.173616</td>\n",
              "      <td>-5.544483</td>\n",
              "      <td>-4.531592</td>\n",
              "      <td>-1.218367</td>\n",
              "      <td>-4.226231</td>\n",
              "      <td>-7.439675</td>\n",
              "      <td>-2.918094</td>\n",
              "      <td>-4.972703</td>\n",
              "      <td>-4.242082</td>\n",
              "      <td>-4.365048</td>\n",
              "      <td>-2.168633</td>\n",
              "      <td>-3.950434</td>\n",
              "      <td>-0.162558</td>\n",
              "      <td>2.696079</td>\n",
              "      <td>3.357330</td>\n",
              "      <td>4.228732</td>\n",
              "      <td>5.612514</td>\n",
              "      <td>6.008360</td>\n",
              "      <td>8.070698</td>\n",
              "      <td>5.162092</td>\n",
              "      <td>4.522226</td>\n",
              "      <td>2.342329</td>\n",
              "      <td>0.923058</td>\n",
              "      <td>1.369484</td>\n",
              "      <td>1.216854</td>\n",
              "      <td>0.766931</td>\n",
              "      <td>0.479213</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2568 rows  41 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0          1          2  ...        38        39  labels\n",
              "0    -648.970947  71.907417   2.742204  ... -0.405711 -1.539037       4\n",
              "1    -745.476807  61.395737   1.011072  ...  6.436582  6.961022       4\n",
              "2    -694.004333  61.496510  -3.262744  ... -2.979813 -2.769281       4\n",
              "3    -710.096069  40.656712 -13.243647  ... -1.998029 -0.062299       4\n",
              "4    -600.512085  73.287621   5.885289  ... -2.069784 -1.805427       4\n",
              "...          ...        ...        ...  ...       ...       ...     ...\n",
              "2563 -578.713196  61.370003  -8.907619  ... -2.507972 -1.427291       1\n",
              "2564 -576.611877  41.392681 -17.155254  ...  0.935435  2.211232       1\n",
              "2565 -513.391968  35.304344 -27.528683  ... -2.270415  0.339581       1\n",
              "2566 -503.895874  46.516319  -5.204802  ... -0.379200 -1.097025       1\n",
              "2567 -560.837097  50.168743 -19.705780  ...  0.766931  0.479213       1\n",
              "\n",
              "[2568 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "2sMe2TmawHjT",
        "outputId": "7b8a5546-db1c-4ab7-cd98-53abd1fbe563"
      },
      "source": [
        "import sklearn\n",
        "shuffled_df = sklearn.utils.shuffle(df)\n",
        "shuffled_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>-825.414673</td>\n",
              "      <td>60.533543</td>\n",
              "      <td>6.114007</td>\n",
              "      <td>22.442820</td>\n",
              "      <td>8.682250</td>\n",
              "      <td>12.127042</td>\n",
              "      <td>-7.129900</td>\n",
              "      <td>-3.363456</td>\n",
              "      <td>-2.776660</td>\n",
              "      <td>3.205381</td>\n",
              "      <td>-2.205625</td>\n",
              "      <td>3.076697</td>\n",
              "      <td>0.352401</td>\n",
              "      <td>0.712972</td>\n",
              "      <td>-3.023001</td>\n",
              "      <td>1.563250</td>\n",
              "      <td>0.166959</td>\n",
              "      <td>-1.877100</td>\n",
              "      <td>0.372673</td>\n",
              "      <td>1.767398</td>\n",
              "      <td>-1.637978</td>\n",
              "      <td>-1.397981</td>\n",
              "      <td>-4.431309</td>\n",
              "      <td>-0.144106</td>\n",
              "      <td>-2.466515</td>\n",
              "      <td>0.560809</td>\n",
              "      <td>-1.322437</td>\n",
              "      <td>-0.638741</td>\n",
              "      <td>-0.768000</td>\n",
              "      <td>-0.276426</td>\n",
              "      <td>-0.550817</td>\n",
              "      <td>-1.549996</td>\n",
              "      <td>-0.950785</td>\n",
              "      <td>-1.204997</td>\n",
              "      <td>-0.397445</td>\n",
              "      <td>-0.152079</td>\n",
              "      <td>-0.329717</td>\n",
              "      <td>-1.176098</td>\n",
              "      <td>-2.586861</td>\n",
              "      <td>-1.945768</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>-363.945038</td>\n",
              "      <td>132.844086</td>\n",
              "      <td>18.439739</td>\n",
              "      <td>41.831051</td>\n",
              "      <td>-1.238362</td>\n",
              "      <td>-10.838084</td>\n",
              "      <td>-2.721811</td>\n",
              "      <td>-5.487784</td>\n",
              "      <td>-14.612775</td>\n",
              "      <td>-2.332597</td>\n",
              "      <td>-1.363916</td>\n",
              "      <td>-2.175982</td>\n",
              "      <td>-2.970377</td>\n",
              "      <td>-3.081370</td>\n",
              "      <td>5.915540</td>\n",
              "      <td>2.661922</td>\n",
              "      <td>-2.749998</td>\n",
              "      <td>-2.856540</td>\n",
              "      <td>-1.961338</td>\n",
              "      <td>-5.204691</td>\n",
              "      <td>-2.508525</td>\n",
              "      <td>1.265571</td>\n",
              "      <td>-0.145492</td>\n",
              "      <td>-3.125991</td>\n",
              "      <td>-1.386497</td>\n",
              "      <td>-0.560353</td>\n",
              "      <td>-0.372894</td>\n",
              "      <td>0.948636</td>\n",
              "      <td>-1.577159</td>\n",
              "      <td>-2.073719</td>\n",
              "      <td>0.184617</td>\n",
              "      <td>0.511796</td>\n",
              "      <td>0.028943</td>\n",
              "      <td>-0.172511</td>\n",
              "      <td>0.542554</td>\n",
              "      <td>0.723892</td>\n",
              "      <td>0.219070</td>\n",
              "      <td>0.134097</td>\n",
              "      <td>0.170139</td>\n",
              "      <td>0.447085</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2395</th>\n",
              "      <td>-465.725433</td>\n",
              "      <td>31.941385</td>\n",
              "      <td>-25.003664</td>\n",
              "      <td>4.907607</td>\n",
              "      <td>-19.889526</td>\n",
              "      <td>-9.170860</td>\n",
              "      <td>-9.586270</td>\n",
              "      <td>-7.387171</td>\n",
              "      <td>-13.214993</td>\n",
              "      <td>2.549872</td>\n",
              "      <td>-14.778431</td>\n",
              "      <td>5.845198</td>\n",
              "      <td>-7.229172</td>\n",
              "      <td>-5.142927</td>\n",
              "      <td>-1.738818</td>\n",
              "      <td>-7.529268</td>\n",
              "      <td>-5.103505</td>\n",
              "      <td>-2.644557</td>\n",
              "      <td>-5.231592</td>\n",
              "      <td>0.949450</td>\n",
              "      <td>-0.928979</td>\n",
              "      <td>0.367746</td>\n",
              "      <td>0.617664</td>\n",
              "      <td>4.046716</td>\n",
              "      <td>6.830860</td>\n",
              "      <td>9.474530</td>\n",
              "      <td>7.255089</td>\n",
              "      <td>4.313036</td>\n",
              "      <td>0.737455</td>\n",
              "      <td>-0.195848</td>\n",
              "      <td>1.585593</td>\n",
              "      <td>1.624621</td>\n",
              "      <td>0.186931</td>\n",
              "      <td>-1.141509</td>\n",
              "      <td>-1.663077</td>\n",
              "      <td>1.092556</td>\n",
              "      <td>0.946853</td>\n",
              "      <td>-0.870504</td>\n",
              "      <td>-1.804306</td>\n",
              "      <td>-0.360051</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>838</th>\n",
              "      <td>-595.746338</td>\n",
              "      <td>49.765644</td>\n",
              "      <td>-8.958530</td>\n",
              "      <td>12.199486</td>\n",
              "      <td>-3.776050</td>\n",
              "      <td>0.010150</td>\n",
              "      <td>-10.700888</td>\n",
              "      <td>-5.692644</td>\n",
              "      <td>-18.228153</td>\n",
              "      <td>0.660218</td>\n",
              "      <td>-5.705032</td>\n",
              "      <td>-3.009626</td>\n",
              "      <td>-3.356527</td>\n",
              "      <td>-3.385922</td>\n",
              "      <td>-7.317917</td>\n",
              "      <td>1.660827</td>\n",
              "      <td>-10.712868</td>\n",
              "      <td>-2.140275</td>\n",
              "      <td>-2.229816</td>\n",
              "      <td>-2.711436</td>\n",
              "      <td>-7.811651</td>\n",
              "      <td>-1.970351</td>\n",
              "      <td>-6.790730</td>\n",
              "      <td>-4.862625</td>\n",
              "      <td>-3.824659</td>\n",
              "      <td>-3.195015</td>\n",
              "      <td>-2.880560</td>\n",
              "      <td>-2.336787</td>\n",
              "      <td>-3.552398</td>\n",
              "      <td>-1.434603</td>\n",
              "      <td>-2.390821</td>\n",
              "      <td>-2.664303</td>\n",
              "      <td>-2.683732</td>\n",
              "      <td>-3.462965</td>\n",
              "      <td>-3.246841</td>\n",
              "      <td>-1.516502</td>\n",
              "      <td>0.032365</td>\n",
              "      <td>-0.552158</td>\n",
              "      <td>-0.681581</td>\n",
              "      <td>0.875309</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>676</th>\n",
              "      <td>-626.350830</td>\n",
              "      <td>61.393463</td>\n",
              "      <td>-14.373200</td>\n",
              "      <td>9.492665</td>\n",
              "      <td>-3.040496</td>\n",
              "      <td>-3.125919</td>\n",
              "      <td>-9.247561</td>\n",
              "      <td>-8.394060</td>\n",
              "      <td>-13.596136</td>\n",
              "      <td>0.447127</td>\n",
              "      <td>-8.957449</td>\n",
              "      <td>0.407861</td>\n",
              "      <td>-8.963594</td>\n",
              "      <td>-4.015021</td>\n",
              "      <td>1.375812</td>\n",
              "      <td>-5.278077</td>\n",
              "      <td>-8.105624</td>\n",
              "      <td>-0.869345</td>\n",
              "      <td>-6.277256</td>\n",
              "      <td>-1.990680</td>\n",
              "      <td>-2.267434</td>\n",
              "      <td>1.658722</td>\n",
              "      <td>-1.016263</td>\n",
              "      <td>3.804404</td>\n",
              "      <td>-0.447727</td>\n",
              "      <td>2.564703</td>\n",
              "      <td>2.049600</td>\n",
              "      <td>5.439877</td>\n",
              "      <td>6.862554</td>\n",
              "      <td>10.003140</td>\n",
              "      <td>10.753455</td>\n",
              "      <td>9.746710</td>\n",
              "      <td>5.667435</td>\n",
              "      <td>4.925363</td>\n",
              "      <td>2.985759</td>\n",
              "      <td>1.120952</td>\n",
              "      <td>1.602449</td>\n",
              "      <td>1.420502</td>\n",
              "      <td>0.355261</td>\n",
              "      <td>-1.133214</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>-625.132874</td>\n",
              "      <td>64.716370</td>\n",
              "      <td>-26.561125</td>\n",
              "      <td>19.475462</td>\n",
              "      <td>-2.377839</td>\n",
              "      <td>-10.179889</td>\n",
              "      <td>-9.272346</td>\n",
              "      <td>-11.643109</td>\n",
              "      <td>-11.622691</td>\n",
              "      <td>3.125078</td>\n",
              "      <td>-9.914325</td>\n",
              "      <td>-0.749009</td>\n",
              "      <td>-0.698782</td>\n",
              "      <td>-7.417189</td>\n",
              "      <td>-4.534481</td>\n",
              "      <td>0.047580</td>\n",
              "      <td>-13.477597</td>\n",
              "      <td>0.659697</td>\n",
              "      <td>-8.122779</td>\n",
              "      <td>-6.516964</td>\n",
              "      <td>-5.119418</td>\n",
              "      <td>-3.837847</td>\n",
              "      <td>-6.178879</td>\n",
              "      <td>-4.919551</td>\n",
              "      <td>-5.600528</td>\n",
              "      <td>0.569786</td>\n",
              "      <td>-4.730884</td>\n",
              "      <td>-0.564183</td>\n",
              "      <td>-2.898862</td>\n",
              "      <td>-3.993762</td>\n",
              "      <td>-4.394069</td>\n",
              "      <td>-0.959330</td>\n",
              "      <td>-1.092253</td>\n",
              "      <td>-0.538384</td>\n",
              "      <td>3.502781</td>\n",
              "      <td>1.669756</td>\n",
              "      <td>4.677682</td>\n",
              "      <td>6.247907</td>\n",
              "      <td>10.456415</td>\n",
              "      <td>12.188598</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2289</th>\n",
              "      <td>-424.692383</td>\n",
              "      <td>20.815155</td>\n",
              "      <td>-28.414587</td>\n",
              "      <td>0.583950</td>\n",
              "      <td>-18.214512</td>\n",
              "      <td>-10.805991</td>\n",
              "      <td>-17.283522</td>\n",
              "      <td>-11.428418</td>\n",
              "      <td>-13.334943</td>\n",
              "      <td>-0.924081</td>\n",
              "      <td>-15.549328</td>\n",
              "      <td>5.028229</td>\n",
              "      <td>-13.622225</td>\n",
              "      <td>-1.557733</td>\n",
              "      <td>-3.454623</td>\n",
              "      <td>-8.322736</td>\n",
              "      <td>-1.618503</td>\n",
              "      <td>5.821856</td>\n",
              "      <td>5.079574</td>\n",
              "      <td>14.880066</td>\n",
              "      <td>12.842129</td>\n",
              "      <td>10.316431</td>\n",
              "      <td>2.314389</td>\n",
              "      <td>-1.028987</td>\n",
              "      <td>-4.437511</td>\n",
              "      <td>-1.516412</td>\n",
              "      <td>1.821853</td>\n",
              "      <td>2.899392</td>\n",
              "      <td>-3.779098</td>\n",
              "      <td>-4.537612</td>\n",
              "      <td>-3.470069</td>\n",
              "      <td>3.128976</td>\n",
              "      <td>1.509248</td>\n",
              "      <td>-3.425199</td>\n",
              "      <td>-2.644326</td>\n",
              "      <td>0.561523</td>\n",
              "      <td>1.497484</td>\n",
              "      <td>-2.038859</td>\n",
              "      <td>-0.212162</td>\n",
              "      <td>5.201227</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1167</th>\n",
              "      <td>-582.898621</td>\n",
              "      <td>94.474586</td>\n",
              "      <td>22.367788</td>\n",
              "      <td>49.819572</td>\n",
              "      <td>-0.691075</td>\n",
              "      <td>0.839949</td>\n",
              "      <td>-10.800381</td>\n",
              "      <td>2.487875</td>\n",
              "      <td>-7.325592</td>\n",
              "      <td>-10.454241</td>\n",
              "      <td>-5.319189</td>\n",
              "      <td>-5.060174</td>\n",
              "      <td>-0.436529</td>\n",
              "      <td>0.662336</td>\n",
              "      <td>-0.975422</td>\n",
              "      <td>-3.178003</td>\n",
              "      <td>0.377104</td>\n",
              "      <td>0.166042</td>\n",
              "      <td>3.856387</td>\n",
              "      <td>2.266340</td>\n",
              "      <td>-3.008166</td>\n",
              "      <td>-4.696426</td>\n",
              "      <td>2.593040</td>\n",
              "      <td>-0.121497</td>\n",
              "      <td>-1.530484</td>\n",
              "      <td>0.510399</td>\n",
              "      <td>-1.604053</td>\n",
              "      <td>-0.510490</td>\n",
              "      <td>-0.786538</td>\n",
              "      <td>-0.514911</td>\n",
              "      <td>-0.980235</td>\n",
              "      <td>1.740963</td>\n",
              "      <td>2.374023</td>\n",
              "      <td>1.863116</td>\n",
              "      <td>3.787547</td>\n",
              "      <td>3.317013</td>\n",
              "      <td>2.981250</td>\n",
              "      <td>3.029734</td>\n",
              "      <td>1.917622</td>\n",
              "      <td>2.436971</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2302</th>\n",
              "      <td>-453.688293</td>\n",
              "      <td>54.436947</td>\n",
              "      <td>-20.837057</td>\n",
              "      <td>12.399979</td>\n",
              "      <td>4.598026</td>\n",
              "      <td>-10.157798</td>\n",
              "      <td>-3.613032</td>\n",
              "      <td>5.971458</td>\n",
              "      <td>-17.891111</td>\n",
              "      <td>4.994667</td>\n",
              "      <td>-5.933625</td>\n",
              "      <td>3.367061</td>\n",
              "      <td>-3.364632</td>\n",
              "      <td>-0.197297</td>\n",
              "      <td>-1.930623</td>\n",
              "      <td>-1.551808</td>\n",
              "      <td>-6.128464</td>\n",
              "      <td>-1.747104</td>\n",
              "      <td>-5.295576</td>\n",
              "      <td>0.045491</td>\n",
              "      <td>-6.443377</td>\n",
              "      <td>-1.570445</td>\n",
              "      <td>-1.858576</td>\n",
              "      <td>-1.485167</td>\n",
              "      <td>-3.111758</td>\n",
              "      <td>-1.082204</td>\n",
              "      <td>-4.353946</td>\n",
              "      <td>-1.348092</td>\n",
              "      <td>-1.476537</td>\n",
              "      <td>0.831332</td>\n",
              "      <td>1.484614</td>\n",
              "      <td>2.430773</td>\n",
              "      <td>4.423322</td>\n",
              "      <td>2.457987</td>\n",
              "      <td>0.564883</td>\n",
              "      <td>2.526512</td>\n",
              "      <td>4.930277</td>\n",
              "      <td>8.633234</td>\n",
              "      <td>12.932058</td>\n",
              "      <td>12.609378</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1126</th>\n",
              "      <td>-534.025574</td>\n",
              "      <td>48.958679</td>\n",
              "      <td>-15.409526</td>\n",
              "      <td>12.054539</td>\n",
              "      <td>-2.988102</td>\n",
              "      <td>-2.156705</td>\n",
              "      <td>-9.763924</td>\n",
              "      <td>-9.472930</td>\n",
              "      <td>-7.854082</td>\n",
              "      <td>5.645915</td>\n",
              "      <td>-9.166075</td>\n",
              "      <td>-0.215249</td>\n",
              "      <td>-5.583377</td>\n",
              "      <td>2.543697</td>\n",
              "      <td>-0.975078</td>\n",
              "      <td>-3.777294</td>\n",
              "      <td>-4.063252</td>\n",
              "      <td>1.030010</td>\n",
              "      <td>0.440615</td>\n",
              "      <td>3.945432</td>\n",
              "      <td>7.034808</td>\n",
              "      <td>8.334291</td>\n",
              "      <td>2.197643</td>\n",
              "      <td>4.334787</td>\n",
              "      <td>3.564072</td>\n",
              "      <td>8.128938</td>\n",
              "      <td>7.024919</td>\n",
              "      <td>7.778437</td>\n",
              "      <td>4.735685</td>\n",
              "      <td>3.979075</td>\n",
              "      <td>2.260893</td>\n",
              "      <td>4.297817</td>\n",
              "      <td>4.218397</td>\n",
              "      <td>3.175505</td>\n",
              "      <td>0.310580</td>\n",
              "      <td>0.366625</td>\n",
              "      <td>2.389436</td>\n",
              "      <td>2.160803</td>\n",
              "      <td>0.460945</td>\n",
              "      <td>-0.436673</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2568 rows  41 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0           1          2  ...         38         39  labels\n",
              "315  -825.414673   60.533543   6.114007  ...  -2.586861  -1.945768       5\n",
              "300  -363.945038  132.844086  18.439739  ...   0.170139   0.447085       4\n",
              "2395 -465.725433   31.941385 -25.003664  ...  -1.804306  -0.360051       0\n",
              "838  -595.746338   49.765644  -8.958530  ...  -0.681581   0.875309       6\n",
              "676  -626.350830   61.393463 -14.373200  ...   0.355261  -1.133214       5\n",
              "...          ...         ...        ...  ...        ...        ...     ...\n",
              "162  -625.132874   64.716370 -26.561125  ...  10.456415  12.188598       4\n",
              "2289 -424.692383   20.815155 -28.414587  ...  -0.212162   5.201227       0\n",
              "1167 -582.898621   94.474586  22.367788  ...   1.917622   2.436971       2\n",
              "2302 -453.688293   54.436947 -20.837057  ...  12.932058  12.609378       0\n",
              "1126 -534.025574   48.958679 -15.409526  ...   0.460945  -0.436673       2\n",
              "\n",
              "[2568 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seB22CMowZ9G"
      },
      "source": [
        "X_input = shuffled_df.iloc[:,:-1]\n",
        "Y_output_lables = shuffled_df.iloc[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZDlIciUwfu3",
        "outputId": "1adc3005-5259-451e-811b-ec9d38ddf6e6"
      },
      "source": [
        "X_input.shape,Y_output_lables.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2568, 40), (2568,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McUpMDEswiql"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_input, Y_output_lables, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Toez3l-rwlUB",
        "outputId": "904ee6b9-8193-4b4b-b8f9-6ca195821e3a"
      },
      "source": [
        "X_train.shape,y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1926, 40), (1926,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHH0_Me6wpbA",
        "outputId": "142c0806-45f3-48d6-bf4d-8028a88def6e"
      },
      "source": [
        "X_test.shape,y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((642, 40), (642,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb8vK-Edwr0Y"
      },
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygnwGSBvxc96",
        "outputId": "ea2c1928-7404-405c-805e-3a416bde5d78"
      },
      "source": [
        "x_traincnn.shape, x_testcnn.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1926, 40, 1), (642, 40, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI2AfInSyJne"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "modelNN3 = Sequential()\n",
        "\n",
        "modelNN3.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "modelNN3.add(Activation('relu'))\n",
        "modelNN3.add(Dropout(0.1))\n",
        "modelNN3.add(MaxPooling1D(pool_size=(8)))\n",
        "\n",
        "modelNN3.add(Conv1D(256, 5,padding='same',))\n",
        "modelNN3.add(Activation('relu'))\n",
        "modelNN3.add(Dropout(0.1))\n",
        "\n",
        "modelNN3.add(Flatten())\n",
        "modelNN3.add(Dense(7))\n",
        "modelNN3.add(Activation('softmax'))\n",
        "opt = keras.optimizers.RMSprop(lr=0.00001, rho=0.9, epsilon=None, decay=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnCO6h7YyUD5",
        "outputId": "b579f68d-161e-4679-d992-fdeb04829fd8"
      },
      "source": [
        "modelNN3.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_8 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 5, 256)            164096    \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 5, 256)            0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 5, 256)            0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 7)                 8967      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 173,831\n",
            "Trainable params: 173,831\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhRToMt1yWvu"
      },
      "source": [
        "modelNN3.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlq-yDs-ytJq",
        "outputId": "151675f9-92a8-4871-9f4b-641eb976f061"
      },
      "source": [
        "modelCNNhistory3=modelNN3.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "121/121 [==============================] - 3s 16ms/step - loss: 15.2381 - accuracy: 0.1490 - val_loss: 2.1492 - val_accuracy: 0.1916\n",
            "Epoch 2/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 5.3008 - accuracy: 0.1517 - val_loss: 2.2161 - val_accuracy: 0.1838\n",
            "Epoch 3/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 5.1012 - accuracy: 0.1595 - val_loss: 2.1041 - val_accuracy: 0.2181\n",
            "Epoch 4/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 4.9724 - accuracy: 0.1822 - val_loss: 1.9249 - val_accuracy: 0.2492\n",
            "Epoch 5/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 4.6950 - accuracy: 0.1819 - val_loss: 1.9453 - val_accuracy: 0.2726\n",
            "Epoch 6/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 4.5822 - accuracy: 0.1782 - val_loss: 1.8393 - val_accuracy: 0.2477\n",
            "Epoch 7/1000\n",
            "121/121 [==============================] - 2s 14ms/step - loss: 4.3815 - accuracy: 0.1909 - val_loss: 1.8486 - val_accuracy: 0.2492\n",
            "Epoch 8/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 4.3629 - accuracy: 0.1796 - val_loss: 1.7372 - val_accuracy: 0.2773\n",
            "Epoch 9/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 4.4387 - accuracy: 0.1876 - val_loss: 1.8187 - val_accuracy: 0.2975\n",
            "Epoch 10/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 4.0423 - accuracy: 0.1932 - val_loss: 1.7245 - val_accuracy: 0.2850\n",
            "Epoch 11/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 3.9800 - accuracy: 0.1977 - val_loss: 1.7774 - val_accuracy: 0.2773\n",
            "Epoch 12/1000\n",
            "121/121 [==============================] - 2s 14ms/step - loss: 3.9704 - accuracy: 0.2033 - val_loss: 1.8020 - val_accuracy: 0.2773\n",
            "Epoch 13/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 3.7742 - accuracy: 0.2066 - val_loss: 1.6890 - val_accuracy: 0.2975\n",
            "Epoch 14/1000\n",
            "121/121 [==============================] - 2s 14ms/step - loss: 3.5735 - accuracy: 0.2144 - val_loss: 1.7223 - val_accuracy: 0.2928\n",
            "Epoch 15/1000\n",
            "121/121 [==============================] - 2s 14ms/step - loss: 3.6227 - accuracy: 0.2070 - val_loss: 1.7501 - val_accuracy: 0.2773\n",
            "Epoch 16/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 3.5670 - accuracy: 0.1915 - val_loss: 1.7127 - val_accuracy: 0.3084\n",
            "Epoch 17/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 3.4634 - accuracy: 0.2010 - val_loss: 1.6985 - val_accuracy: 0.3053\n",
            "Epoch 18/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 3.1453 - accuracy: 0.2188 - val_loss: 1.6641 - val_accuracy: 0.3146\n",
            "Epoch 19/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 3.1717 - accuracy: 0.2227 - val_loss: 1.7019 - val_accuracy: 0.3037\n",
            "Epoch 20/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 3.1905 - accuracy: 0.2117 - val_loss: 1.6694 - val_accuracy: 0.3209\n",
            "Epoch 21/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 3.1114 - accuracy: 0.2117 - val_loss: 1.6391 - val_accuracy: 0.3598\n",
            "Epoch 22/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.9903 - accuracy: 0.2420 - val_loss: 1.6356 - val_accuracy: 0.3287\n",
            "Epoch 23/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.8579 - accuracy: 0.2494 - val_loss: 1.6690 - val_accuracy: 0.3629\n",
            "Epoch 24/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.8759 - accuracy: 0.2240 - val_loss: 1.7020 - val_accuracy: 0.3458\n",
            "Epoch 25/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.7415 - accuracy: 0.2350 - val_loss: 1.6884 - val_accuracy: 0.3209\n",
            "Epoch 26/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.7961 - accuracy: 0.2439 - val_loss: 1.6260 - val_accuracy: 0.3396\n",
            "Epoch 27/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.7089 - accuracy: 0.2112 - val_loss: 1.6247 - val_accuracy: 0.3474\n",
            "Epoch 28/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.6292 - accuracy: 0.2427 - val_loss: 1.6153 - val_accuracy: 0.3411\n",
            "Epoch 29/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.5460 - accuracy: 0.2497 - val_loss: 1.6016 - val_accuracy: 0.3442\n",
            "Epoch 30/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.5051 - accuracy: 0.2252 - val_loss: 1.6505 - val_accuracy: 0.3505\n",
            "Epoch 31/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.4372 - accuracy: 0.2786 - val_loss: 1.5891 - val_accuracy: 0.3629\n",
            "Epoch 32/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.4133 - accuracy: 0.2459 - val_loss: 1.6224 - val_accuracy: 0.3551\n",
            "Epoch 33/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.4099 - accuracy: 0.2553 - val_loss: 1.7028 - val_accuracy: 0.3333\n",
            "Epoch 34/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.3903 - accuracy: 0.2401 - val_loss: 1.6388 - val_accuracy: 0.3396\n",
            "Epoch 35/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.2005 - accuracy: 0.2621 - val_loss: 1.6068 - val_accuracy: 0.3614\n",
            "Epoch 36/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.2772 - accuracy: 0.2778 - val_loss: 1.5709 - val_accuracy: 0.3676\n",
            "Epoch 37/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.3017 - accuracy: 0.2562 - val_loss: 1.5709 - val_accuracy: 0.3879\n",
            "Epoch 38/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.2229 - accuracy: 0.2893 - val_loss: 1.6327 - val_accuracy: 0.3536\n",
            "Epoch 39/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.1975 - accuracy: 0.2908 - val_loss: 1.5901 - val_accuracy: 0.3505\n",
            "Epoch 40/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.1595 - accuracy: 0.2839 - val_loss: 1.5956 - val_accuracy: 0.3442\n",
            "Epoch 41/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.0685 - accuracy: 0.2917 - val_loss: 1.5827 - val_accuracy: 0.3505\n",
            "Epoch 42/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.0610 - accuracy: 0.3261 - val_loss: 1.5830 - val_accuracy: 0.3598\n",
            "Epoch 43/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.0023 - accuracy: 0.2915 - val_loss: 1.5583 - val_accuracy: 0.3769\n",
            "Epoch 44/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.9782 - accuracy: 0.2977 - val_loss: 1.5782 - val_accuracy: 0.3567\n",
            "Epoch 45/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.0181 - accuracy: 0.2855 - val_loss: 1.5563 - val_accuracy: 0.3785\n",
            "Epoch 46/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.9987 - accuracy: 0.2960 - val_loss: 1.6071 - val_accuracy: 0.3910\n",
            "Epoch 47/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 2.0038 - accuracy: 0.2812 - val_loss: 1.5789 - val_accuracy: 0.3754\n",
            "Epoch 48/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.9662 - accuracy: 0.2958 - val_loss: 1.6109 - val_accuracy: 0.3738\n",
            "Epoch 49/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.9040 - accuracy: 0.3106 - val_loss: 1.6426 - val_accuracy: 0.3551\n",
            "Epoch 50/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.9921 - accuracy: 0.2886 - val_loss: 1.5388 - val_accuracy: 0.3894\n",
            "Epoch 51/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.9187 - accuracy: 0.2894 - val_loss: 1.6055 - val_accuracy: 0.3598\n",
            "Epoch 52/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.8765 - accuracy: 0.3174 - val_loss: 1.5403 - val_accuracy: 0.4128\n",
            "Epoch 53/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.8287 - accuracy: 0.3154 - val_loss: 1.5468 - val_accuracy: 0.3660\n",
            "Epoch 54/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.9054 - accuracy: 0.2605 - val_loss: 1.5435 - val_accuracy: 0.3769\n",
            "Epoch 55/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.8491 - accuracy: 0.3225 - val_loss: 1.5676 - val_accuracy: 0.4050\n",
            "Epoch 56/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.8375 - accuracy: 0.3200 - val_loss: 1.5608 - val_accuracy: 0.3972\n",
            "Epoch 57/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.8385 - accuracy: 0.3245 - val_loss: 1.5440 - val_accuracy: 0.4003\n",
            "Epoch 58/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.7893 - accuracy: 0.3265 - val_loss: 1.5277 - val_accuracy: 0.4190\n",
            "Epoch 59/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.7601 - accuracy: 0.3244 - val_loss: 1.5284 - val_accuracy: 0.3894\n",
            "Epoch 60/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.8282 - accuracy: 0.3391 - val_loss: 1.5491 - val_accuracy: 0.3847\n",
            "Epoch 61/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.8116 - accuracy: 0.3102 - val_loss: 1.5216 - val_accuracy: 0.4283\n",
            "Epoch 62/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.7671 - accuracy: 0.3234 - val_loss: 1.5644 - val_accuracy: 0.3910\n",
            "Epoch 63/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.7573 - accuracy: 0.3355 - val_loss: 1.5256 - val_accuracy: 0.3988\n",
            "Epoch 64/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.7475 - accuracy: 0.3565 - val_loss: 1.5223 - val_accuracy: 0.4174\n",
            "Epoch 65/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.8327 - accuracy: 0.3192 - val_loss: 1.5173 - val_accuracy: 0.4112\n",
            "Epoch 66/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.7157 - accuracy: 0.3346 - val_loss: 1.5104 - val_accuracy: 0.4034\n",
            "Epoch 67/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.7305 - accuracy: 0.3506 - val_loss: 1.5160 - val_accuracy: 0.4393\n",
            "Epoch 68/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.7718 - accuracy: 0.3234 - val_loss: 1.5134 - val_accuracy: 0.4299\n",
            "Epoch 69/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.7544 - accuracy: 0.3318 - val_loss: 1.5115 - val_accuracy: 0.4190\n",
            "Epoch 70/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.7374 - accuracy: 0.3158 - val_loss: 1.5046 - val_accuracy: 0.4330\n",
            "Epoch 71/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6951 - accuracy: 0.3512 - val_loss: 1.5119 - val_accuracy: 0.4283\n",
            "Epoch 72/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6658 - accuracy: 0.3556 - val_loss: 1.5319 - val_accuracy: 0.4221\n",
            "Epoch 73/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.7479 - accuracy: 0.3409 - val_loss: 1.5006 - val_accuracy: 0.4252\n",
            "Epoch 74/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6852 - accuracy: 0.3485 - val_loss: 1.5298 - val_accuracy: 0.4268\n",
            "Epoch 75/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6577 - accuracy: 0.3692 - val_loss: 1.4918 - val_accuracy: 0.4299\n",
            "Epoch 76/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6911 - accuracy: 0.3319 - val_loss: 1.5023 - val_accuracy: 0.4439\n",
            "Epoch 77/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6567 - accuracy: 0.3706 - val_loss: 1.4911 - val_accuracy: 0.4424\n",
            "Epoch 78/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6178 - accuracy: 0.3712 - val_loss: 1.4888 - val_accuracy: 0.4237\n",
            "Epoch 79/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6540 - accuracy: 0.3528 - val_loss: 1.4830 - val_accuracy: 0.4439\n",
            "Epoch 80/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6414 - accuracy: 0.3736 - val_loss: 1.5043 - val_accuracy: 0.4237\n",
            "Epoch 81/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6604 - accuracy: 0.3609 - val_loss: 1.4956 - val_accuracy: 0.4050\n",
            "Epoch 82/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6348 - accuracy: 0.3686 - val_loss: 1.4802 - val_accuracy: 0.4315\n",
            "Epoch 83/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5777 - accuracy: 0.3995 - val_loss: 1.5108 - val_accuracy: 0.4315\n",
            "Epoch 84/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6359 - accuracy: 0.3815 - val_loss: 1.4982 - val_accuracy: 0.4470\n",
            "Epoch 85/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6530 - accuracy: 0.3705 - val_loss: 1.4981 - val_accuracy: 0.4424\n",
            "Epoch 86/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6379 - accuracy: 0.3532 - val_loss: 1.4834 - val_accuracy: 0.4642\n",
            "Epoch 87/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6055 - accuracy: 0.3937 - val_loss: 1.4813 - val_accuracy: 0.4330\n",
            "Epoch 88/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6230 - accuracy: 0.3827 - val_loss: 1.5059 - val_accuracy: 0.4143\n",
            "Epoch 89/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6012 - accuracy: 0.3767 - val_loss: 1.4758 - val_accuracy: 0.4346\n",
            "Epoch 90/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6678 - accuracy: 0.3578 - val_loss: 1.4627 - val_accuracy: 0.4237\n",
            "Epoch 91/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6206 - accuracy: 0.3698 - val_loss: 1.4850 - val_accuracy: 0.4299\n",
            "Epoch 92/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6336 - accuracy: 0.3874 - val_loss: 1.4703 - val_accuracy: 0.4517\n",
            "Epoch 93/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.6112 - accuracy: 0.3812 - val_loss: 1.4782 - val_accuracy: 0.4455\n",
            "Epoch 94/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5849 - accuracy: 0.3828 - val_loss: 1.4670 - val_accuracy: 0.4424\n",
            "Epoch 95/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5862 - accuracy: 0.3896 - val_loss: 1.4787 - val_accuracy: 0.4408\n",
            "Epoch 96/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5524 - accuracy: 0.3940 - val_loss: 1.4660 - val_accuracy: 0.4439\n",
            "Epoch 97/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5992 - accuracy: 0.3669 - val_loss: 1.4626 - val_accuracy: 0.4455\n",
            "Epoch 98/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5922 - accuracy: 0.3802 - val_loss: 1.4531 - val_accuracy: 0.4579\n",
            "Epoch 99/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5623 - accuracy: 0.3765 - val_loss: 1.4537 - val_accuracy: 0.4330\n",
            "Epoch 100/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5890 - accuracy: 0.3924 - val_loss: 1.4611 - val_accuracy: 0.4283\n",
            "Epoch 101/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5821 - accuracy: 0.4094 - val_loss: 1.4600 - val_accuracy: 0.4408\n",
            "Epoch 102/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5843 - accuracy: 0.4072 - val_loss: 1.4452 - val_accuracy: 0.4502\n",
            "Epoch 103/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5322 - accuracy: 0.4216 - val_loss: 1.4483 - val_accuracy: 0.4237\n",
            "Epoch 104/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5520 - accuracy: 0.3871 - val_loss: 1.4562 - val_accuracy: 0.4206\n",
            "Epoch 105/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5530 - accuracy: 0.3921 - val_loss: 1.4405 - val_accuracy: 0.4782\n",
            "Epoch 106/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5805 - accuracy: 0.4021 - val_loss: 1.4357 - val_accuracy: 0.4408\n",
            "Epoch 107/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5384 - accuracy: 0.4072 - val_loss: 1.4408 - val_accuracy: 0.4673\n",
            "Epoch 108/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5670 - accuracy: 0.3814 - val_loss: 1.4605 - val_accuracy: 0.4455\n",
            "Epoch 109/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5799 - accuracy: 0.3854 - val_loss: 1.4341 - val_accuracy: 0.4595\n",
            "Epoch 110/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5219 - accuracy: 0.3914 - val_loss: 1.4486 - val_accuracy: 0.4517\n",
            "Epoch 111/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5267 - accuracy: 0.4159 - val_loss: 1.4362 - val_accuracy: 0.4455\n",
            "Epoch 112/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5268 - accuracy: 0.4274 - val_loss: 1.4569 - val_accuracy: 0.4424\n",
            "Epoch 113/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5499 - accuracy: 0.3995 - val_loss: 1.4241 - val_accuracy: 0.4798\n",
            "Epoch 114/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5154 - accuracy: 0.4143 - val_loss: 1.4190 - val_accuracy: 0.4688\n",
            "Epoch 115/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5831 - accuracy: 0.3884 - val_loss: 1.4193 - val_accuracy: 0.4642\n",
            "Epoch 116/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5420 - accuracy: 0.3856 - val_loss: 1.4322 - val_accuracy: 0.4626\n",
            "Epoch 117/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5177 - accuracy: 0.4172 - val_loss: 1.4240 - val_accuracy: 0.4657\n",
            "Epoch 118/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5389 - accuracy: 0.4173 - val_loss: 1.4270 - val_accuracy: 0.4688\n",
            "Epoch 119/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5298 - accuracy: 0.3896 - val_loss: 1.4257 - val_accuracy: 0.4751\n",
            "Epoch 120/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4776 - accuracy: 0.4352 - val_loss: 1.4373 - val_accuracy: 0.4455\n",
            "Epoch 121/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5120 - accuracy: 0.4055 - val_loss: 1.4106 - val_accuracy: 0.4766\n",
            "Epoch 122/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5142 - accuracy: 0.4217 - val_loss: 1.4160 - val_accuracy: 0.4766\n",
            "Epoch 123/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4977 - accuracy: 0.4362 - val_loss: 1.4288 - val_accuracy: 0.4673\n",
            "Epoch 124/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4783 - accuracy: 0.4454 - val_loss: 1.4099 - val_accuracy: 0.4798\n",
            "Epoch 125/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4548 - accuracy: 0.4335 - val_loss: 1.4034 - val_accuracy: 0.4922\n",
            "Epoch 126/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4771 - accuracy: 0.4402 - val_loss: 1.4010 - val_accuracy: 0.4766\n",
            "Epoch 127/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4568 - accuracy: 0.4270 - val_loss: 1.3995 - val_accuracy: 0.4642\n",
            "Epoch 128/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5022 - accuracy: 0.4250 - val_loss: 1.4031 - val_accuracy: 0.4907\n",
            "Epoch 129/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4953 - accuracy: 0.4260 - val_loss: 1.3956 - val_accuracy: 0.4829\n",
            "Epoch 130/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4822 - accuracy: 0.4208 - val_loss: 1.4084 - val_accuracy: 0.4922\n",
            "Epoch 131/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5022 - accuracy: 0.4321 - val_loss: 1.3994 - val_accuracy: 0.4829\n",
            "Epoch 132/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4571 - accuracy: 0.4386 - val_loss: 1.3888 - val_accuracy: 0.4720\n",
            "Epoch 133/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4734 - accuracy: 0.4487 - val_loss: 1.4012 - val_accuracy: 0.4766\n",
            "Epoch 134/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5437 - accuracy: 0.4256 - val_loss: 1.4155 - val_accuracy: 0.4829\n",
            "Epoch 135/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4489 - accuracy: 0.4379 - val_loss: 1.3909 - val_accuracy: 0.4875\n",
            "Epoch 136/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.5127 - accuracy: 0.4069 - val_loss: 1.4009 - val_accuracy: 0.4844\n",
            "Epoch 137/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4781 - accuracy: 0.4470 - val_loss: 1.3975 - val_accuracy: 0.4829\n",
            "Epoch 138/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4626 - accuracy: 0.4343 - val_loss: 1.3928 - val_accuracy: 0.4969\n",
            "Epoch 139/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.4039 - accuracy: 0.4884 - val_loss: 1.3793 - val_accuracy: 0.4891\n",
            "Epoch 140/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4628 - accuracy: 0.4513 - val_loss: 1.3924 - val_accuracy: 0.4875\n",
            "Epoch 141/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4410 - accuracy: 0.4519 - val_loss: 1.3980 - val_accuracy: 0.4798\n",
            "Epoch 142/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4526 - accuracy: 0.4564 - val_loss: 1.3800 - val_accuracy: 0.4844\n",
            "Epoch 143/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4101 - accuracy: 0.4484 - val_loss: 1.3836 - val_accuracy: 0.4860\n",
            "Epoch 144/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.4170 - accuracy: 0.4660 - val_loss: 1.3782 - val_accuracy: 0.5047\n",
            "Epoch 145/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4496 - accuracy: 0.4364 - val_loss: 1.3681 - val_accuracy: 0.4907\n",
            "Epoch 146/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4230 - accuracy: 0.4687 - val_loss: 1.3635 - val_accuracy: 0.4922\n",
            "Epoch 147/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4591 - accuracy: 0.4365 - val_loss: 1.3661 - val_accuracy: 0.5093\n",
            "Epoch 148/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4059 - accuracy: 0.4665 - val_loss: 1.3673 - val_accuracy: 0.5140\n",
            "Epoch 149/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4595 - accuracy: 0.4348 - val_loss: 1.3696 - val_accuracy: 0.4907\n",
            "Epoch 150/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.4000 - accuracy: 0.4428 - val_loss: 1.3694 - val_accuracy: 0.4673\n",
            "Epoch 151/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.4398 - accuracy: 0.4404 - val_loss: 1.3560 - val_accuracy: 0.5000\n",
            "Epoch 152/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4512 - accuracy: 0.4483 - val_loss: 1.3557 - val_accuracy: 0.4860\n",
            "Epoch 153/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4040 - accuracy: 0.4756 - val_loss: 1.3598 - val_accuracy: 0.4984\n",
            "Epoch 154/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4072 - accuracy: 0.4680 - val_loss: 1.3733 - val_accuracy: 0.4907\n",
            "Epoch 155/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4138 - accuracy: 0.4727 - val_loss: 1.3505 - val_accuracy: 0.4844\n",
            "Epoch 156/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4095 - accuracy: 0.4561 - val_loss: 1.3589 - val_accuracy: 0.5031\n",
            "Epoch 157/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3931 - accuracy: 0.4690 - val_loss: 1.3553 - val_accuracy: 0.4938\n",
            "Epoch 158/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.4469 - accuracy: 0.4485 - val_loss: 1.3456 - val_accuracy: 0.5109\n",
            "Epoch 159/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.3730 - accuracy: 0.4654 - val_loss: 1.3537 - val_accuracy: 0.4922\n",
            "Epoch 160/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.3947 - accuracy: 0.4687 - val_loss: 1.3578 - val_accuracy: 0.5280\n",
            "Epoch 161/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.4190 - accuracy: 0.4625 - val_loss: 1.3530 - val_accuracy: 0.5234\n",
            "Epoch 162/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.4241 - accuracy: 0.4394 - val_loss: 1.3508 - val_accuracy: 0.5062\n",
            "Epoch 163/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4198 - accuracy: 0.4683 - val_loss: 1.3385 - val_accuracy: 0.5078\n",
            "Epoch 164/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3889 - accuracy: 0.4983 - val_loss: 1.3608 - val_accuracy: 0.4969\n",
            "Epoch 165/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4071 - accuracy: 0.4501 - val_loss: 1.3338 - val_accuracy: 0.5187\n",
            "Epoch 166/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3711 - accuracy: 0.4817 - val_loss: 1.3427 - val_accuracy: 0.5171\n",
            "Epoch 167/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.3864 - accuracy: 0.4793 - val_loss: 1.3428 - val_accuracy: 0.5202\n",
            "Epoch 168/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3822 - accuracy: 0.4934 - val_loss: 1.3502 - val_accuracy: 0.4969\n",
            "Epoch 169/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.4473 - accuracy: 0.4589 - val_loss: 1.3493 - val_accuracy: 0.5125\n",
            "Epoch 170/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3941 - accuracy: 0.4755 - val_loss: 1.3273 - val_accuracy: 0.5062\n",
            "Epoch 171/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3915 - accuracy: 0.4798 - val_loss: 1.3266 - val_accuracy: 0.4984\n",
            "Epoch 172/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3868 - accuracy: 0.4752 - val_loss: 1.3312 - val_accuracy: 0.5218\n",
            "Epoch 173/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3777 - accuracy: 0.4740 - val_loss: 1.3255 - val_accuracy: 0.5249\n",
            "Epoch 174/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3696 - accuracy: 0.4765 - val_loss: 1.3267 - val_accuracy: 0.5249\n",
            "Epoch 175/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3700 - accuracy: 0.4541 - val_loss: 1.3239 - val_accuracy: 0.5312\n",
            "Epoch 176/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3585 - accuracy: 0.4836 - val_loss: 1.3205 - val_accuracy: 0.5234\n",
            "Epoch 177/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3998 - accuracy: 0.4738 - val_loss: 1.3203 - val_accuracy: 0.5327\n",
            "Epoch 178/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.3445 - accuracy: 0.4904 - val_loss: 1.3172 - val_accuracy: 0.5296\n",
            "Epoch 179/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3535 - accuracy: 0.4848 - val_loss: 1.3224 - val_accuracy: 0.5187\n",
            "Epoch 180/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3515 - accuracy: 0.4733 - val_loss: 1.3188 - val_accuracy: 0.5249\n",
            "Epoch 181/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3794 - accuracy: 0.4861 - val_loss: 1.3291 - val_accuracy: 0.5031\n",
            "Epoch 182/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3495 - accuracy: 0.4960 - val_loss: 1.3115 - val_accuracy: 0.5078\n",
            "Epoch 183/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3585 - accuracy: 0.4930 - val_loss: 1.3171 - val_accuracy: 0.5093\n",
            "Epoch 184/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3176 - accuracy: 0.5028 - val_loss: 1.3253 - val_accuracy: 0.5109\n",
            "Epoch 185/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3457 - accuracy: 0.4998 - val_loss: 1.3204 - val_accuracy: 0.5078\n",
            "Epoch 186/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3987 - accuracy: 0.4791 - val_loss: 1.3112 - val_accuracy: 0.5140\n",
            "Epoch 187/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3691 - accuracy: 0.4824 - val_loss: 1.3202 - val_accuracy: 0.4969\n",
            "Epoch 188/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3934 - accuracy: 0.4715 - val_loss: 1.3085 - val_accuracy: 0.5218\n",
            "Epoch 189/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3545 - accuracy: 0.4852 - val_loss: 1.3110 - val_accuracy: 0.5202\n",
            "Epoch 190/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.3286 - accuracy: 0.4891 - val_loss: 1.3021 - val_accuracy: 0.5327\n",
            "Epoch 191/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2989 - accuracy: 0.5219 - val_loss: 1.2971 - val_accuracy: 0.5016\n",
            "Epoch 192/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3428 - accuracy: 0.4782 - val_loss: 1.2983 - val_accuracy: 0.5374\n",
            "Epoch 193/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3538 - accuracy: 0.4939 - val_loss: 1.2963 - val_accuracy: 0.5343\n",
            "Epoch 194/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3346 - accuracy: 0.5004 - val_loss: 1.3004 - val_accuracy: 0.5436\n",
            "Epoch 195/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3827 - accuracy: 0.4684 - val_loss: 1.2991 - val_accuracy: 0.5327\n",
            "Epoch 196/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3066 - accuracy: 0.5194 - val_loss: 1.2832 - val_accuracy: 0.5296\n",
            "Epoch 197/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3218 - accuracy: 0.5096 - val_loss: 1.2822 - val_accuracy: 0.5218\n",
            "Epoch 198/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3678 - accuracy: 0.4662 - val_loss: 1.2844 - val_accuracy: 0.5374\n",
            "Epoch 199/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3340 - accuracy: 0.4975 - val_loss: 1.2971 - val_accuracy: 0.5389\n",
            "Epoch 200/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.3122 - accuracy: 0.4838 - val_loss: 1.2866 - val_accuracy: 0.5265\n",
            "Epoch 201/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.3200 - accuracy: 0.5078 - val_loss: 1.2832 - val_accuracy: 0.5312\n",
            "Epoch 202/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3148 - accuracy: 0.4859 - val_loss: 1.2829 - val_accuracy: 0.5265\n",
            "Epoch 203/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2909 - accuracy: 0.5044 - val_loss: 1.2889 - val_accuracy: 0.5156\n",
            "Epoch 204/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.3129 - accuracy: 0.5060 - val_loss: 1.2872 - val_accuracy: 0.5389\n",
            "Epoch 205/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2968 - accuracy: 0.5133 - val_loss: 1.2779 - val_accuracy: 0.5374\n",
            "Epoch 206/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3174 - accuracy: 0.5308 - val_loss: 1.2802 - val_accuracy: 0.5374\n",
            "Epoch 207/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2923 - accuracy: 0.5061 - val_loss: 1.2721 - val_accuracy: 0.5358\n",
            "Epoch 208/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3124 - accuracy: 0.5190 - val_loss: 1.2675 - val_accuracy: 0.5421\n",
            "Epoch 209/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3450 - accuracy: 0.4949 - val_loss: 1.2842 - val_accuracy: 0.5062\n",
            "Epoch 210/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.3138 - accuracy: 0.5040 - val_loss: 1.2683 - val_accuracy: 0.5436\n",
            "Epoch 211/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2635 - accuracy: 0.5155 - val_loss: 1.2755 - val_accuracy: 0.5498\n",
            "Epoch 212/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.3122 - accuracy: 0.5049 - val_loss: 1.2642 - val_accuracy: 0.5436\n",
            "Epoch 213/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.3179 - accuracy: 0.4925 - val_loss: 1.2667 - val_accuracy: 0.5265\n",
            "Epoch 214/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.3234 - accuracy: 0.5012 - val_loss: 1.2773 - val_accuracy: 0.5389\n",
            "Epoch 215/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2870 - accuracy: 0.5173 - val_loss: 1.2621 - val_accuracy: 0.5421\n",
            "Epoch 216/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2975 - accuracy: 0.5106 - val_loss: 1.2591 - val_accuracy: 0.5421\n",
            "Epoch 217/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3048 - accuracy: 0.5012 - val_loss: 1.2650 - val_accuracy: 0.5483\n",
            "Epoch 218/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2455 - accuracy: 0.5347 - val_loss: 1.2597 - val_accuracy: 0.5358\n",
            "Epoch 219/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2801 - accuracy: 0.5093 - val_loss: 1.2654 - val_accuracy: 0.5421\n",
            "Epoch 220/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2717 - accuracy: 0.5239 - val_loss: 1.2493 - val_accuracy: 0.5218\n",
            "Epoch 221/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.3052 - accuracy: 0.5014 - val_loss: 1.2574 - val_accuracy: 0.5483\n",
            "Epoch 222/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2768 - accuracy: 0.5108 - val_loss: 1.2634 - val_accuracy: 0.5483\n",
            "Epoch 223/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2915 - accuracy: 0.5090 - val_loss: 1.2507 - val_accuracy: 0.5452\n",
            "Epoch 224/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2858 - accuracy: 0.5176 - val_loss: 1.2576 - val_accuracy: 0.5389\n",
            "Epoch 225/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2920 - accuracy: 0.5153 - val_loss: 1.2647 - val_accuracy: 0.5421\n",
            "Epoch 226/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2709 - accuracy: 0.5163 - val_loss: 1.2542 - val_accuracy: 0.5296\n",
            "Epoch 227/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2901 - accuracy: 0.4935 - val_loss: 1.2603 - val_accuracy: 0.5249\n",
            "Epoch 228/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2630 - accuracy: 0.5248 - val_loss: 1.2490 - val_accuracy: 0.5187\n",
            "Epoch 229/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2966 - accuracy: 0.5142 - val_loss: 1.2438 - val_accuracy: 0.5514\n",
            "Epoch 230/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2629 - accuracy: 0.5412 - val_loss: 1.2406 - val_accuracy: 0.5514\n",
            "Epoch 231/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2701 - accuracy: 0.5243 - val_loss: 1.2585 - val_accuracy: 0.5452\n",
            "Epoch 232/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2405 - accuracy: 0.5243 - val_loss: 1.2426 - val_accuracy: 0.5483\n",
            "Epoch 233/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2526 - accuracy: 0.5260 - val_loss: 1.2364 - val_accuracy: 0.5452\n",
            "Epoch 234/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2692 - accuracy: 0.5047 - val_loss: 1.2394 - val_accuracy: 0.5530\n",
            "Epoch 235/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2567 - accuracy: 0.5292 - val_loss: 1.2380 - val_accuracy: 0.5467\n",
            "Epoch 236/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2507 - accuracy: 0.5373 - val_loss: 1.2449 - val_accuracy: 0.5343\n",
            "Epoch 237/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2348 - accuracy: 0.5445 - val_loss: 1.2375 - val_accuracy: 0.5530\n",
            "Epoch 238/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2243 - accuracy: 0.5511 - val_loss: 1.2386 - val_accuracy: 0.5467\n",
            "Epoch 239/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2592 - accuracy: 0.5160 - val_loss: 1.2393 - val_accuracy: 0.5389\n",
            "Epoch 240/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2460 - accuracy: 0.5094 - val_loss: 1.2360 - val_accuracy: 0.5545\n",
            "Epoch 241/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2594 - accuracy: 0.5063 - val_loss: 1.2312 - val_accuracy: 0.5514\n",
            "Epoch 242/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2223 - accuracy: 0.5406 - val_loss: 1.2416 - val_accuracy: 0.5343\n",
            "Epoch 243/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2452 - accuracy: 0.5358 - val_loss: 1.2261 - val_accuracy: 0.5452\n",
            "Epoch 244/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2006 - accuracy: 0.5641 - val_loss: 1.2304 - val_accuracy: 0.5327\n",
            "Epoch 245/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2336 - accuracy: 0.5387 - val_loss: 1.2245 - val_accuracy: 0.5530\n",
            "Epoch 246/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2169 - accuracy: 0.5320 - val_loss: 1.2226 - val_accuracy: 0.5654\n",
            "Epoch 247/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2282 - accuracy: 0.5404 - val_loss: 1.2306 - val_accuracy: 0.5576\n",
            "Epoch 248/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2710 - accuracy: 0.5185 - val_loss: 1.2273 - val_accuracy: 0.5498\n",
            "Epoch 249/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2315 - accuracy: 0.5452 - val_loss: 1.2194 - val_accuracy: 0.5530\n",
            "Epoch 250/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2076 - accuracy: 0.5443 - val_loss: 1.2299 - val_accuracy: 0.5592\n",
            "Epoch 251/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2491 - accuracy: 0.5447 - val_loss: 1.2255 - val_accuracy: 0.5436\n",
            "Epoch 252/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2131 - accuracy: 0.5353 - val_loss: 1.2372 - val_accuracy: 0.5545\n",
            "Epoch 253/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2438 - accuracy: 0.5257 - val_loss: 1.2251 - val_accuracy: 0.5514\n",
            "Epoch 254/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2274 - accuracy: 0.5380 - val_loss: 1.2179 - val_accuracy: 0.5498\n",
            "Epoch 255/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2523 - accuracy: 0.5292 - val_loss: 1.2215 - val_accuracy: 0.5545\n",
            "Epoch 256/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2226 - accuracy: 0.5485 - val_loss: 1.2157 - val_accuracy: 0.5389\n",
            "Epoch 257/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2252 - accuracy: 0.5481 - val_loss: 1.2134 - val_accuracy: 0.5592\n",
            "Epoch 258/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2083 - accuracy: 0.5496 - val_loss: 1.2252 - val_accuracy: 0.5483\n",
            "Epoch 259/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2062 - accuracy: 0.5391 - val_loss: 1.2106 - val_accuracy: 0.5592\n",
            "Epoch 260/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2233 - accuracy: 0.5499 - val_loss: 1.2188 - val_accuracy: 0.5514\n",
            "Epoch 261/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1982 - accuracy: 0.5554 - val_loss: 1.2129 - val_accuracy: 0.5592\n",
            "Epoch 262/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2018 - accuracy: 0.5455 - val_loss: 1.2092 - val_accuracy: 0.5530\n",
            "Epoch 263/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1797 - accuracy: 0.5643 - val_loss: 1.2129 - val_accuracy: 0.5327\n",
            "Epoch 264/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1633 - accuracy: 0.5781 - val_loss: 1.2201 - val_accuracy: 0.5483\n",
            "Epoch 265/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1521 - accuracy: 0.5669 - val_loss: 1.2139 - val_accuracy: 0.5514\n",
            "Epoch 266/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1868 - accuracy: 0.5513 - val_loss: 1.2108 - val_accuracy: 0.5717\n",
            "Epoch 267/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1946 - accuracy: 0.5411 - val_loss: 1.2115 - val_accuracy: 0.5561\n",
            "Epoch 268/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1972 - accuracy: 0.5459 - val_loss: 1.2086 - val_accuracy: 0.5467\n",
            "Epoch 269/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2277 - accuracy: 0.5202 - val_loss: 1.2173 - val_accuracy: 0.5514\n",
            "Epoch 270/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2038 - accuracy: 0.5440 - val_loss: 1.2131 - val_accuracy: 0.5639\n",
            "Epoch 271/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2104 - accuracy: 0.5579 - val_loss: 1.2352 - val_accuracy: 0.5312\n",
            "Epoch 272/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.2216 - accuracy: 0.5278 - val_loss: 1.2040 - val_accuracy: 0.5701\n",
            "Epoch 273/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2249 - accuracy: 0.5367 - val_loss: 1.2012 - val_accuracy: 0.5639\n",
            "Epoch 274/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1798 - accuracy: 0.5657 - val_loss: 1.1990 - val_accuracy: 0.5670\n",
            "Epoch 275/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2169 - accuracy: 0.5382 - val_loss: 1.2095 - val_accuracy: 0.5545\n",
            "Epoch 276/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1847 - accuracy: 0.5567 - val_loss: 1.2112 - val_accuracy: 0.5514\n",
            "Epoch 277/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.1955 - accuracy: 0.5492 - val_loss: 1.2076 - val_accuracy: 0.5607\n",
            "Epoch 278/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.1817 - accuracy: 0.5731 - val_loss: 1.2075 - val_accuracy: 0.5670\n",
            "Epoch 279/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1966 - accuracy: 0.5346 - val_loss: 1.1986 - val_accuracy: 0.5592\n",
            "Epoch 280/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2043 - accuracy: 0.5413 - val_loss: 1.1954 - val_accuracy: 0.5467\n",
            "Epoch 281/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1884 - accuracy: 0.5640 - val_loss: 1.2082 - val_accuracy: 0.5654\n",
            "Epoch 282/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1759 - accuracy: 0.5603 - val_loss: 1.2020 - val_accuracy: 0.5358\n",
            "Epoch 283/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1537 - accuracy: 0.5542 - val_loss: 1.2065 - val_accuracy: 0.5498\n",
            "Epoch 284/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1672 - accuracy: 0.5615 - val_loss: 1.1966 - val_accuracy: 0.5701\n",
            "Epoch 285/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1804 - accuracy: 0.5699 - val_loss: 1.1969 - val_accuracy: 0.5654\n",
            "Epoch 286/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1789 - accuracy: 0.5398 - val_loss: 1.1854 - val_accuracy: 0.5607\n",
            "Epoch 287/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1635 - accuracy: 0.5682 - val_loss: 1.1919 - val_accuracy: 0.5607\n",
            "Epoch 288/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1747 - accuracy: 0.5594 - val_loss: 1.1906 - val_accuracy: 0.5607\n",
            "Epoch 289/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1628 - accuracy: 0.5492 - val_loss: 1.1897 - val_accuracy: 0.5654\n",
            "Epoch 290/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1684 - accuracy: 0.5594 - val_loss: 1.1862 - val_accuracy: 0.5623\n",
            "Epoch 291/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1826 - accuracy: 0.5658 - val_loss: 1.1888 - val_accuracy: 0.5592\n",
            "Epoch 292/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1818 - accuracy: 0.5613 - val_loss: 1.1815 - val_accuracy: 0.5639\n",
            "Epoch 293/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1955 - accuracy: 0.5498 - val_loss: 1.1961 - val_accuracy: 0.5623\n",
            "Epoch 294/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.1408 - accuracy: 0.5783 - val_loss: 1.1852 - val_accuracy: 0.5561\n",
            "Epoch 295/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.1547 - accuracy: 0.5568 - val_loss: 1.1838 - val_accuracy: 0.5545\n",
            "Epoch 296/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.2004 - accuracy: 0.5496 - val_loss: 1.1901 - val_accuracy: 0.5654\n",
            "Epoch 297/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1529 - accuracy: 0.5936 - val_loss: 1.1749 - val_accuracy: 0.5623\n",
            "Epoch 298/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1671 - accuracy: 0.5586 - val_loss: 1.1827 - val_accuracy: 0.5623\n",
            "Epoch 299/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1273 - accuracy: 0.5757 - val_loss: 1.1820 - val_accuracy: 0.5576\n",
            "Epoch 300/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1570 - accuracy: 0.5753 - val_loss: 1.1869 - val_accuracy: 0.5436\n",
            "Epoch 301/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1546 - accuracy: 0.5586 - val_loss: 1.1822 - val_accuracy: 0.5763\n",
            "Epoch 302/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1496 - accuracy: 0.5703 - val_loss: 1.1783 - val_accuracy: 0.5685\n",
            "Epoch 303/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1272 - accuracy: 0.5609 - val_loss: 1.1684 - val_accuracy: 0.5514\n",
            "Epoch 304/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1692 - accuracy: 0.5529 - val_loss: 1.1753 - val_accuracy: 0.5717\n",
            "Epoch 305/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1562 - accuracy: 0.5573 - val_loss: 1.1654 - val_accuracy: 0.5670\n",
            "Epoch 306/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1679 - accuracy: 0.5602 - val_loss: 1.1785 - val_accuracy: 0.5561\n",
            "Epoch 307/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.1567 - accuracy: 0.5429 - val_loss: 1.1698 - val_accuracy: 0.5639\n",
            "Epoch 308/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1461 - accuracy: 0.5670 - val_loss: 1.1713 - val_accuracy: 0.5779\n",
            "Epoch 309/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1430 - accuracy: 0.5774 - val_loss: 1.1911 - val_accuracy: 0.5467\n",
            "Epoch 310/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1478 - accuracy: 0.5739 - val_loss: 1.1609 - val_accuracy: 0.5763\n",
            "Epoch 311/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1400 - accuracy: 0.5866 - val_loss: 1.1693 - val_accuracy: 0.5639\n",
            "Epoch 312/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1146 - accuracy: 0.5884 - val_loss: 1.1787 - val_accuracy: 0.5717\n",
            "Epoch 313/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1693 - accuracy: 0.5634 - val_loss: 1.1653 - val_accuracy: 0.5670\n",
            "Epoch 314/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1537 - accuracy: 0.5760 - val_loss: 1.1699 - val_accuracy: 0.5639\n",
            "Epoch 315/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1532 - accuracy: 0.5601 - val_loss: 1.1878 - val_accuracy: 0.5576\n",
            "Epoch 316/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1471 - accuracy: 0.5574 - val_loss: 1.1663 - val_accuracy: 0.5732\n",
            "Epoch 317/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1696 - accuracy: 0.5596 - val_loss: 1.1602 - val_accuracy: 0.5623\n",
            "Epoch 318/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1395 - accuracy: 0.5701 - val_loss: 1.1594 - val_accuracy: 0.5826\n",
            "Epoch 319/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1505 - accuracy: 0.5670 - val_loss: 1.1636 - val_accuracy: 0.5794\n",
            "Epoch 320/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1273 - accuracy: 0.5647 - val_loss: 1.1572 - val_accuracy: 0.5701\n",
            "Epoch 321/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1461 - accuracy: 0.5765 - val_loss: 1.1614 - val_accuracy: 0.5701\n",
            "Epoch 322/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1173 - accuracy: 0.5945 - val_loss: 1.1552 - val_accuracy: 0.5717\n",
            "Epoch 323/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0980 - accuracy: 0.5958 - val_loss: 1.1688 - val_accuracy: 0.5701\n",
            "Epoch 324/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.1612 - accuracy: 0.5720 - val_loss: 1.1668 - val_accuracy: 0.5576\n",
            "Epoch 325/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0981 - accuracy: 0.5994 - val_loss: 1.1548 - val_accuracy: 0.5717\n",
            "Epoch 326/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0877 - accuracy: 0.5865 - val_loss: 1.1723 - val_accuracy: 0.5685\n",
            "Epoch 327/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1093 - accuracy: 0.5716 - val_loss: 1.1634 - val_accuracy: 0.5857\n",
            "Epoch 328/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1537 - accuracy: 0.5626 - val_loss: 1.1575 - val_accuracy: 0.5701\n",
            "Epoch 329/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1141 - accuracy: 0.5688 - val_loss: 1.1482 - val_accuracy: 0.5685\n",
            "Epoch 330/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0740 - accuracy: 0.5989 - val_loss: 1.1479 - val_accuracy: 0.5623\n",
            "Epoch 331/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1025 - accuracy: 0.5795 - val_loss: 1.1497 - val_accuracy: 0.5732\n",
            "Epoch 332/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1376 - accuracy: 0.5753 - val_loss: 1.1413 - val_accuracy: 0.5701\n",
            "Epoch 333/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1021 - accuracy: 0.5897 - val_loss: 1.1498 - val_accuracy: 0.5748\n",
            "Epoch 334/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1147 - accuracy: 0.5766 - val_loss: 1.1551 - val_accuracy: 0.5717\n",
            "Epoch 335/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1149 - accuracy: 0.5861 - val_loss: 1.1523 - val_accuracy: 0.5763\n",
            "Epoch 336/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1300 - accuracy: 0.5815 - val_loss: 1.1412 - val_accuracy: 0.5888\n",
            "Epoch 337/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0964 - accuracy: 0.5764 - val_loss: 1.1466 - val_accuracy: 0.5748\n",
            "Epoch 338/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0981 - accuracy: 0.5881 - val_loss: 1.1449 - val_accuracy: 0.5857\n",
            "Epoch 339/1000\n",
            "121/121 [==============================] - 2s 15ms/step - loss: 1.0762 - accuracy: 0.6007 - val_loss: 1.1457 - val_accuracy: 0.5654\n",
            "Epoch 340/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1222 - accuracy: 0.5901 - val_loss: 1.1594 - val_accuracy: 0.5607\n",
            "Epoch 341/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1100 - accuracy: 0.5785 - val_loss: 1.1604 - val_accuracy: 0.5592\n",
            "Epoch 342/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1237 - accuracy: 0.5895 - val_loss: 1.1469 - val_accuracy: 0.5748\n",
            "Epoch 343/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1270 - accuracy: 0.5794 - val_loss: 1.1494 - val_accuracy: 0.5732\n",
            "Epoch 344/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1031 - accuracy: 0.6031 - val_loss: 1.1474 - val_accuracy: 0.5701\n",
            "Epoch 345/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1210 - accuracy: 0.5893 - val_loss: 1.1482 - val_accuracy: 0.5763\n",
            "Epoch 346/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1052 - accuracy: 0.5783 - val_loss: 1.1485 - val_accuracy: 0.5639\n",
            "Epoch 347/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1319 - accuracy: 0.5733 - val_loss: 1.1538 - val_accuracy: 0.5732\n",
            "Epoch 348/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0997 - accuracy: 0.5826 - val_loss: 1.1401 - val_accuracy: 0.5654\n",
            "Epoch 349/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0844 - accuracy: 0.5768 - val_loss: 1.1396 - val_accuracy: 0.5685\n",
            "Epoch 350/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1213 - accuracy: 0.5820 - val_loss: 1.1482 - val_accuracy: 0.5639\n",
            "Epoch 351/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0631 - accuracy: 0.5922 - val_loss: 1.1373 - val_accuracy: 0.5685\n",
            "Epoch 352/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1178 - accuracy: 0.5908 - val_loss: 1.1392 - val_accuracy: 0.5872\n",
            "Epoch 353/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0643 - accuracy: 0.6256 - val_loss: 1.1320 - val_accuracy: 0.5810\n",
            "Epoch 354/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0651 - accuracy: 0.6084 - val_loss: 1.1527 - val_accuracy: 0.5857\n",
            "Epoch 355/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1045 - accuracy: 0.5871 - val_loss: 1.1364 - val_accuracy: 0.5701\n",
            "Epoch 356/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1030 - accuracy: 0.5823 - val_loss: 1.1307 - val_accuracy: 0.5841\n",
            "Epoch 357/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1034 - accuracy: 0.5748 - val_loss: 1.1297 - val_accuracy: 0.5748\n",
            "Epoch 358/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0639 - accuracy: 0.5942 - val_loss: 1.1347 - val_accuracy: 0.5732\n",
            "Epoch 359/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0664 - accuracy: 0.5941 - val_loss: 1.1307 - val_accuracy: 0.5841\n",
            "Epoch 360/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0719 - accuracy: 0.5837 - val_loss: 1.1264 - val_accuracy: 0.5748\n",
            "Epoch 361/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0726 - accuracy: 0.5991 - val_loss: 1.1267 - val_accuracy: 0.5701\n",
            "Epoch 362/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0873 - accuracy: 0.5957 - val_loss: 1.1357 - val_accuracy: 0.5810\n",
            "Epoch 363/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0456 - accuracy: 0.6159 - val_loss: 1.1266 - val_accuracy: 0.5779\n",
            "Epoch 364/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0749 - accuracy: 0.6064 - val_loss: 1.1260 - val_accuracy: 0.5717\n",
            "Epoch 365/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0923 - accuracy: 0.5865 - val_loss: 1.1307 - val_accuracy: 0.5826\n",
            "Epoch 366/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1080 - accuracy: 0.5666 - val_loss: 1.1283 - val_accuracy: 0.5732\n",
            "Epoch 367/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0486 - accuracy: 0.6300 - val_loss: 1.1229 - val_accuracy: 0.5717\n",
            "Epoch 368/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0904 - accuracy: 0.5834 - val_loss: 1.1263 - val_accuracy: 0.5794\n",
            "Epoch 369/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1069 - accuracy: 0.5822 - val_loss: 1.1259 - val_accuracy: 0.5717\n",
            "Epoch 370/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1012 - accuracy: 0.5860 - val_loss: 1.1300 - val_accuracy: 0.5685\n",
            "Epoch 371/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0350 - accuracy: 0.6325 - val_loss: 1.1176 - val_accuracy: 0.5670\n",
            "Epoch 372/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0861 - accuracy: 0.5921 - val_loss: 1.1287 - val_accuracy: 0.5685\n",
            "Epoch 373/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0715 - accuracy: 0.5856 - val_loss: 1.1253 - val_accuracy: 0.5748\n",
            "Epoch 374/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0950 - accuracy: 0.5835 - val_loss: 1.1349 - val_accuracy: 0.5685\n",
            "Epoch 375/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0569 - accuracy: 0.6014 - val_loss: 1.1330 - val_accuracy: 0.5732\n",
            "Epoch 376/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0268 - accuracy: 0.6097 - val_loss: 1.1206 - val_accuracy: 0.5654\n",
            "Epoch 377/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1100 - accuracy: 0.5801 - val_loss: 1.1284 - val_accuracy: 0.5701\n",
            "Epoch 378/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0525 - accuracy: 0.6030 - val_loss: 1.1374 - val_accuracy: 0.5732\n",
            "Epoch 379/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0588 - accuracy: 0.6223 - val_loss: 1.1245 - val_accuracy: 0.5561\n",
            "Epoch 380/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0765 - accuracy: 0.6003 - val_loss: 1.1136 - val_accuracy: 0.5763\n",
            "Epoch 381/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.1038 - accuracy: 0.5930 - val_loss: 1.1157 - val_accuracy: 0.5810\n",
            "Epoch 382/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0597 - accuracy: 0.6081 - val_loss: 1.1192 - val_accuracy: 0.5732\n",
            "Epoch 383/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0434 - accuracy: 0.5895 - val_loss: 1.1185 - val_accuracy: 0.5685\n",
            "Epoch 384/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0942 - accuracy: 0.5942 - val_loss: 1.1219 - val_accuracy: 0.5841\n",
            "Epoch 385/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0787 - accuracy: 0.5750 - val_loss: 1.1143 - val_accuracy: 0.5794\n",
            "Epoch 386/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0878 - accuracy: 0.5919 - val_loss: 1.1209 - val_accuracy: 0.5888\n",
            "Epoch 387/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0648 - accuracy: 0.6011 - val_loss: 1.1116 - val_accuracy: 0.5872\n",
            "Epoch 388/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0643 - accuracy: 0.5936 - val_loss: 1.1182 - val_accuracy: 0.5779\n",
            "Epoch 389/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0479 - accuracy: 0.6122 - val_loss: 1.1277 - val_accuracy: 0.5670\n",
            "Epoch 390/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0568 - accuracy: 0.6108 - val_loss: 1.1086 - val_accuracy: 0.5903\n",
            "Epoch 391/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0802 - accuracy: 0.5980 - val_loss: 1.1098 - val_accuracy: 0.5639\n",
            "Epoch 392/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0564 - accuracy: 0.6127 - val_loss: 1.1113 - val_accuracy: 0.5685\n",
            "Epoch 393/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0377 - accuracy: 0.6077 - val_loss: 1.1112 - val_accuracy: 0.5841\n",
            "Epoch 394/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0906 - accuracy: 0.6040 - val_loss: 1.1187 - val_accuracy: 0.5732\n",
            "Epoch 395/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0501 - accuracy: 0.6220 - val_loss: 1.1176 - val_accuracy: 0.5592\n",
            "Epoch 396/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0202 - accuracy: 0.6189 - val_loss: 1.1172 - val_accuracy: 0.5639\n",
            "Epoch 397/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0616 - accuracy: 0.5983 - val_loss: 1.1172 - val_accuracy: 0.5717\n",
            "Epoch 398/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0312 - accuracy: 0.6093 - val_loss: 1.1136 - val_accuracy: 0.5670\n",
            "Epoch 399/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0236 - accuracy: 0.6100 - val_loss: 1.1056 - val_accuracy: 0.5810\n",
            "Epoch 400/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0270 - accuracy: 0.6214 - val_loss: 1.1121 - val_accuracy: 0.5841\n",
            "Epoch 401/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0240 - accuracy: 0.6243 - val_loss: 1.1040 - val_accuracy: 0.5763\n",
            "Epoch 402/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0403 - accuracy: 0.6209 - val_loss: 1.1182 - val_accuracy: 0.5592\n",
            "Epoch 403/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0557 - accuracy: 0.5981 - val_loss: 1.1003 - val_accuracy: 0.5935\n",
            "Epoch 404/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0559 - accuracy: 0.6132 - val_loss: 1.1186 - val_accuracy: 0.5763\n",
            "Epoch 405/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0603 - accuracy: 0.6125 - val_loss: 1.0959 - val_accuracy: 0.5810\n",
            "Epoch 406/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0160 - accuracy: 0.6251 - val_loss: 1.0961 - val_accuracy: 0.5826\n",
            "Epoch 407/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0507 - accuracy: 0.6000 - val_loss: 1.1078 - val_accuracy: 0.5576\n",
            "Epoch 408/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0209 - accuracy: 0.6137 - val_loss: 1.1080 - val_accuracy: 0.5701\n",
            "Epoch 409/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0289 - accuracy: 0.6083 - val_loss: 1.0987 - val_accuracy: 0.5779\n",
            "Epoch 410/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0635 - accuracy: 0.5994 - val_loss: 1.1013 - val_accuracy: 0.5810\n",
            "Epoch 411/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0602 - accuracy: 0.6100 - val_loss: 1.0969 - val_accuracy: 0.5748\n",
            "Epoch 412/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0476 - accuracy: 0.6202 - val_loss: 1.0862 - val_accuracy: 0.5826\n",
            "Epoch 413/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0363 - accuracy: 0.6125 - val_loss: 1.0918 - val_accuracy: 0.5903\n",
            "Epoch 414/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0498 - accuracy: 0.6069 - val_loss: 1.1018 - val_accuracy: 0.5903\n",
            "Epoch 415/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0352 - accuracy: 0.6091 - val_loss: 1.1152 - val_accuracy: 0.5903\n",
            "Epoch 416/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0133 - accuracy: 0.6272 - val_loss: 1.1061 - val_accuracy: 0.5732\n",
            "Epoch 417/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 0.9979 - accuracy: 0.6448 - val_loss: 1.0948 - val_accuracy: 0.5903\n",
            "Epoch 418/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 1.0150 - accuracy: 0.6302 - val_loss: 1.0941 - val_accuracy: 0.5888\n",
            "Epoch 419/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0549 - accuracy: 0.6135 - val_loss: 1.1019 - val_accuracy: 0.5763\n",
            "Epoch 420/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0101 - accuracy: 0.6298 - val_loss: 1.1030 - val_accuracy: 0.5857\n",
            "Epoch 421/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0179 - accuracy: 0.6164 - val_loss: 1.0892 - val_accuracy: 0.5950\n",
            "Epoch 422/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0326 - accuracy: 0.6273 - val_loss: 1.0919 - val_accuracy: 0.5857\n",
            "Epoch 423/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 0.9867 - accuracy: 0.6390 - val_loss: 1.0900 - val_accuracy: 0.5950\n",
            "Epoch 424/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0199 - accuracy: 0.6263 - val_loss: 1.1063 - val_accuracy: 0.5779\n",
            "Epoch 425/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0261 - accuracy: 0.6154 - val_loss: 1.0991 - val_accuracy: 0.5794\n",
            "Epoch 426/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9753 - accuracy: 0.6417 - val_loss: 1.0895 - val_accuracy: 0.5919\n",
            "Epoch 427/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0137 - accuracy: 0.6389 - val_loss: 1.0868 - val_accuracy: 0.5841\n",
            "Epoch 428/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0455 - accuracy: 0.6099 - val_loss: 1.1082 - val_accuracy: 0.5701\n",
            "Epoch 429/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9895 - accuracy: 0.6417 - val_loss: 1.0902 - val_accuracy: 0.5841\n",
            "Epoch 430/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0182 - accuracy: 0.6246 - val_loss: 1.0896 - val_accuracy: 0.5763\n",
            "Epoch 431/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0290 - accuracy: 0.6196 - val_loss: 1.0888 - val_accuracy: 0.5748\n",
            "Epoch 432/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9977 - accuracy: 0.6220 - val_loss: 1.1066 - val_accuracy: 0.5794\n",
            "Epoch 433/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0108 - accuracy: 0.6189 - val_loss: 1.0815 - val_accuracy: 0.5857\n",
            "Epoch 434/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9785 - accuracy: 0.6262 - val_loss: 1.0845 - val_accuracy: 0.5826\n",
            "Epoch 435/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9642 - accuracy: 0.6385 - val_loss: 1.0856 - val_accuracy: 0.5810\n",
            "Epoch 436/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0192 - accuracy: 0.6257 - val_loss: 1.0930 - val_accuracy: 0.5794\n",
            "Epoch 437/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0359 - accuracy: 0.6079 - val_loss: 1.0952 - val_accuracy: 0.5654\n",
            "Epoch 438/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9703 - accuracy: 0.6268 - val_loss: 1.0995 - val_accuracy: 0.5732\n",
            "Epoch 439/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0030 - accuracy: 0.6364 - val_loss: 1.1009 - val_accuracy: 0.5685\n",
            "Epoch 440/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9759 - accuracy: 0.6389 - val_loss: 1.0864 - val_accuracy: 0.5810\n",
            "Epoch 441/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9973 - accuracy: 0.6321 - val_loss: 1.0891 - val_accuracy: 0.5888\n",
            "Epoch 442/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0371 - accuracy: 0.5980 - val_loss: 1.0929 - val_accuracy: 0.5763\n",
            "Epoch 443/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0022 - accuracy: 0.6271 - val_loss: 1.0877 - val_accuracy: 0.5857\n",
            "Epoch 444/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0345 - accuracy: 0.6129 - val_loss: 1.0826 - val_accuracy: 0.5779\n",
            "Epoch 445/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0022 - accuracy: 0.6227 - val_loss: 1.0705 - val_accuracy: 0.5841\n",
            "Epoch 446/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9926 - accuracy: 0.6313 - val_loss: 1.0840 - val_accuracy: 0.5810\n",
            "Epoch 447/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9988 - accuracy: 0.6285 - val_loss: 1.0834 - val_accuracy: 0.5857\n",
            "Epoch 448/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0274 - accuracy: 0.6170 - val_loss: 1.0775 - val_accuracy: 0.5826\n",
            "Epoch 449/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9902 - accuracy: 0.6329 - val_loss: 1.0761 - val_accuracy: 0.5903\n",
            "Epoch 450/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0125 - accuracy: 0.6160 - val_loss: 1.0709 - val_accuracy: 0.5872\n",
            "Epoch 451/1000\n",
            "121/121 [==============================] - 2s 16ms/step - loss: 0.9867 - accuracy: 0.6494 - val_loss: 1.0759 - val_accuracy: 0.5888\n",
            "Epoch 452/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9861 - accuracy: 0.6327 - val_loss: 1.0731 - val_accuracy: 0.5919\n",
            "Epoch 453/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9836 - accuracy: 0.6357 - val_loss: 1.0724 - val_accuracy: 0.5857\n",
            "Epoch 454/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0085 - accuracy: 0.6218 - val_loss: 1.0774 - val_accuracy: 0.5810\n",
            "Epoch 455/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9815 - accuracy: 0.6130 - val_loss: 1.0740 - val_accuracy: 0.5779\n",
            "Epoch 456/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9947 - accuracy: 0.6352 - val_loss: 1.0723 - val_accuracy: 0.5841\n",
            "Epoch 457/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9961 - accuracy: 0.6249 - val_loss: 1.0737 - val_accuracy: 0.5888\n",
            "Epoch 458/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9847 - accuracy: 0.6280 - val_loss: 1.0693 - val_accuracy: 0.5950\n",
            "Epoch 459/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9733 - accuracy: 0.6471 - val_loss: 1.0687 - val_accuracy: 0.5903\n",
            "Epoch 460/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9890 - accuracy: 0.6320 - val_loss: 1.0834 - val_accuracy: 0.5841\n",
            "Epoch 461/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9881 - accuracy: 0.6312 - val_loss: 1.0727 - val_accuracy: 0.5857\n",
            "Epoch 462/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9651 - accuracy: 0.6488 - val_loss: 1.0646 - val_accuracy: 0.5779\n",
            "Epoch 463/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0064 - accuracy: 0.6183 - val_loss: 1.0746 - val_accuracy: 0.5639\n",
            "Epoch 464/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9870 - accuracy: 0.6424 - val_loss: 1.0743 - val_accuracy: 0.5857\n",
            "Epoch 465/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9803 - accuracy: 0.6296 - val_loss: 1.0668 - val_accuracy: 0.5903\n",
            "Epoch 466/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9763 - accuracy: 0.6439 - val_loss: 1.0765 - val_accuracy: 0.5872\n",
            "Epoch 467/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0050 - accuracy: 0.6200 - val_loss: 1.0801 - val_accuracy: 0.5888\n",
            "Epoch 468/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9907 - accuracy: 0.6427 - val_loss: 1.0627 - val_accuracy: 0.5919\n",
            "Epoch 469/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9467 - accuracy: 0.6613 - val_loss: 1.0739 - val_accuracy: 0.5888\n",
            "Epoch 470/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9715 - accuracy: 0.6377 - val_loss: 1.0737 - val_accuracy: 0.5857\n",
            "Epoch 471/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9769 - accuracy: 0.6459 - val_loss: 1.0816 - val_accuracy: 0.5872\n",
            "Epoch 472/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9812 - accuracy: 0.6301 - val_loss: 1.0626 - val_accuracy: 0.5841\n",
            "Epoch 473/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9914 - accuracy: 0.6365 - val_loss: 1.0554 - val_accuracy: 0.5966\n",
            "Epoch 474/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9652 - accuracy: 0.6247 - val_loss: 1.0593 - val_accuracy: 0.5981\n",
            "Epoch 475/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9602 - accuracy: 0.6413 - val_loss: 1.0540 - val_accuracy: 0.6012\n",
            "Epoch 476/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9860 - accuracy: 0.6240 - val_loss: 1.0645 - val_accuracy: 0.5966\n",
            "Epoch 477/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9801 - accuracy: 0.6322 - val_loss: 1.0652 - val_accuracy: 0.5794\n",
            "Epoch 478/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9949 - accuracy: 0.6357 - val_loss: 1.0636 - val_accuracy: 0.5888\n",
            "Epoch 479/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9659 - accuracy: 0.6409 - val_loss: 1.0670 - val_accuracy: 0.5794\n",
            "Epoch 480/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9418 - accuracy: 0.6554 - val_loss: 1.0619 - val_accuracy: 0.5872\n",
            "Epoch 481/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9464 - accuracy: 0.6615 - val_loss: 1.0568 - val_accuracy: 0.5872\n",
            "Epoch 482/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9509 - accuracy: 0.6669 - val_loss: 1.0631 - val_accuracy: 0.5810\n",
            "Epoch 483/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9468 - accuracy: 0.6472 - val_loss: 1.0630 - val_accuracy: 0.5950\n",
            "Epoch 484/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9908 - accuracy: 0.6279 - val_loss: 1.0595 - val_accuracy: 0.5888\n",
            "Epoch 485/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9490 - accuracy: 0.6370 - val_loss: 1.0651 - val_accuracy: 0.5810\n",
            "Epoch 486/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9805 - accuracy: 0.6258 - val_loss: 1.0669 - val_accuracy: 0.5654\n",
            "Epoch 487/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9698 - accuracy: 0.6416 - val_loss: 1.0884 - val_accuracy: 0.5685\n",
            "Epoch 488/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9628 - accuracy: 0.6527 - val_loss: 1.0623 - val_accuracy: 0.6012\n",
            "Epoch 489/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9347 - accuracy: 0.6692 - val_loss: 1.0713 - val_accuracy: 0.5794\n",
            "Epoch 490/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9621 - accuracy: 0.6481 - val_loss: 1.0579 - val_accuracy: 0.5763\n",
            "Epoch 491/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 1.0045 - accuracy: 0.6160 - val_loss: 1.0619 - val_accuracy: 0.5857\n",
            "Epoch 492/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9631 - accuracy: 0.6416 - val_loss: 1.0547 - val_accuracy: 0.5903\n",
            "Epoch 493/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9783 - accuracy: 0.6434 - val_loss: 1.0739 - val_accuracy: 0.5919\n",
            "Epoch 494/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9747 - accuracy: 0.6361 - val_loss: 1.0518 - val_accuracy: 0.5841\n",
            "Epoch 495/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9512 - accuracy: 0.6476 - val_loss: 1.0626 - val_accuracy: 0.5935\n",
            "Epoch 496/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9855 - accuracy: 0.6298 - val_loss: 1.0499 - val_accuracy: 0.5841\n",
            "Epoch 497/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9389 - accuracy: 0.6443 - val_loss: 1.0598 - val_accuracy: 0.5794\n",
            "Epoch 498/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9644 - accuracy: 0.6381 - val_loss: 1.0548 - val_accuracy: 0.5950\n",
            "Epoch 499/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9561 - accuracy: 0.6480 - val_loss: 1.0547 - val_accuracy: 0.5935\n",
            "Epoch 500/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9651 - accuracy: 0.6434 - val_loss: 1.0546 - val_accuracy: 0.5717\n",
            "Epoch 501/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9632 - accuracy: 0.6455 - val_loss: 1.0543 - val_accuracy: 0.5950\n",
            "Epoch 502/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9503 - accuracy: 0.6506 - val_loss: 1.0483 - val_accuracy: 0.5841\n",
            "Epoch 503/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9274 - accuracy: 0.6588 - val_loss: 1.0581 - val_accuracy: 0.5857\n",
            "Epoch 504/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9218 - accuracy: 0.6593 - val_loss: 1.0425 - val_accuracy: 0.5966\n",
            "Epoch 505/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9517 - accuracy: 0.6460 - val_loss: 1.0505 - val_accuracy: 0.5903\n",
            "Epoch 506/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9600 - accuracy: 0.6443 - val_loss: 1.0493 - val_accuracy: 0.5903\n",
            "Epoch 507/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9700 - accuracy: 0.6559 - val_loss: 1.0514 - val_accuracy: 0.5950\n",
            "Epoch 508/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9510 - accuracy: 0.6496 - val_loss: 1.0568 - val_accuracy: 0.5794\n",
            "Epoch 509/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9368 - accuracy: 0.6453 - val_loss: 1.0589 - val_accuracy: 0.5810\n",
            "Epoch 510/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9773 - accuracy: 0.6501 - val_loss: 1.0423 - val_accuracy: 0.5872\n",
            "Epoch 511/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9656 - accuracy: 0.6523 - val_loss: 1.0532 - val_accuracy: 0.5919\n",
            "Epoch 512/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9500 - accuracy: 0.6342 - val_loss: 1.0552 - val_accuracy: 0.5966\n",
            "Epoch 513/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9053 - accuracy: 0.6692 - val_loss: 1.0575 - val_accuracy: 0.5872\n",
            "Epoch 514/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9394 - accuracy: 0.6548 - val_loss: 1.0492 - val_accuracy: 0.5888\n",
            "Epoch 515/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9411 - accuracy: 0.6713 - val_loss: 1.0557 - val_accuracy: 0.5857\n",
            "Epoch 516/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9520 - accuracy: 0.6500 - val_loss: 1.0494 - val_accuracy: 0.5966\n",
            "Epoch 517/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9438 - accuracy: 0.6488 - val_loss: 1.0559 - val_accuracy: 0.5935\n",
            "Epoch 518/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9781 - accuracy: 0.6487 - val_loss: 1.0478 - val_accuracy: 0.5872\n",
            "Epoch 519/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9125 - accuracy: 0.6802 - val_loss: 1.0586 - val_accuracy: 0.5763\n",
            "Epoch 520/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9357 - accuracy: 0.6616 - val_loss: 1.0453 - val_accuracy: 0.5841\n",
            "Epoch 521/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9292 - accuracy: 0.6474 - val_loss: 1.0494 - val_accuracy: 0.5903\n",
            "Epoch 522/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9508 - accuracy: 0.6532 - val_loss: 1.0468 - val_accuracy: 0.5935\n",
            "Epoch 523/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9250 - accuracy: 0.6679 - val_loss: 1.0377 - val_accuracy: 0.5903\n",
            "Epoch 524/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9127 - accuracy: 0.6687 - val_loss: 1.0424 - val_accuracy: 0.5903\n",
            "Epoch 525/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9359 - accuracy: 0.6570 - val_loss: 1.0512 - val_accuracy: 0.5857\n",
            "Epoch 526/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9255 - accuracy: 0.6380 - val_loss: 1.0425 - val_accuracy: 0.5950\n",
            "Epoch 527/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9322 - accuracy: 0.6430 - val_loss: 1.0495 - val_accuracy: 0.5872\n",
            "Epoch 528/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9233 - accuracy: 0.6686 - val_loss: 1.0462 - val_accuracy: 0.5826\n",
            "Epoch 529/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9641 - accuracy: 0.6478 - val_loss: 1.0476 - val_accuracy: 0.5888\n",
            "Epoch 530/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9033 - accuracy: 0.6537 - val_loss: 1.0569 - val_accuracy: 0.5935\n",
            "Epoch 531/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9521 - accuracy: 0.6391 - val_loss: 1.0555 - val_accuracy: 0.5888\n",
            "Epoch 532/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9131 - accuracy: 0.6642 - val_loss: 1.0351 - val_accuracy: 0.5935\n",
            "Epoch 533/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9410 - accuracy: 0.6460 - val_loss: 1.0435 - val_accuracy: 0.5903\n",
            "Epoch 534/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9452 - accuracy: 0.6406 - val_loss: 1.0422 - val_accuracy: 0.6044\n",
            "Epoch 535/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9193 - accuracy: 0.6499 - val_loss: 1.0484 - val_accuracy: 0.5872\n",
            "Epoch 536/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9277 - accuracy: 0.6496 - val_loss: 1.0333 - val_accuracy: 0.6012\n",
            "Epoch 537/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9251 - accuracy: 0.6526 - val_loss: 1.0497 - val_accuracy: 0.5872\n",
            "Epoch 538/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9067 - accuracy: 0.6750 - val_loss: 1.0451 - val_accuracy: 0.5981\n",
            "Epoch 539/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9334 - accuracy: 0.6576 - val_loss: 1.0320 - val_accuracy: 0.5888\n",
            "Epoch 540/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9291 - accuracy: 0.6392 - val_loss: 1.0475 - val_accuracy: 0.5981\n",
            "Epoch 541/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8935 - accuracy: 0.6645 - val_loss: 1.0403 - val_accuracy: 0.5903\n",
            "Epoch 542/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9302 - accuracy: 0.6386 - val_loss: 1.0460 - val_accuracy: 0.6044\n",
            "Epoch 543/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9218 - accuracy: 0.6693 - val_loss: 1.0583 - val_accuracy: 0.5841\n",
            "Epoch 544/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9474 - accuracy: 0.6444 - val_loss: 1.0297 - val_accuracy: 0.5950\n",
            "Epoch 545/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9394 - accuracy: 0.6550 - val_loss: 1.0396 - val_accuracy: 0.5935\n",
            "Epoch 546/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9409 - accuracy: 0.6511 - val_loss: 1.0276 - val_accuracy: 0.5997\n",
            "Epoch 547/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9037 - accuracy: 0.6661 - val_loss: 1.0339 - val_accuracy: 0.5919\n",
            "Epoch 548/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9680 - accuracy: 0.6526 - val_loss: 1.0336 - val_accuracy: 0.5826\n",
            "Epoch 549/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9447 - accuracy: 0.6351 - val_loss: 1.0308 - val_accuracy: 0.5919\n",
            "Epoch 550/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9288 - accuracy: 0.6669 - val_loss: 1.0326 - val_accuracy: 0.5935\n",
            "Epoch 551/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9302 - accuracy: 0.6379 - val_loss: 1.0397 - val_accuracy: 0.5903\n",
            "Epoch 552/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9380 - accuracy: 0.6567 - val_loss: 1.0399 - val_accuracy: 0.5872\n",
            "Epoch 553/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9545 - accuracy: 0.6203 - val_loss: 1.0429 - val_accuracy: 0.5763\n",
            "Epoch 554/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.8999 - accuracy: 0.6584 - val_loss: 1.0298 - val_accuracy: 0.6059\n",
            "Epoch 555/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9303 - accuracy: 0.6583 - val_loss: 1.0409 - val_accuracy: 0.5919\n",
            "Epoch 556/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9395 - accuracy: 0.6583 - val_loss: 1.0294 - val_accuracy: 0.5966\n",
            "Epoch 557/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8845 - accuracy: 0.6883 - val_loss: 1.0418 - val_accuracy: 0.6012\n",
            "Epoch 558/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9260 - accuracy: 0.6519 - val_loss: 1.0389 - val_accuracy: 0.5997\n",
            "Epoch 559/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9021 - accuracy: 0.6519 - val_loss: 1.0330 - val_accuracy: 0.5919\n",
            "Epoch 560/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9316 - accuracy: 0.6435 - val_loss: 1.0338 - val_accuracy: 0.5950\n",
            "Epoch 561/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9225 - accuracy: 0.6425 - val_loss: 1.0347 - val_accuracy: 0.5888\n",
            "Epoch 562/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9000 - accuracy: 0.6766 - val_loss: 1.0185 - val_accuracy: 0.6012\n",
            "Epoch 563/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9367 - accuracy: 0.6304 - val_loss: 1.0304 - val_accuracy: 0.5966\n",
            "Epoch 564/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9035 - accuracy: 0.6700 - val_loss: 1.0260 - val_accuracy: 0.5981\n",
            "Epoch 565/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.9144 - accuracy: 0.6472 - val_loss: 1.0278 - val_accuracy: 0.6012\n",
            "Epoch 566/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9038 - accuracy: 0.6719 - val_loss: 1.0351 - val_accuracy: 0.5857\n",
            "Epoch 567/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8977 - accuracy: 0.6675 - val_loss: 1.0225 - val_accuracy: 0.6012\n",
            "Epoch 568/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9105 - accuracy: 0.6578 - val_loss: 1.0348 - val_accuracy: 0.5950\n",
            "Epoch 569/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.8927 - accuracy: 0.6640 - val_loss: 1.0296 - val_accuracy: 0.5919\n",
            "Epoch 570/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9046 - accuracy: 0.6745 - val_loss: 1.0244 - val_accuracy: 0.5888\n",
            "Epoch 571/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9477 - accuracy: 0.6525 - val_loss: 1.0323 - val_accuracy: 0.5841\n",
            "Epoch 572/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9177 - accuracy: 0.6579 - val_loss: 1.0290 - val_accuracy: 0.5826\n",
            "Epoch 573/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8810 - accuracy: 0.6697 - val_loss: 1.0244 - val_accuracy: 0.5919\n",
            "Epoch 574/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8750 - accuracy: 0.6791 - val_loss: 1.0289 - val_accuracy: 0.5872\n",
            "Epoch 575/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9031 - accuracy: 0.6549 - val_loss: 1.0301 - val_accuracy: 0.5950\n",
            "Epoch 576/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9307 - accuracy: 0.6648 - val_loss: 1.0272 - val_accuracy: 0.5950\n",
            "Epoch 577/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8818 - accuracy: 0.6780 - val_loss: 1.0434 - val_accuracy: 0.5872\n",
            "Epoch 578/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9010 - accuracy: 0.6705 - val_loss: 1.0317 - val_accuracy: 0.5888\n",
            "Epoch 579/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8994 - accuracy: 0.6634 - val_loss: 1.0337 - val_accuracy: 0.5919\n",
            "Epoch 580/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8523 - accuracy: 0.6805 - val_loss: 1.0286 - val_accuracy: 0.5966\n",
            "Epoch 581/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8976 - accuracy: 0.6705 - val_loss: 1.0272 - val_accuracy: 0.5950\n",
            "Epoch 582/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8844 - accuracy: 0.6657 - val_loss: 1.0406 - val_accuracy: 0.5935\n",
            "Epoch 583/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8810 - accuracy: 0.6661 - val_loss: 1.0206 - val_accuracy: 0.5981\n",
            "Epoch 584/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9240 - accuracy: 0.6516 - val_loss: 1.0196 - val_accuracy: 0.6028\n",
            "Epoch 585/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9029 - accuracy: 0.6778 - val_loss: 1.0245 - val_accuracy: 0.6075\n",
            "Epoch 586/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8886 - accuracy: 0.6787 - val_loss: 1.0159 - val_accuracy: 0.5981\n",
            "Epoch 587/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9049 - accuracy: 0.6609 - val_loss: 1.0264 - val_accuracy: 0.5888\n",
            "Epoch 588/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8923 - accuracy: 0.6603 - val_loss: 1.0230 - val_accuracy: 0.5872\n",
            "Epoch 589/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8659 - accuracy: 0.6831 - val_loss: 1.0155 - val_accuracy: 0.5981\n",
            "Epoch 590/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8965 - accuracy: 0.6592 - val_loss: 1.0335 - val_accuracy: 0.5919\n",
            "Epoch 591/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8985 - accuracy: 0.6719 - val_loss: 1.0305 - val_accuracy: 0.5919\n",
            "Epoch 592/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8775 - accuracy: 0.6933 - val_loss: 1.0136 - val_accuracy: 0.6090\n",
            "Epoch 593/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8879 - accuracy: 0.6650 - val_loss: 1.0148 - val_accuracy: 0.6121\n",
            "Epoch 594/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8516 - accuracy: 0.6887 - val_loss: 1.0270 - val_accuracy: 0.5981\n",
            "Epoch 595/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8670 - accuracy: 0.6897 - val_loss: 1.0230 - val_accuracy: 0.5903\n",
            "Epoch 596/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8596 - accuracy: 0.6879 - val_loss: 1.0159 - val_accuracy: 0.5966\n",
            "Epoch 597/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8954 - accuracy: 0.6691 - val_loss: 1.0108 - val_accuracy: 0.5981\n",
            "Epoch 598/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9164 - accuracy: 0.6566 - val_loss: 1.0153 - val_accuracy: 0.5950\n",
            "Epoch 599/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9163 - accuracy: 0.6467 - val_loss: 1.0216 - val_accuracy: 0.5950\n",
            "Epoch 600/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8810 - accuracy: 0.6802 - val_loss: 1.0184 - val_accuracy: 0.5950\n",
            "Epoch 601/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8811 - accuracy: 0.6806 - val_loss: 1.0175 - val_accuracy: 0.5981\n",
            "Epoch 602/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8867 - accuracy: 0.6695 - val_loss: 1.0142 - val_accuracy: 0.5935\n",
            "Epoch 603/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8655 - accuracy: 0.6850 - val_loss: 1.0146 - val_accuracy: 0.5966\n",
            "Epoch 604/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8482 - accuracy: 0.6946 - val_loss: 1.0166 - val_accuracy: 0.5919\n",
            "Epoch 605/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8485 - accuracy: 0.6974 - val_loss: 1.0242 - val_accuracy: 0.5981\n",
            "Epoch 606/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8794 - accuracy: 0.6735 - val_loss: 1.0180 - val_accuracy: 0.5997\n",
            "Epoch 607/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8683 - accuracy: 0.6785 - val_loss: 1.0183 - val_accuracy: 0.5966\n",
            "Epoch 608/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8683 - accuracy: 0.6846 - val_loss: 1.0049 - val_accuracy: 0.6028\n",
            "Epoch 609/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8642 - accuracy: 0.6946 - val_loss: 1.0177 - val_accuracy: 0.6121\n",
            "Epoch 610/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8962 - accuracy: 0.6881 - val_loss: 1.0017 - val_accuracy: 0.6012\n",
            "Epoch 611/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8818 - accuracy: 0.6741 - val_loss: 1.0111 - val_accuracy: 0.5981\n",
            "Epoch 612/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8992 - accuracy: 0.6616 - val_loss: 1.0068 - val_accuracy: 0.6106\n",
            "Epoch 613/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8301 - accuracy: 0.7007 - val_loss: 1.0069 - val_accuracy: 0.6121\n",
            "Epoch 614/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8675 - accuracy: 0.6747 - val_loss: 1.0077 - val_accuracy: 0.6106\n",
            "Epoch 615/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8548 - accuracy: 0.6815 - val_loss: 1.0174 - val_accuracy: 0.6044\n",
            "Epoch 616/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8928 - accuracy: 0.6892 - val_loss: 1.0136 - val_accuracy: 0.6075\n",
            "Epoch 617/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8726 - accuracy: 0.6594 - val_loss: 1.0121 - val_accuracy: 0.6106\n",
            "Epoch 618/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8611 - accuracy: 0.6840 - val_loss: 1.0068 - val_accuracy: 0.6028\n",
            "Epoch 619/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8492 - accuracy: 0.6958 - val_loss: 1.0145 - val_accuracy: 0.6028\n",
            "Epoch 620/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8328 - accuracy: 0.7015 - val_loss: 1.0083 - val_accuracy: 0.6106\n",
            "Epoch 621/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8823 - accuracy: 0.6717 - val_loss: 1.0136 - val_accuracy: 0.6121\n",
            "Epoch 622/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8853 - accuracy: 0.6718 - val_loss: 1.0114 - val_accuracy: 0.5981\n",
            "Epoch 623/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8848 - accuracy: 0.6587 - val_loss: 1.0057 - val_accuracy: 0.6059\n",
            "Epoch 624/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8707 - accuracy: 0.6818 - val_loss: 1.0072 - val_accuracy: 0.5981\n",
            "Epoch 625/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9028 - accuracy: 0.6598 - val_loss: 1.0069 - val_accuracy: 0.6106\n",
            "Epoch 626/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.8916 - accuracy: 0.6786 - val_loss: 1.0084 - val_accuracy: 0.6075\n",
            "Epoch 627/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9102 - accuracy: 0.6581 - val_loss: 1.0082 - val_accuracy: 0.6090\n",
            "Epoch 628/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8398 - accuracy: 0.6990 - val_loss: 1.0053 - val_accuracy: 0.6106\n",
            "Epoch 629/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9014 - accuracy: 0.6600 - val_loss: 0.9996 - val_accuracy: 0.6153\n",
            "Epoch 630/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.8502 - accuracy: 0.6920 - val_loss: 1.0015 - val_accuracy: 0.6075\n",
            "Epoch 631/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8347 - accuracy: 0.6914 - val_loss: 1.0018 - val_accuracy: 0.6059\n",
            "Epoch 632/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8524 - accuracy: 0.6893 - val_loss: 0.9938 - val_accuracy: 0.6106\n",
            "Epoch 633/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8789 - accuracy: 0.6819 - val_loss: 1.0031 - val_accuracy: 0.5872\n",
            "Epoch 634/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8262 - accuracy: 0.7066 - val_loss: 1.0042 - val_accuracy: 0.5950\n",
            "Epoch 635/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8211 - accuracy: 0.6901 - val_loss: 1.0013 - val_accuracy: 0.5997\n",
            "Epoch 636/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.9031 - accuracy: 0.6562 - val_loss: 1.0167 - val_accuracy: 0.6044\n",
            "Epoch 637/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8526 - accuracy: 0.6795 - val_loss: 1.0035 - val_accuracy: 0.5872\n",
            "Epoch 638/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8387 - accuracy: 0.6982 - val_loss: 1.0116 - val_accuracy: 0.5966\n",
            "Epoch 639/1000\n",
            "121/121 [==============================] - 2s 17ms/step - loss: 0.8574 - accuracy: 0.6840 - val_loss: 1.0001 - val_accuracy: 0.5919\n",
            "Epoch 640/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8443 - accuracy: 0.6727 - val_loss: 1.0107 - val_accuracy: 0.5997\n",
            "Epoch 641/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8730 - accuracy: 0.6757 - val_loss: 0.9997 - val_accuracy: 0.6075\n",
            "Epoch 642/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8434 - accuracy: 0.6938 - val_loss: 1.0032 - val_accuracy: 0.6044\n",
            "Epoch 643/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8762 - accuracy: 0.6883 - val_loss: 0.9941 - val_accuracy: 0.6044\n",
            "Epoch 644/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8461 - accuracy: 0.6884 - val_loss: 1.0024 - val_accuracy: 0.6012\n",
            "Epoch 645/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8157 - accuracy: 0.6973 - val_loss: 0.9931 - val_accuracy: 0.5966\n",
            "Epoch 646/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8564 - accuracy: 0.6874 - val_loss: 1.0081 - val_accuracy: 0.6075\n",
            "Epoch 647/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8169 - accuracy: 0.7090 - val_loss: 0.9937 - val_accuracy: 0.6137\n",
            "Epoch 648/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8874 - accuracy: 0.6585 - val_loss: 1.0072 - val_accuracy: 0.6075\n",
            "Epoch 649/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8800 - accuracy: 0.6817 - val_loss: 1.0079 - val_accuracy: 0.6059\n",
            "Epoch 650/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8417 - accuracy: 0.6789 - val_loss: 0.9981 - val_accuracy: 0.6044\n",
            "Epoch 651/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8334 - accuracy: 0.6997 - val_loss: 1.0062 - val_accuracy: 0.5903\n",
            "Epoch 652/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8407 - accuracy: 0.6930 - val_loss: 1.0185 - val_accuracy: 0.6044\n",
            "Epoch 653/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8296 - accuracy: 0.6955 - val_loss: 0.9951 - val_accuracy: 0.6168\n",
            "Epoch 654/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8444 - accuracy: 0.6938 - val_loss: 0.9984 - val_accuracy: 0.6121\n",
            "Epoch 655/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8595 - accuracy: 0.6865 - val_loss: 0.9977 - val_accuracy: 0.6121\n",
            "Epoch 656/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8492 - accuracy: 0.6761 - val_loss: 0.9991 - val_accuracy: 0.6137\n",
            "Epoch 657/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8460 - accuracy: 0.6750 - val_loss: 1.0142 - val_accuracy: 0.6106\n",
            "Epoch 658/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8311 - accuracy: 0.7022 - val_loss: 1.0088 - val_accuracy: 0.5935\n",
            "Epoch 659/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8498 - accuracy: 0.6829 - val_loss: 0.9951 - val_accuracy: 0.6059\n",
            "Epoch 660/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8245 - accuracy: 0.7009 - val_loss: 1.0117 - val_accuracy: 0.5966\n",
            "Epoch 661/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8248 - accuracy: 0.6887 - val_loss: 0.9911 - val_accuracy: 0.6106\n",
            "Epoch 662/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8612 - accuracy: 0.6759 - val_loss: 1.0004 - val_accuracy: 0.5981\n",
            "Epoch 663/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8993 - accuracy: 0.6516 - val_loss: 1.0049 - val_accuracy: 0.6012\n",
            "Epoch 664/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8446 - accuracy: 0.6945 - val_loss: 1.0024 - val_accuracy: 0.5981\n",
            "Epoch 665/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8471 - accuracy: 0.6825 - val_loss: 1.0011 - val_accuracy: 0.6075\n",
            "Epoch 666/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8426 - accuracy: 0.6824 - val_loss: 1.0063 - val_accuracy: 0.5966\n",
            "Epoch 667/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8270 - accuracy: 0.6940 - val_loss: 0.9982 - val_accuracy: 0.6137\n",
            "Epoch 668/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8686 - accuracy: 0.6942 - val_loss: 0.9950 - val_accuracy: 0.6059\n",
            "Epoch 669/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8362 - accuracy: 0.6894 - val_loss: 0.9978 - val_accuracy: 0.6044\n",
            "Epoch 670/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8249 - accuracy: 0.6997 - val_loss: 0.9972 - val_accuracy: 0.6121\n",
            "Epoch 671/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8561 - accuracy: 0.6884 - val_loss: 0.9958 - val_accuracy: 0.5981\n",
            "Epoch 672/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8196 - accuracy: 0.7002 - val_loss: 1.0002 - val_accuracy: 0.6012\n",
            "Epoch 673/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8158 - accuracy: 0.6982 - val_loss: 1.0059 - val_accuracy: 0.6059\n",
            "Epoch 674/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8147 - accuracy: 0.6874 - val_loss: 0.9942 - val_accuracy: 0.6137\n",
            "Epoch 675/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8498 - accuracy: 0.6895 - val_loss: 1.0068 - val_accuracy: 0.6059\n",
            "Epoch 676/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8502 - accuracy: 0.6834 - val_loss: 0.9874 - val_accuracy: 0.6153\n",
            "Epoch 677/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8241 - accuracy: 0.6979 - val_loss: 0.9892 - val_accuracy: 0.6044\n",
            "Epoch 678/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8216 - accuracy: 0.6977 - val_loss: 0.9902 - val_accuracy: 0.5981\n",
            "Epoch 679/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.7979 - accuracy: 0.7070 - val_loss: 0.9837 - val_accuracy: 0.6137\n",
            "Epoch 680/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8273 - accuracy: 0.7099 - val_loss: 0.9911 - val_accuracy: 0.6028\n",
            "Epoch 681/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8340 - accuracy: 0.6921 - val_loss: 0.9933 - val_accuracy: 0.5997\n",
            "Epoch 682/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8258 - accuracy: 0.6955 - val_loss: 0.9823 - val_accuracy: 0.6106\n",
            "Epoch 683/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8129 - accuracy: 0.6998 - val_loss: 0.9809 - val_accuracy: 0.6090\n",
            "Epoch 684/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.7947 - accuracy: 0.7147 - val_loss: 0.9989 - val_accuracy: 0.6044\n",
            "Epoch 685/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8215 - accuracy: 0.6916 - val_loss: 0.9812 - val_accuracy: 0.6059\n",
            "Epoch 686/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8242 - accuracy: 0.6962 - val_loss: 1.0010 - val_accuracy: 0.6106\n",
            "Epoch 687/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8173 - accuracy: 0.7080 - val_loss: 0.9878 - val_accuracy: 0.6153\n",
            "Epoch 688/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.7996 - accuracy: 0.7099 - val_loss: 1.0027 - val_accuracy: 0.6153\n",
            "Epoch 689/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8366 - accuracy: 0.6990 - val_loss: 0.9905 - val_accuracy: 0.6106\n",
            "Epoch 690/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8324 - accuracy: 0.7054 - val_loss: 0.9836 - val_accuracy: 0.6075\n",
            "Epoch 691/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8668 - accuracy: 0.6729 - val_loss: 0.9856 - val_accuracy: 0.6044\n",
            "Epoch 692/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7948 - accuracy: 0.7152 - val_loss: 0.9888 - val_accuracy: 0.6075\n",
            "Epoch 693/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8285 - accuracy: 0.6820 - val_loss: 0.9791 - val_accuracy: 0.6184\n",
            "Epoch 694/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7951 - accuracy: 0.7192 - val_loss: 0.9793 - val_accuracy: 0.6137\n",
            "Epoch 695/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8510 - accuracy: 0.6938 - val_loss: 0.9978 - val_accuracy: 0.6075\n",
            "Epoch 696/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8249 - accuracy: 0.6902 - val_loss: 0.9815 - val_accuracy: 0.6168\n",
            "Epoch 697/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8235 - accuracy: 0.7024 - val_loss: 0.9957 - val_accuracy: 0.6044\n",
            "Epoch 698/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8027 - accuracy: 0.7059 - val_loss: 0.9812 - val_accuracy: 0.6184\n",
            "Epoch 699/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8078 - accuracy: 0.6913 - val_loss: 0.9828 - val_accuracy: 0.6059\n",
            "Epoch 700/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8283 - accuracy: 0.6951 - val_loss: 0.9871 - val_accuracy: 0.6199\n",
            "Epoch 701/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8099 - accuracy: 0.6964 - val_loss: 0.9844 - val_accuracy: 0.6199\n",
            "Epoch 702/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.7939 - accuracy: 0.7004 - val_loss: 0.9906 - val_accuracy: 0.6199\n",
            "Epoch 703/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7792 - accuracy: 0.7115 - val_loss: 0.9883 - val_accuracy: 0.6153\n",
            "Epoch 704/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8079 - accuracy: 0.7049 - val_loss: 0.9970 - val_accuracy: 0.6012\n",
            "Epoch 705/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.7892 - accuracy: 0.7225 - val_loss: 0.9899 - val_accuracy: 0.6090\n",
            "Epoch 706/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8111 - accuracy: 0.6937 - val_loss: 0.9852 - val_accuracy: 0.6059\n",
            "Epoch 707/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8226 - accuracy: 0.6984 - val_loss: 0.9980 - val_accuracy: 0.6044\n",
            "Epoch 708/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.7642 - accuracy: 0.7179 - val_loss: 0.9930 - val_accuracy: 0.6153\n",
            "Epoch 709/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8343 - accuracy: 0.7000 - val_loss: 0.9803 - val_accuracy: 0.6184\n",
            "Epoch 710/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8272 - accuracy: 0.7044 - val_loss: 0.9954 - val_accuracy: 0.5950\n",
            "Epoch 711/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8046 - accuracy: 0.7137 - val_loss: 0.9880 - val_accuracy: 0.6168\n",
            "Epoch 712/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8005 - accuracy: 0.7109 - val_loss: 0.9823 - val_accuracy: 0.6137\n",
            "Epoch 713/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8231 - accuracy: 0.7021 - val_loss: 0.9838 - val_accuracy: 0.6184\n",
            "Epoch 714/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7977 - accuracy: 0.7101 - val_loss: 0.9852 - val_accuracy: 0.6044\n",
            "Epoch 715/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7965 - accuracy: 0.7030 - val_loss: 0.9816 - val_accuracy: 0.6121\n",
            "Epoch 716/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8401 - accuracy: 0.6896 - val_loss: 0.9889 - val_accuracy: 0.6121\n",
            "Epoch 717/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8107 - accuracy: 0.7087 - val_loss: 0.9913 - val_accuracy: 0.6137\n",
            "Epoch 718/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8007 - accuracy: 0.7031 - val_loss: 0.9843 - val_accuracy: 0.6137\n",
            "Epoch 719/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8263 - accuracy: 0.6925 - val_loss: 0.9823 - val_accuracy: 0.6059\n",
            "Epoch 720/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8392 - accuracy: 0.6958 - val_loss: 0.9862 - val_accuracy: 0.6168\n",
            "Epoch 721/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8146 - accuracy: 0.7013 - val_loss: 0.9887 - val_accuracy: 0.6044\n",
            "Epoch 722/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8351 - accuracy: 0.6987 - val_loss: 0.9974 - val_accuracy: 0.6028\n",
            "Epoch 723/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8022 - accuracy: 0.6938 - val_loss: 0.9860 - val_accuracy: 0.6137\n",
            "Epoch 724/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8050 - accuracy: 0.7102 - val_loss: 0.9849 - val_accuracy: 0.6059\n",
            "Epoch 725/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8126 - accuracy: 0.7027 - val_loss: 0.9816 - val_accuracy: 0.6059\n",
            "Epoch 726/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7837 - accuracy: 0.7169 - val_loss: 0.9880 - val_accuracy: 0.6106\n",
            "Epoch 727/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7823 - accuracy: 0.7122 - val_loss: 0.9740 - val_accuracy: 0.6121\n",
            "Epoch 728/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7665 - accuracy: 0.7294 - val_loss: 0.9861 - val_accuracy: 0.6153\n",
            "Epoch 729/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8406 - accuracy: 0.6860 - val_loss: 0.9808 - val_accuracy: 0.6075\n",
            "Epoch 730/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7954 - accuracy: 0.7129 - val_loss: 0.9778 - val_accuracy: 0.6075\n",
            "Epoch 731/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8058 - accuracy: 0.7043 - val_loss: 0.9760 - val_accuracy: 0.6246\n",
            "Epoch 732/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8458 - accuracy: 0.6863 - val_loss: 0.9743 - val_accuracy: 0.6028\n",
            "Epoch 733/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7794 - accuracy: 0.7052 - val_loss: 0.9835 - val_accuracy: 0.6059\n",
            "Epoch 734/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.7799 - accuracy: 0.7011 - val_loss: 0.9779 - val_accuracy: 0.6106\n",
            "Epoch 735/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7980 - accuracy: 0.6822 - val_loss: 0.9748 - val_accuracy: 0.6184\n",
            "Epoch 736/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7892 - accuracy: 0.7093 - val_loss: 0.9708 - val_accuracy: 0.6106\n",
            "Epoch 737/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.8020 - accuracy: 0.7102 - val_loss: 0.9754 - val_accuracy: 0.6199\n",
            "Epoch 738/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7951 - accuracy: 0.7061 - val_loss: 0.9752 - val_accuracy: 0.6137\n",
            "Epoch 739/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7814 - accuracy: 0.7169 - val_loss: 0.9691 - val_accuracy: 0.6199\n",
            "Epoch 740/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7944 - accuracy: 0.7177 - val_loss: 0.9769 - val_accuracy: 0.6075\n",
            "Epoch 741/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.7996 - accuracy: 0.7043 - val_loss: 0.9800 - val_accuracy: 0.6106\n",
            "Epoch 742/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7840 - accuracy: 0.7203 - val_loss: 0.9663 - val_accuracy: 0.6215\n",
            "Epoch 743/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.7849 - accuracy: 0.7185 - val_loss: 0.9708 - val_accuracy: 0.6262\n",
            "Epoch 744/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7710 - accuracy: 0.7138 - val_loss: 0.9738 - val_accuracy: 0.6215\n",
            "Epoch 745/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8033 - accuracy: 0.7138 - val_loss: 0.9745 - val_accuracy: 0.6106\n",
            "Epoch 746/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7684 - accuracy: 0.7229 - val_loss: 0.9746 - val_accuracy: 0.6121\n",
            "Epoch 747/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7993 - accuracy: 0.7056 - val_loss: 0.9707 - val_accuracy: 0.6277\n",
            "Epoch 748/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7789 - accuracy: 0.7156 - val_loss: 0.9773 - val_accuracy: 0.6137\n",
            "Epoch 749/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8441 - accuracy: 0.6859 - val_loss: 0.9779 - val_accuracy: 0.6090\n",
            "Epoch 750/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7799 - accuracy: 0.7264 - val_loss: 0.9777 - val_accuracy: 0.6075\n",
            "Epoch 751/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8002 - accuracy: 0.7068 - val_loss: 0.9661 - val_accuracy: 0.6277\n",
            "Epoch 752/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8053 - accuracy: 0.6962 - val_loss: 0.9768 - val_accuracy: 0.6199\n",
            "Epoch 753/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.7736 - accuracy: 0.7241 - val_loss: 0.9722 - val_accuracy: 0.6168\n",
            "Epoch 754/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7834 - accuracy: 0.6999 - val_loss: 0.9688 - val_accuracy: 0.6168\n",
            "Epoch 755/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7847 - accuracy: 0.7126 - val_loss: 0.9762 - val_accuracy: 0.6153\n",
            "Epoch 756/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7729 - accuracy: 0.7135 - val_loss: 0.9623 - val_accuracy: 0.6184\n",
            "Epoch 757/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8108 - accuracy: 0.7052 - val_loss: 0.9799 - val_accuracy: 0.6215\n",
            "Epoch 758/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8061 - accuracy: 0.7092 - val_loss: 0.9709 - val_accuracy: 0.6262\n",
            "Epoch 759/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7856 - accuracy: 0.7084 - val_loss: 0.9735 - val_accuracy: 0.6106\n",
            "Epoch 760/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7967 - accuracy: 0.6939 - val_loss: 0.9768 - val_accuracy: 0.6215\n",
            "Epoch 761/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7755 - accuracy: 0.7175 - val_loss: 0.9713 - val_accuracy: 0.6168\n",
            "Epoch 762/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7918 - accuracy: 0.7053 - val_loss: 0.9779 - val_accuracy: 0.6262\n",
            "Epoch 763/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7519 - accuracy: 0.7420 - val_loss: 0.9649 - val_accuracy: 0.6168\n",
            "Epoch 764/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7969 - accuracy: 0.7205 - val_loss: 0.9664 - val_accuracy: 0.6277\n",
            "Epoch 765/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7564 - accuracy: 0.7258 - val_loss: 0.9631 - val_accuracy: 0.6324\n",
            "Epoch 766/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8320 - accuracy: 0.6886 - val_loss: 0.9729 - val_accuracy: 0.6293\n",
            "Epoch 767/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8215 - accuracy: 0.6975 - val_loss: 0.9637 - val_accuracy: 0.6106\n",
            "Epoch 768/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7982 - accuracy: 0.6965 - val_loss: 0.9618 - val_accuracy: 0.6106\n",
            "Epoch 769/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7827 - accuracy: 0.7107 - val_loss: 0.9605 - val_accuracy: 0.6231\n",
            "Epoch 770/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7590 - accuracy: 0.7287 - val_loss: 0.9716 - val_accuracy: 0.6199\n",
            "Epoch 771/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7811 - accuracy: 0.7040 - val_loss: 0.9624 - val_accuracy: 0.6340\n",
            "Epoch 772/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7894 - accuracy: 0.6905 - val_loss: 0.9699 - val_accuracy: 0.6090\n",
            "Epoch 773/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7397 - accuracy: 0.7311 - val_loss: 0.9623 - val_accuracy: 0.6215\n",
            "Epoch 774/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7957 - accuracy: 0.7129 - val_loss: 0.9700 - val_accuracy: 0.6106\n",
            "Epoch 775/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7645 - accuracy: 0.7128 - val_loss: 0.9636 - val_accuracy: 0.6262\n",
            "Epoch 776/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7815 - accuracy: 0.7198 - val_loss: 0.9622 - val_accuracy: 0.6340\n",
            "Epoch 777/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7521 - accuracy: 0.7224 - val_loss: 0.9634 - val_accuracy: 0.6184\n",
            "Epoch 778/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7361 - accuracy: 0.7344 - val_loss: 0.9735 - val_accuracy: 0.6277\n",
            "Epoch 779/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7598 - accuracy: 0.7249 - val_loss: 0.9631 - val_accuracy: 0.6153\n",
            "Epoch 780/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8063 - accuracy: 0.6995 - val_loss: 0.9708 - val_accuracy: 0.6199\n",
            "Epoch 781/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7495 - accuracy: 0.7124 - val_loss: 0.9600 - val_accuracy: 0.6277\n",
            "Epoch 782/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7830 - accuracy: 0.7012 - val_loss: 0.9548 - val_accuracy: 0.6184\n",
            "Epoch 783/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7848 - accuracy: 0.7099 - val_loss: 0.9633 - val_accuracy: 0.6262\n",
            "Epoch 784/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7210 - accuracy: 0.7303 - val_loss: 0.9596 - val_accuracy: 0.6231\n",
            "Epoch 785/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7592 - accuracy: 0.7310 - val_loss: 0.9535 - val_accuracy: 0.6168\n",
            "Epoch 786/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7786 - accuracy: 0.7230 - val_loss: 0.9803 - val_accuracy: 0.6215\n",
            "Epoch 787/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7641 - accuracy: 0.7109 - val_loss: 0.9600 - val_accuracy: 0.6246\n",
            "Epoch 788/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7317 - accuracy: 0.7323 - val_loss: 0.9635 - val_accuracy: 0.6184\n",
            "Epoch 789/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7808 - accuracy: 0.7019 - val_loss: 0.9708 - val_accuracy: 0.6153\n",
            "Epoch 790/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7730 - accuracy: 0.7178 - val_loss: 0.9571 - val_accuracy: 0.6168\n",
            "Epoch 791/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7567 - accuracy: 0.7328 - val_loss: 0.9587 - val_accuracy: 0.6231\n",
            "Epoch 792/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8078 - accuracy: 0.7079 - val_loss: 0.9683 - val_accuracy: 0.6215\n",
            "Epoch 793/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7748 - accuracy: 0.7111 - val_loss: 0.9654 - val_accuracy: 0.6215\n",
            "Epoch 794/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7986 - accuracy: 0.7107 - val_loss: 0.9710 - val_accuracy: 0.6106\n",
            "Epoch 795/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7717 - accuracy: 0.7204 - val_loss: 0.9676 - val_accuracy: 0.6121\n",
            "Epoch 796/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7295 - accuracy: 0.7238 - val_loss: 0.9675 - val_accuracy: 0.6168\n",
            "Epoch 797/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7543 - accuracy: 0.7227 - val_loss: 0.9602 - val_accuracy: 0.6215\n",
            "Epoch 798/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7614 - accuracy: 0.7247 - val_loss: 0.9614 - val_accuracy: 0.6199\n",
            "Epoch 799/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7539 - accuracy: 0.7128 - val_loss: 0.9713 - val_accuracy: 0.6075\n",
            "Epoch 800/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7890 - accuracy: 0.7168 - val_loss: 0.9657 - val_accuracy: 0.6246\n",
            "Epoch 801/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7513 - accuracy: 0.7337 - val_loss: 0.9545 - val_accuracy: 0.6168\n",
            "Epoch 802/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7607 - accuracy: 0.7267 - val_loss: 0.9563 - val_accuracy: 0.6153\n",
            "Epoch 803/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7562 - accuracy: 0.7215 - val_loss: 0.9787 - val_accuracy: 0.6153\n",
            "Epoch 804/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.7707 - accuracy: 0.7194 - val_loss: 0.9522 - val_accuracy: 0.6277\n",
            "Epoch 805/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7488 - accuracy: 0.7306 - val_loss: 0.9592 - val_accuracy: 0.6215\n",
            "Epoch 806/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7635 - accuracy: 0.7302 - val_loss: 0.9592 - val_accuracy: 0.6277\n",
            "Epoch 807/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7507 - accuracy: 0.7370 - val_loss: 0.9643 - val_accuracy: 0.6168\n",
            "Epoch 808/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7310 - accuracy: 0.7309 - val_loss: 0.9602 - val_accuracy: 0.6246\n",
            "Epoch 809/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.8024 - accuracy: 0.7075 - val_loss: 0.9657 - val_accuracy: 0.6184\n",
            "Epoch 810/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7411 - accuracy: 0.7443 - val_loss: 0.9566 - val_accuracy: 0.6308\n",
            "Epoch 811/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7256 - accuracy: 0.7475 - val_loss: 0.9607 - val_accuracy: 0.6231\n",
            "Epoch 812/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7836 - accuracy: 0.7027 - val_loss: 0.9542 - val_accuracy: 0.6324\n",
            "Epoch 813/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7746 - accuracy: 0.7181 - val_loss: 0.9559 - val_accuracy: 0.6308\n",
            "Epoch 814/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7480 - accuracy: 0.7320 - val_loss: 0.9746 - val_accuracy: 0.6168\n",
            "Epoch 815/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7795 - accuracy: 0.7230 - val_loss: 0.9547 - val_accuracy: 0.6355\n",
            "Epoch 816/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7565 - accuracy: 0.7213 - val_loss: 0.9636 - val_accuracy: 0.6215\n",
            "Epoch 817/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7122 - accuracy: 0.7393 - val_loss: 0.9580 - val_accuracy: 0.6215\n",
            "Epoch 818/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7433 - accuracy: 0.7261 - val_loss: 0.9593 - val_accuracy: 0.6293\n",
            "Epoch 819/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7561 - accuracy: 0.7316 - val_loss: 0.9674 - val_accuracy: 0.6262\n",
            "Epoch 820/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7671 - accuracy: 0.7231 - val_loss: 0.9605 - val_accuracy: 0.6277\n",
            "Epoch 821/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7760 - accuracy: 0.7146 - val_loss: 0.9525 - val_accuracy: 0.6293\n",
            "Epoch 822/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7992 - accuracy: 0.6958 - val_loss: 0.9693 - val_accuracy: 0.6184\n",
            "Epoch 823/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7486 - accuracy: 0.7213 - val_loss: 0.9502 - val_accuracy: 0.6246\n",
            "Epoch 824/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7328 - accuracy: 0.7419 - val_loss: 0.9433 - val_accuracy: 0.6293\n",
            "Epoch 825/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7334 - accuracy: 0.7339 - val_loss: 0.9537 - val_accuracy: 0.6449\n",
            "Epoch 826/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7364 - accuracy: 0.7352 - val_loss: 0.9566 - val_accuracy: 0.6293\n",
            "Epoch 827/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7357 - accuracy: 0.7219 - val_loss: 0.9591 - val_accuracy: 0.6168\n",
            "Epoch 828/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7335 - accuracy: 0.7306 - val_loss: 0.9562 - val_accuracy: 0.6231\n",
            "Epoch 829/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7711 - accuracy: 0.7151 - val_loss: 0.9568 - val_accuracy: 0.6199\n",
            "Epoch 830/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7327 - accuracy: 0.7305 - val_loss: 0.9572 - val_accuracy: 0.6324\n",
            "Epoch 831/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7485 - accuracy: 0.7157 - val_loss: 0.9591 - val_accuracy: 0.6199\n",
            "Epoch 832/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7300 - accuracy: 0.7327 - val_loss: 0.9624 - val_accuracy: 0.6168\n",
            "Epoch 833/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7291 - accuracy: 0.7492 - val_loss: 0.9573 - val_accuracy: 0.6184\n",
            "Epoch 834/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7394 - accuracy: 0.7402 - val_loss: 0.9450 - val_accuracy: 0.6277\n",
            "Epoch 835/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7122 - accuracy: 0.7500 - val_loss: 0.9624 - val_accuracy: 0.6340\n",
            "Epoch 836/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7406 - accuracy: 0.7273 - val_loss: 0.9575 - val_accuracy: 0.6262\n",
            "Epoch 837/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7314 - accuracy: 0.7348 - val_loss: 0.9582 - val_accuracy: 0.6137\n",
            "Epoch 838/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7294 - accuracy: 0.7362 - val_loss: 0.9484 - val_accuracy: 0.6293\n",
            "Epoch 839/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7674 - accuracy: 0.7331 - val_loss: 0.9499 - val_accuracy: 0.6324\n",
            "Epoch 840/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7338 - accuracy: 0.7151 - val_loss: 0.9574 - val_accuracy: 0.6277\n",
            "Epoch 841/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7787 - accuracy: 0.7178 - val_loss: 0.9620 - val_accuracy: 0.6168\n",
            "Epoch 842/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7165 - accuracy: 0.7400 - val_loss: 0.9535 - val_accuracy: 0.6199\n",
            "Epoch 843/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7715 - accuracy: 0.6946 - val_loss: 0.9523 - val_accuracy: 0.6199\n",
            "Epoch 844/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7250 - accuracy: 0.7456 - val_loss: 0.9648 - val_accuracy: 0.6184\n",
            "Epoch 845/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.6953 - accuracy: 0.7518 - val_loss: 0.9509 - val_accuracy: 0.6355\n",
            "Epoch 846/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7427 - accuracy: 0.7267 - val_loss: 0.9464 - val_accuracy: 0.6184\n",
            "Epoch 847/1000\n",
            "121/121 [==============================] - 2s 18ms/step - loss: 0.7410 - accuracy: 0.7295 - val_loss: 0.9524 - val_accuracy: 0.6324\n",
            "Epoch 848/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7467 - accuracy: 0.7323 - val_loss: 0.9550 - val_accuracy: 0.6231\n",
            "Epoch 849/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7302 - accuracy: 0.7316 - val_loss: 0.9681 - val_accuracy: 0.6262\n",
            "Epoch 850/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7354 - accuracy: 0.7252 - val_loss: 0.9553 - val_accuracy: 0.6324\n",
            "Epoch 851/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7536 - accuracy: 0.7143 - val_loss: 0.9558 - val_accuracy: 0.6215\n",
            "Epoch 852/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7228 - accuracy: 0.7504 - val_loss: 0.9620 - val_accuracy: 0.6215\n",
            "Epoch 853/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7327 - accuracy: 0.7184 - val_loss: 0.9520 - val_accuracy: 0.6324\n",
            "Epoch 854/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7370 - accuracy: 0.7291 - val_loss: 0.9560 - val_accuracy: 0.6121\n",
            "Epoch 855/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7596 - accuracy: 0.7267 - val_loss: 0.9552 - val_accuracy: 0.6168\n",
            "Epoch 856/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7150 - accuracy: 0.7420 - val_loss: 0.9510 - val_accuracy: 0.6246\n",
            "Epoch 857/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7477 - accuracy: 0.7213 - val_loss: 0.9632 - val_accuracy: 0.6215\n",
            "Epoch 858/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.6906 - accuracy: 0.7636 - val_loss: 0.9464 - val_accuracy: 0.6199\n",
            "Epoch 859/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7812 - accuracy: 0.7121 - val_loss: 0.9542 - val_accuracy: 0.6184\n",
            "Epoch 860/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7514 - accuracy: 0.7235 - val_loss: 0.9560 - val_accuracy: 0.6231\n",
            "Epoch 861/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7418 - accuracy: 0.7254 - val_loss: 0.9422 - val_accuracy: 0.6293\n",
            "Epoch 862/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7616 - accuracy: 0.7014 - val_loss: 0.9491 - val_accuracy: 0.6231\n",
            "Epoch 863/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7514 - accuracy: 0.7222 - val_loss: 0.9462 - val_accuracy: 0.6137\n",
            "Epoch 864/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7334 - accuracy: 0.7278 - val_loss: 0.9452 - val_accuracy: 0.6246\n",
            "Epoch 865/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.6935 - accuracy: 0.7567 - val_loss: 0.9390 - val_accuracy: 0.6231\n",
            "Epoch 866/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7353 - accuracy: 0.7211 - val_loss: 0.9471 - val_accuracy: 0.6262\n",
            "Epoch 867/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7024 - accuracy: 0.7462 - val_loss: 0.9514 - val_accuracy: 0.6293\n",
            "Epoch 868/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.6955 - accuracy: 0.7537 - val_loss: 0.9522 - val_accuracy: 0.6246\n",
            "Epoch 869/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7294 - accuracy: 0.7406 - val_loss: 0.9590 - val_accuracy: 0.6262\n",
            "Epoch 870/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7234 - accuracy: 0.7313 - val_loss: 0.9604 - val_accuracy: 0.6308\n",
            "Epoch 871/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7175 - accuracy: 0.7516 - val_loss: 0.9524 - val_accuracy: 0.6246\n",
            "Epoch 872/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7248 - accuracy: 0.7319 - val_loss: 0.9604 - val_accuracy: 0.6153\n",
            "Epoch 873/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7274 - accuracy: 0.7339 - val_loss: 0.9451 - val_accuracy: 0.6215\n",
            "Epoch 874/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7355 - accuracy: 0.7290 - val_loss: 0.9469 - val_accuracy: 0.6277\n",
            "Epoch 875/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7083 - accuracy: 0.7520 - val_loss: 0.9492 - val_accuracy: 0.6277\n",
            "Epoch 876/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7281 - accuracy: 0.7405 - val_loss: 0.9569 - val_accuracy: 0.6277\n",
            "Epoch 877/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7126 - accuracy: 0.7393 - val_loss: 0.9603 - val_accuracy: 0.6184\n",
            "Epoch 878/1000\n",
            "121/121 [==============================] - 2s 19ms/step - loss: 0.7418 - accuracy: 0.7350 - val_loss: 0.9580 - val_accuracy: 0.6215\n",
            "Epoch 879/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6990 - accuracy: 0.7488 - val_loss: 0.9554 - val_accuracy: 0.6153\n",
            "Epoch 880/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7407 - accuracy: 0.7360 - val_loss: 0.9440 - val_accuracy: 0.6293\n",
            "Epoch 881/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7234 - accuracy: 0.7232 - val_loss: 0.9572 - val_accuracy: 0.6262\n",
            "Epoch 882/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7011 - accuracy: 0.7529 - val_loss: 0.9450 - val_accuracy: 0.6277\n",
            "Epoch 883/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6950 - accuracy: 0.7439 - val_loss: 0.9572 - val_accuracy: 0.6262\n",
            "Epoch 884/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7096 - accuracy: 0.7564 - val_loss: 0.9503 - val_accuracy: 0.6355\n",
            "Epoch 885/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.7085 - accuracy: 0.7381 - val_loss: 0.9536 - val_accuracy: 0.6277\n",
            "Epoch 886/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6963 - accuracy: 0.7534 - val_loss: 0.9590 - val_accuracy: 0.6246\n",
            "Epoch 887/1000\n",
            "121/121 [==============================] - 3s 22ms/step - loss: 0.6984 - accuracy: 0.7314 - val_loss: 0.9530 - val_accuracy: 0.6262\n",
            "Epoch 888/1000\n",
            "121/121 [==============================] - 3s 22ms/step - loss: 0.7079 - accuracy: 0.7509 - val_loss: 0.9481 - val_accuracy: 0.6308\n",
            "Epoch 889/1000\n",
            "121/121 [==============================] - 3s 23ms/step - loss: 0.6932 - accuracy: 0.7472 - val_loss: 0.9452 - val_accuracy: 0.6231\n",
            "Epoch 890/1000\n",
            "121/121 [==============================] - 3s 23ms/step - loss: 0.6946 - accuracy: 0.7523 - val_loss: 0.9492 - val_accuracy: 0.6246\n",
            "Epoch 891/1000\n",
            "121/121 [==============================] - 3s 22ms/step - loss: 0.6847 - accuracy: 0.7500 - val_loss: 0.9432 - val_accuracy: 0.6277\n",
            "Epoch 892/1000\n",
            "121/121 [==============================] - 3s 23ms/step - loss: 0.6577 - accuracy: 0.7682 - val_loss: 0.9432 - val_accuracy: 0.6324\n",
            "Epoch 893/1000\n",
            "121/121 [==============================] - 3s 23ms/step - loss: 0.7295 - accuracy: 0.7362 - val_loss: 0.9486 - val_accuracy: 0.6199\n",
            "Epoch 894/1000\n",
            "121/121 [==============================] - 3s 23ms/step - loss: 0.7242 - accuracy: 0.7331 - val_loss: 0.9657 - val_accuracy: 0.6277\n",
            "Epoch 895/1000\n",
            "121/121 [==============================] - 3s 23ms/step - loss: 0.7417 - accuracy: 0.7225 - val_loss: 0.9393 - val_accuracy: 0.6324\n",
            "Epoch 896/1000\n",
            "121/121 [==============================] - 3s 23ms/step - loss: 0.7244 - accuracy: 0.7256 - val_loss: 0.9487 - val_accuracy: 0.6340\n",
            "Epoch 897/1000\n",
            "121/121 [==============================] - 3s 22ms/step - loss: 0.7335 - accuracy: 0.7373 - val_loss: 0.9527 - val_accuracy: 0.6371\n",
            "Epoch 898/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.7026 - accuracy: 0.7461 - val_loss: 0.9398 - val_accuracy: 0.6308\n",
            "Epoch 899/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7335 - accuracy: 0.7365 - val_loss: 0.9478 - val_accuracy: 0.6293\n",
            "Epoch 900/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6869 - accuracy: 0.7439 - val_loss: 0.9416 - val_accuracy: 0.6355\n",
            "Epoch 901/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7235 - accuracy: 0.7219 - val_loss: 0.9588 - val_accuracy: 0.6215\n",
            "Epoch 902/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7066 - accuracy: 0.7439 - val_loss: 0.9450 - val_accuracy: 0.6293\n",
            "Epoch 903/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7053 - accuracy: 0.7452 - val_loss: 0.9461 - val_accuracy: 0.6433\n",
            "Epoch 904/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7186 - accuracy: 0.7255 - val_loss: 0.9635 - val_accuracy: 0.6308\n",
            "Epoch 905/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6920 - accuracy: 0.7506 - val_loss: 0.9557 - val_accuracy: 0.6277\n",
            "Epoch 906/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6757 - accuracy: 0.7680 - val_loss: 0.9527 - val_accuracy: 0.6246\n",
            "Epoch 907/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7580 - accuracy: 0.7187 - val_loss: 0.9697 - val_accuracy: 0.6262\n",
            "Epoch 908/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7268 - accuracy: 0.7518 - val_loss: 0.9537 - val_accuracy: 0.6308\n",
            "Epoch 909/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6894 - accuracy: 0.7518 - val_loss: 0.9486 - val_accuracy: 0.6246\n",
            "Epoch 910/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6845 - accuracy: 0.7606 - val_loss: 0.9778 - val_accuracy: 0.6231\n",
            "Epoch 911/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7277 - accuracy: 0.7335 - val_loss: 0.9532 - val_accuracy: 0.6340\n",
            "Epoch 912/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7130 - accuracy: 0.7415 - val_loss: 0.9534 - val_accuracy: 0.6246\n",
            "Epoch 913/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7031 - accuracy: 0.7548 - val_loss: 0.9521 - val_accuracy: 0.6340\n",
            "Epoch 914/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6776 - accuracy: 0.7674 - val_loss: 0.9537 - val_accuracy: 0.6293\n",
            "Epoch 915/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7302 - accuracy: 0.7355 - val_loss: 0.9489 - val_accuracy: 0.6355\n",
            "Epoch 916/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7105 - accuracy: 0.7430 - val_loss: 0.9490 - val_accuracy: 0.6293\n",
            "Epoch 917/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6842 - accuracy: 0.7511 - val_loss: 0.9513 - val_accuracy: 0.6293\n",
            "Epoch 918/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6750 - accuracy: 0.7581 - val_loss: 0.9537 - val_accuracy: 0.6277\n",
            "Epoch 919/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.7403 - accuracy: 0.7146 - val_loss: 0.9552 - val_accuracy: 0.6246\n",
            "Epoch 920/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.7127 - accuracy: 0.7388 - val_loss: 0.9462 - val_accuracy: 0.6246\n",
            "Epoch 921/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.7219 - accuracy: 0.7258 - val_loss: 0.9377 - val_accuracy: 0.6386\n",
            "Epoch 922/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6880 - accuracy: 0.7466 - val_loss: 0.9388 - val_accuracy: 0.6246\n",
            "Epoch 923/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6982 - accuracy: 0.7493 - val_loss: 0.9494 - val_accuracy: 0.6277\n",
            "Epoch 924/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.7042 - accuracy: 0.7421 - val_loss: 0.9389 - val_accuracy: 0.6215\n",
            "Epoch 925/1000\n",
            "121/121 [==============================] - 2s 21ms/step - loss: 0.7097 - accuracy: 0.7548 - val_loss: 0.9399 - val_accuracy: 0.6262\n",
            "Epoch 926/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6853 - accuracy: 0.7485 - val_loss: 0.9586 - val_accuracy: 0.6293\n",
            "Epoch 927/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6919 - accuracy: 0.7405 - val_loss: 0.9496 - val_accuracy: 0.6215\n",
            "Epoch 928/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6858 - accuracy: 0.7454 - val_loss: 0.9375 - val_accuracy: 0.6277\n",
            "Epoch 929/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.7346 - accuracy: 0.7293 - val_loss: 0.9351 - val_accuracy: 0.6293\n",
            "Epoch 930/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6794 - accuracy: 0.7566 - val_loss: 0.9312 - val_accuracy: 0.6308\n",
            "Epoch 931/1000\n",
            "121/121 [==============================] - 2s 21ms/step - loss: 0.6846 - accuracy: 0.7518 - val_loss: 0.9353 - val_accuracy: 0.6262\n",
            "Epoch 932/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6850 - accuracy: 0.7553 - val_loss: 0.9429 - val_accuracy: 0.6324\n",
            "Epoch 933/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6872 - accuracy: 0.7655 - val_loss: 0.9311 - val_accuracy: 0.6262\n",
            "Epoch 934/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6726 - accuracy: 0.7599 - val_loss: 0.9430 - val_accuracy: 0.6277\n",
            "Epoch 935/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6691 - accuracy: 0.7661 - val_loss: 0.9412 - val_accuracy: 0.6324\n",
            "Epoch 936/1000\n",
            "121/121 [==============================] - 3s 23ms/step - loss: 0.7260 - accuracy: 0.7295 - val_loss: 0.9445 - val_accuracy: 0.6262\n",
            "Epoch 937/1000\n",
            "121/121 [==============================] - 3s 24ms/step - loss: 0.7036 - accuracy: 0.7502 - val_loss: 0.9467 - val_accuracy: 0.6215\n",
            "Epoch 938/1000\n",
            "121/121 [==============================] - 3s 23ms/step - loss: 0.6983 - accuracy: 0.7494 - val_loss: 0.9373 - val_accuracy: 0.6262\n",
            "Epoch 939/1000\n",
            "121/121 [==============================] - 3s 22ms/step - loss: 0.6744 - accuracy: 0.7553 - val_loss: 0.9438 - val_accuracy: 0.6246\n",
            "Epoch 940/1000\n",
            "121/121 [==============================] - 3s 23ms/step - loss: 0.6728 - accuracy: 0.7724 - val_loss: 0.9351 - val_accuracy: 0.6277\n",
            "Epoch 941/1000\n",
            "121/121 [==============================] - 3s 22ms/step - loss: 0.6826 - accuracy: 0.7561 - val_loss: 0.9338 - val_accuracy: 0.6308\n",
            "Epoch 942/1000\n",
            "121/121 [==============================] - 3s 22ms/step - loss: 0.6828 - accuracy: 0.7606 - val_loss: 0.9361 - val_accuracy: 0.6355\n",
            "Epoch 943/1000\n",
            "121/121 [==============================] - 2s 21ms/step - loss: 0.7248 - accuracy: 0.7287 - val_loss: 0.9448 - val_accuracy: 0.6293\n",
            "Epoch 944/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7066 - accuracy: 0.7499 - val_loss: 0.9436 - val_accuracy: 0.6262\n",
            "Epoch 945/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6936 - accuracy: 0.7451 - val_loss: 0.9407 - val_accuracy: 0.6293\n",
            "Epoch 946/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6906 - accuracy: 0.7548 - val_loss: 0.9496 - val_accuracy: 0.6324\n",
            "Epoch 947/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6911 - accuracy: 0.7553 - val_loss: 0.9584 - val_accuracy: 0.6324\n",
            "Epoch 948/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6808 - accuracy: 0.7475 - val_loss: 0.9327 - val_accuracy: 0.6417\n",
            "Epoch 949/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6603 - accuracy: 0.7593 - val_loss: 0.9404 - val_accuracy: 0.6340\n",
            "Epoch 950/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6957 - accuracy: 0.7401 - val_loss: 0.9358 - val_accuracy: 0.6371\n",
            "Epoch 951/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6910 - accuracy: 0.7454 - val_loss: 0.9291 - val_accuracy: 0.6402\n",
            "Epoch 952/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6763 - accuracy: 0.7509 - val_loss: 0.9552 - val_accuracy: 0.6371\n",
            "Epoch 953/1000\n",
            "121/121 [==============================] - 2s 21ms/step - loss: 0.6859 - accuracy: 0.7505 - val_loss: 0.9664 - val_accuracy: 0.6153\n",
            "Epoch 954/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.7288 - accuracy: 0.7409 - val_loss: 0.9436 - val_accuracy: 0.6371\n",
            "Epoch 955/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6773 - accuracy: 0.7604 - val_loss: 0.9358 - val_accuracy: 0.6371\n",
            "Epoch 956/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6950 - accuracy: 0.7503 - val_loss: 0.9317 - val_accuracy: 0.6402\n",
            "Epoch 957/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6745 - accuracy: 0.7651 - val_loss: 0.9461 - val_accuracy: 0.6293\n",
            "Epoch 958/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6795 - accuracy: 0.7423 - val_loss: 0.9624 - val_accuracy: 0.6215\n",
            "Epoch 959/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6580 - accuracy: 0.7622 - val_loss: 0.9422 - val_accuracy: 0.6324\n",
            "Epoch 960/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6922 - accuracy: 0.7559 - val_loss: 0.9364 - val_accuracy: 0.6355\n",
            "Epoch 961/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6804 - accuracy: 0.7597 - val_loss: 0.9315 - val_accuracy: 0.6308\n",
            "Epoch 962/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6478 - accuracy: 0.7629 - val_loss: 0.9425 - val_accuracy: 0.6308\n",
            "Epoch 963/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6389 - accuracy: 0.7681 - val_loss: 0.9339 - val_accuracy: 0.6308\n",
            "Epoch 964/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6833 - accuracy: 0.7580 - val_loss: 0.9275 - val_accuracy: 0.6386\n",
            "Epoch 965/1000\n",
            "121/121 [==============================] - 2s 21ms/step - loss: 0.6716 - accuracy: 0.7528 - val_loss: 0.9473 - val_accuracy: 0.6324\n",
            "Epoch 966/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6840 - accuracy: 0.7457 - val_loss: 0.9387 - val_accuracy: 0.6324\n",
            "Epoch 967/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6648 - accuracy: 0.7492 - val_loss: 0.9407 - val_accuracy: 0.6371\n",
            "Epoch 968/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6814 - accuracy: 0.7597 - val_loss: 0.9331 - val_accuracy: 0.6402\n",
            "Epoch 969/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6852 - accuracy: 0.7540 - val_loss: 0.9384 - val_accuracy: 0.6308\n",
            "Epoch 970/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6749 - accuracy: 0.7574 - val_loss: 0.9398 - val_accuracy: 0.6293\n",
            "Epoch 971/1000\n",
            "121/121 [==============================] - 2s 21ms/step - loss: 0.6889 - accuracy: 0.7531 - val_loss: 0.9437 - val_accuracy: 0.6293\n",
            "Epoch 972/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6709 - accuracy: 0.7625 - val_loss: 0.9403 - val_accuracy: 0.6293\n",
            "Epoch 973/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6745 - accuracy: 0.7605 - val_loss: 0.9269 - val_accuracy: 0.6417\n",
            "Epoch 974/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6709 - accuracy: 0.7762 - val_loss: 0.9366 - val_accuracy: 0.6355\n",
            "Epoch 975/1000\n",
            "121/121 [==============================] - 2s 21ms/step - loss: 0.6896 - accuracy: 0.7423 - val_loss: 0.9316 - val_accuracy: 0.6402\n",
            "Epoch 976/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6890 - accuracy: 0.7597 - val_loss: 0.9390 - val_accuracy: 0.6371\n",
            "Epoch 977/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.7277 - accuracy: 0.7371 - val_loss: 0.9374 - val_accuracy: 0.6340\n",
            "Epoch 978/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6640 - accuracy: 0.7675 - val_loss: 0.9320 - val_accuracy: 0.6371\n",
            "Epoch 979/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6655 - accuracy: 0.7684 - val_loss: 0.9333 - val_accuracy: 0.6371\n",
            "Epoch 980/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6584 - accuracy: 0.7648 - val_loss: 0.9338 - val_accuracy: 0.6293\n",
            "Epoch 981/1000\n",
            "121/121 [==============================] - 2s 21ms/step - loss: 0.6739 - accuracy: 0.7589 - val_loss: 0.9334 - val_accuracy: 0.6355\n",
            "Epoch 982/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6918 - accuracy: 0.7402 - val_loss: 0.9331 - val_accuracy: 0.6277\n",
            "Epoch 983/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6452 - accuracy: 0.7727 - val_loss: 0.9288 - val_accuracy: 0.6340\n",
            "Epoch 984/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.7068 - accuracy: 0.7384 - val_loss: 0.9414 - val_accuracy: 0.6324\n",
            "Epoch 985/1000\n",
            "121/121 [==============================] - 2s 21ms/step - loss: 0.6558 - accuracy: 0.7559 - val_loss: 0.9347 - val_accuracy: 0.6386\n",
            "Epoch 986/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6392 - accuracy: 0.7566 - val_loss: 0.9428 - val_accuracy: 0.6371\n",
            "Epoch 987/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6661 - accuracy: 0.7613 - val_loss: 0.9391 - val_accuracy: 0.6308\n",
            "Epoch 988/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6486 - accuracy: 0.7623 - val_loss: 0.9356 - val_accuracy: 0.6371\n",
            "Epoch 989/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6800 - accuracy: 0.7428 - val_loss: 0.9364 - val_accuracy: 0.6371\n",
            "Epoch 990/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6800 - accuracy: 0.7456 - val_loss: 0.9289 - val_accuracy: 0.6324\n",
            "Epoch 991/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6353 - accuracy: 0.7862 - val_loss: 0.9294 - val_accuracy: 0.6293\n",
            "Epoch 992/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6977 - accuracy: 0.7678 - val_loss: 0.9298 - val_accuracy: 0.6371\n",
            "Epoch 993/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6623 - accuracy: 0.7720 - val_loss: 0.9405 - val_accuracy: 0.6340\n",
            "Epoch 994/1000\n",
            "121/121 [==============================] - 3s 21ms/step - loss: 0.6739 - accuracy: 0.7626 - val_loss: 0.9363 - val_accuracy: 0.6293\n",
            "Epoch 995/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6493 - accuracy: 0.7619 - val_loss: 0.9292 - val_accuracy: 0.6277\n",
            "Epoch 996/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6720 - accuracy: 0.7568 - val_loss: 0.9306 - val_accuracy: 0.6340\n",
            "Epoch 997/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6582 - accuracy: 0.7632 - val_loss: 0.9286 - val_accuracy: 0.6340\n",
            "Epoch 998/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6817 - accuracy: 0.7655 - val_loss: 0.9237 - val_accuracy: 0.6433\n",
            "Epoch 999/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6231 - accuracy: 0.7703 - val_loss: 0.9324 - val_accuracy: 0.6340\n",
            "Epoch 1000/1000\n",
            "121/121 [==============================] - 2s 20ms/step - loss: 0.6577 - accuracy: 0.7705 - val_loss: 0.9305 - val_accuracy: 0.6371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "t3OQNT0syy8E",
        "outputId": "1cb9d415-d7b6-4cc1-ce14-c3fb37b807e0"
      },
      "source": [
        "plt.plot(modelCNNhistory3.history['loss'])\n",
        "plt.plot(modelCNNhistory3.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "NaYL-RJ-4Nod",
        "outputId": "35c62396-5039-4a72-90ba-e388ed97c8b5"
      },
      "source": [
        "plt.plot(modelCNNhistory3.history['accuracy'])\n",
        "plt.plot(modelCNNhistory3.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvNqRB3y4hyl",
        "outputId": "341adb6b-a377-4ad5-a651-6528cfb47b48"
      },
      "source": [
        "predictions = modelNN3.predict_classes(x_testcnn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh9x90ZhIq6J",
        "outputId": "91ed0c99-acf2-49e0-d6a9-4e954e1639e9"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 2, 2, 5, 1, 2, 5, 1, 0, 5, 4, 0, 6, 2, 2, 6, 5, 1, 2, 1, 0, 1,\n",
              "       0, 5, 0, 5, 5, 5, 1, 2, 4, 5, 5, 3, 5, 2, 6, 1, 1, 1, 4, 4, 2, 5,\n",
              "       6, 4, 3, 0, 1, 2, 1, 6, 0, 0, 1, 1, 5, 0, 0, 2, 2, 0, 3, 5, 0, 1,\n",
              "       5, 5, 4, 4, 1, 3, 5, 2, 0, 2, 2, 0, 3, 6, 2, 2, 2, 2, 4, 5, 5, 0,\n",
              "       4, 3, 6, 5, 2, 0, 3, 1, 5, 3, 0, 3, 0, 2, 2, 5, 0, 2, 3, 1, 2, 2,\n",
              "       4, 0, 3, 3, 5, 0, 1, 6, 0, 6, 2, 4, 5, 2, 5, 4, 0, 6, 4, 4, 5, 0,\n",
              "       5, 2, 6, 5, 3, 1, 4, 6, 2, 5, 2, 6, 5, 2, 5, 4, 6, 5, 5, 5, 1, 0,\n",
              "       5, 0, 2, 2, 6, 5, 1, 1, 1, 1, 0, 6, 2, 1, 1, 2, 1, 3, 2, 2, 3, 2,\n",
              "       2, 5, 6, 1, 3, 3, 5, 1, 5, 0, 4, 3, 4, 3, 3, 3, 2, 1, 3, 2, 4, 1,\n",
              "       2, 4, 3, 5, 5, 4, 2, 3, 5, 5, 5, 2, 2, 0, 0, 1, 0, 1, 2, 5, 4, 3,\n",
              "       2, 2, 0, 2, 0, 6, 0, 4, 2, 6, 3, 1, 3, 3, 6, 2, 5, 5, 2, 2, 2, 5,\n",
              "       1, 2, 2, 0, 2, 3, 0, 2, 6, 1, 5, 3, 4, 5, 0, 2, 6, 6, 0, 3, 3, 2,\n",
              "       2, 1, 0, 6, 4, 0, 3, 0, 5, 2, 4, 6, 0, 5, 6, 4, 3, 2, 1, 5, 2, 6,\n",
              "       4, 6, 0, 3, 5, 0, 4, 3, 2, 1, 6, 5, 4, 5, 2, 0, 5, 3, 0, 3, 3, 2,\n",
              "       2, 6, 5, 6, 6, 6, 3, 0, 0, 3, 1, 2, 0, 4, 4, 1, 5, 2, 5, 6, 0, 0,\n",
              "       0, 0, 1, 3, 5, 5, 3, 5, 0, 3, 5, 4, 4, 2, 0, 6, 4, 5, 4, 2, 5, 0,\n",
              "       3, 2, 3, 0, 5, 2, 5, 6, 5, 0, 2, 3, 5, 0, 5, 4, 0, 5, 0, 2, 2, 6,\n",
              "       5, 4, 5, 4, 3, 5, 6, 1, 6, 5, 3, 2, 6, 0, 6, 0, 5, 0, 0, 5, 2, 3,\n",
              "       3, 2, 3, 6, 6, 4, 0, 1, 1, 6, 5, 2, 1, 5, 6, 6, 4, 1, 3, 0, 4, 6,\n",
              "       6, 0, 2, 6, 2, 0, 3, 2, 0, 1, 0, 2, 2, 5, 2, 3, 5, 4, 1, 4, 3, 4,\n",
              "       2, 2, 6, 3, 5, 6, 6, 2, 1, 6, 0, 3, 0, 3, 5, 6, 0, 4, 0, 6, 3, 0,\n",
              "       4, 5, 5, 2, 1, 2, 3, 3, 5, 5, 6, 1, 0, 5, 0, 5, 6, 6, 6, 2, 3, 4,\n",
              "       4, 6, 5, 4, 4, 4, 3, 6, 1, 3, 0, 3, 2, 5, 5, 5, 6, 3, 6, 5, 2, 1,\n",
              "       0, 1, 2, 0, 3, 0, 3, 6, 6, 3, 4, 2, 0, 0, 6, 0, 0, 1, 4, 5, 6, 4,\n",
              "       2, 3, 6, 2, 5, 4, 5, 6, 5, 4, 4, 2, 2, 3, 0, 6, 6, 5, 2, 3, 2, 2,\n",
              "       2, 5, 1, 2, 1, 5, 3, 4, 2, 1, 5, 3, 6, 4, 5, 5, 3, 1, 0, 2, 6, 6,\n",
              "       0, 6, 2, 5, 0, 3, 5, 4, 3, 1, 0, 3, 2, 4, 3, 3, 4, 4, 3, 5, 3, 3,\n",
              "       5, 5, 2, 2, 3, 3, 6, 2, 6, 1, 6, 4, 4, 6, 3, 2, 4, 2, 0, 0, 0, 2,\n",
              "       0, 3, 5, 4, 3, 5, 3, 0, 6, 0, 5, 4, 5, 5, 6, 5, 4, 2, 3, 5, 1, 2,\n",
              "       0, 6, 5, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAOFGxN8I_c4",
        "outputId": "73e264c8-6a0a-49fc-cafd-04c8faf9f93b"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1399    2\n",
              "312     5\n",
              "1513    2\n",
              "681     5\n",
              "717     5\n",
              "       ..\n",
              "1345    2\n",
              "2043    0\n",
              "254     4\n",
              "751     5\n",
              "1992    4\n",
              "Name: labels, Length: 642, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnAspY-oJGD4"
      },
      "source": [
        "new_Ytest = y_test.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QoD3fEPJiQ2",
        "outputId": "e1f25ced-bc39-4814-c686-929e0845bcf3"
      },
      "source": [
        "new_Ytest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1399    2\n",
              "312     5\n",
              "1513    2\n",
              "681     5\n",
              "717     5\n",
              "       ..\n",
              "1345    2\n",
              "2043    0\n",
              "254     4\n",
              "751     5\n",
              "1992    4\n",
              "Name: labels, Length: 642, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikc3BoLgJk3m",
        "outputId": "7f7756db-02e2-4e9c-af5a-4be22db471f8"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(new_Ytest, predictions)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.70      0.73       109\n",
            "           1       0.51      0.56      0.53        57\n",
            "           2       0.64      0.61      0.62       125\n",
            "           3       0.80      0.64      0.71       114\n",
            "           4       0.69      0.69      0.69        70\n",
            "           5       0.53      0.58      0.55       109\n",
            "           6       0.50      0.71      0.59        58\n",
            "\n",
            "    accuracy                           0.64       642\n",
            "   macro avg       0.63      0.64      0.63       642\n",
            "weighted avg       0.65      0.64      0.64       642\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs2UnccWJrwP",
        "outputId": "7d30ff92-08c3-46fb-b49a-9c7eca8c581b"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(new_Ytest, predictions)\n",
        "print (matrix)\n",
        "\n",
        "#0:Anger #1:Disgust #2:Fear #3:Happy #4:Neutral #5:Sad #6:Surprise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[76 13  4  5  2  2  7]\n",
            " [ 4 32  3  2  6  5  5]\n",
            " [ 7  1 76  3  3 23 12]\n",
            " [ 7  1 12 73  3  7 11]\n",
            " [ 0  2  3  0 48 14  3]\n",
            " [ 4  8 19  5  7 63  3]\n",
            " [ 0  6  2  3  1  5 41]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QncITtsKLr-",
        "outputId": "5410fd5b-9803-4abe-d20e-1505952cf38c"
      },
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/Audio_Model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "modelNN3.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/Audio_Model/Emotion_Voice_Detection_Model.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPCGVgrHKwHX",
        "outputId": "ae71c71a-407c-4d07-dfb5-679a99b355f3"
      },
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/My Drive/Audio_Model/Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_8 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 5, 256)            164096    \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 5, 256)            0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 5, 256)            0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 7)                 8967      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 173,831\n",
            "Trainable params: 173,831\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZXrVfqqLMX-",
        "outputId": "506e8fc4-2b31-4114-887b-f1c6da3e23cb"
      },
      "source": [
        "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 0s 3ms/step - loss: 0.9305 - accuracy: 0.6371\n",
            "Restored model, accuracy: 63.71%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx_m1fncLVpb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}